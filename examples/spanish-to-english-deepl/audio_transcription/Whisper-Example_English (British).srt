1
00:00:00,000 --> 00:00:06,040
2022 will be remembered as the year of Stable Diffusion, of Dalí 2, of incredible models

2
00:00:06,040 --> 00:00:10,160
text generators like Palm or code generators like AlphaCode.

3
00:00:10,160 --> 00:00:13,920
And yet, chatting last month with Andrés Torrubia, he told me that the

4
00:00:13,920 --> 00:00:18,120
most interesting thing he had seen this year was an artificial intelligence that came

5
00:00:18,120 --> 00:00:21,880
from the OpenAI lab, an AI called Whisper.

6
00:00:21,880 --> 00:00:26,880
What do you think is the most impressive thing that has come out this year?

7
00:00:26,880 --> 00:00:31,800
Well, curiously, look, curiously, so far Whisper, I think.

8
00:00:31,800 --> 00:00:32,800
Do you know why?

9
00:00:32,800 --> 00:00:33,800
Curious, eh.

10
00:00:33,800 --> 00:00:39,760
What impresses me about Whisper is that Whisper works, it's like, for me, Whisper, if it were

11
00:00:39,760 --> 00:00:46,880
the autonomous car, would be the first self-driving car in dictation, in other words, it's the first one that looks like

12
00:00:46,880 --> 00:00:47,880
a person.

13
00:00:47,880 --> 00:00:51,000
Well, but in order for you to first understand what Whisper is, I'm going to ask you to

14
00:00:51,000 --> 00:00:53,120
do the following exercise.

15
00:00:53,120 --> 00:00:57,800
I'm going to play you an audio in English and your task is to transcribe each of the words

16
00:00:57,800 --> 00:00:59,600
you hear.

17
00:00:59,600 --> 00:01:00,600
Are you ready?

18
00:01:00,600 --> 00:01:02,600
Three, two, one.

19
00:01:19,800 --> 00:01:21,280
Did you understand anything?

20
00:01:21,280 --> 00:01:22,760
Yeah, me neither.

21
00:01:22,760 --> 00:01:28,160
Well, to this artificial intelligence, this is the perfect transcription it has achieved.

22
00:01:28,160 --> 00:01:29,400
How's your Korean?

23
00:01:29,400 --> 00:01:33,680
Well, it's no problem for Whisper either and it can also transcribe this audio in

24
00:01:33,680 --> 00:01:35,520
perfect English.

25
00:01:44,440 --> 00:01:46,080
And well, it understands me too.

26
00:01:46,080 --> 00:01:50,040
What you are seeing on the screen now is the speech to text that Whisper gets when

27
00:01:50,040 --> 00:01:52,680
passes it the audio track you are listening to.

28
00:01:52,680 --> 00:01:57,440
Look closely, not only does it get an almost perfect transcription, understanding even specific

29
00:01:57,440 --> 00:02:02,760
words like Whisper or speech to text, but it is also able to generate full stops, commas

30
00:02:02,760 --> 00:02:06,560
and other punctuation marks that many other commercial models of

31
00:02:06,560 --> 00:02:08,360
speech recognition tend to struggle with.

32
00:02:08,360 --> 00:02:10,720
And this is very interesting.

33
00:02:10,720 --> 00:02:12,960
Well, not this, but Whisper.

34
00:02:12,960 --> 00:02:18,160
Whisper in general has many interesting things and the first interesting thing is the context

35
00:02:18,160 --> 00:02:20,120
in which this tool appears.

36
00:02:20,120 --> 00:02:23,640
After a year of incredible achievements by the Artificial Intelligence Lab

37
00:02:23,640 --> 00:02:29,680
of OpenAI, suddenly out of nowhere a collaborative initiative like Stability.ai

38
00:02:29,680 --> 00:02:34,320
emerges, which in September takes as a flag to make open source many of the technologies that

39
00:02:34,320 --> 00:02:40,240
OpenAI on its part has decided to keep to itself and share only under paid

40
00:02:40,240 --> 00:02:41,240
services.

41
00:02:41,240 --> 00:02:46,360
For me this is not a problem either, since in the end OpenAI as a company has

42
00:02:46,360 --> 00:02:50,720
to pay its bills and at least it is giving us a way to access these powerful

43
00:02:50,720 --> 00:02:52,360
artificial intelligences.

44
00:02:52,360 --> 00:02:53,920
Learn Google.

45
00:02:53,920 --> 00:02:57,880
But of course, a new kid comes to town and starts giving away candy to the

46
00:02:57,880 --> 00:03:01,920
children and suddenly the popular kid starts to be displaced.

47
00:03:01,920 --> 00:03:07,760
And at that precise moment OpenAI comes out of nowhere and gives us Whisper for the benefit

48
00:03:07,760 --> 00:03:08,760
of all.

49
00:03:08,760 --> 00:03:13,580
Because yes, friends, this is open source, I know you love to hear these words.

50
00:03:13,580 --> 00:03:17,160
At the end of the video I will show a mini tutorial so you can see how easy it is to use

51
00:03:17,160 --> 00:03:21,000
this tool and I will also share a notebook to make it super simple for

52
00:03:21,000 --> 00:03:22,000
you.

53
00:03:22,000 --> 00:03:25,800
And this is what makes Whisper a super interesting tool, but it is not the only thing.

54
00:03:25,800 --> 00:03:29,800
And this is where one of the things that has caught my attention the most is that Whisper

55
00:03:29,800 --> 00:03:34,440
is not a complex system that they have designed to process audio like they have never

56
00:03:34,440 --> 00:03:38,640
done before or a super complex system with a bunch of processing modules.

57
00:03:38,640 --> 00:03:45,840
No, Whisper is this right here, a transformer neural network of 2017, it has

58
00:03:45,840 --> 00:03:47,920
no changes, nothing new.

59
00:03:47,920 --> 00:03:51,280
It is an architecture that we are all already familiar with.

60
00:03:51,280 --> 00:03:55,800
So, if this is so, why didn't a technology like Whisper already exist?

61
00:03:55,800 --> 00:04:00,800
Well, the key that makes Whisper so powerful is in the data and in how

62
00:04:00,800 --> 00:04:02,920
has structured its training.

63
00:04:02,920 --> 00:04:09,040
To train it, OpenAI has used no more and no less than 680,000 hours of audio with its corresponding text

64
00:04:09,040 --> 00:04:12,360
a brutality.

65
00:04:12,360 --> 00:04:17,200
And if you calculate 680,000 hours and start playing them now, you would end up

66
00:04:17,200 --> 00:04:19,880
listening to it in 77 years' time.

67
00:04:19,880 --> 00:04:24,160
You could be sure that at some point in the sky you would see Halley's comet fly.

68
00:04:24,160 --> 00:04:28,560
But what's more, a very interesting thing is that these audios come in multiple languages,

69
00:04:28,560 --> 00:04:32,200
allowing us to train a model that is multilingual, that can understand us

70
00:04:32,200 --> 00:04:36,560
if we speak to it in Spanish, English, Korean, whatever.

71
00:04:36,560 --> 00:04:38,240
But that's not all.

72
00:04:38,240 --> 00:04:43,720
And Whisper, as well as being a multilingual system, is also a multitasking system.

73
00:04:43,720 --> 00:04:47,520
This is a trend that, as we saw in the video about Gato, is becoming more and more frequent in the world of deep

74
00:04:47,520 --> 00:04:49,760
learning.

75
00:04:49,760 --> 00:04:54,680
Do not train artificial intelligence for a single task, but train it for several different

76
00:04:54,680 --> 00:04:59,560
thus making its learning much more solid and robust.

77
00:04:59,560 --> 00:05:04,560
As we have seen, Whisper can take audio in English and transcribe it into English, or

78
00:05:04,560 --> 00:05:06,960
audio in Korean and transcribe it into Korean.

79
00:05:06,960 --> 00:05:11,200
But the same model can also identify which language is being spoken, or act

80
00:05:11,200 --> 00:05:15,360
as a voice detector to classify when a piece of audio is not

81
00:05:15,360 --> 00:05:16,360
listening to a person.

82
00:05:16,360 --> 00:05:20,960
Or, the most interesting task of all, you can speak

83
00:05:20,960 --> 00:05:25,720
to Whisper in any language and it will automatically transcribe it into English for you.

84
00:05:25,720 --> 00:05:29,800
And in this case I can't tell you why, but I find this functionality

85
00:05:29,800 --> 00:05:30,800
fascinating.

86
00:05:30,800 --> 00:05:32,880
It doesn't seem to offer us anything new either, does it?

87
00:05:32,880 --> 00:05:37,560
In the end you can take the text generated by any text transcriber in your language

88
00:05:37,560 --> 00:05:39,520
and pass it through a translator.

89
00:05:39,520 --> 00:05:43,520
But in this case I find it fascinating to see how something as simple as a single

90
00:05:43,520 --> 00:05:47,880
deep learning model allows you to speak to it in any language and it generates the text

91
00:05:47,880 --> 00:05:51,520
in English without having to combine any kind of tools.

92
00:05:51,520 --> 00:05:53,400
It's super simple.

93
00:05:53,400 --> 00:05:56,360
And the data that we mentioned before is also super interesting.

94
00:05:56,360 --> 00:06:00,480
Because my first intuition here is that OpenAI, in the search for a massive dataset

95
00:06:00,480 --> 00:06:05,280
of these 680,000 hours of audio that had a text transcription to be able to do

96
00:06:05,280 --> 00:06:09,800
this supervised learning, had possibly turned to one of the biggest sources

97
00:06:09,800 --> 00:06:12,520
that we can find on the Internet, which is YouTube.

98
00:06:12,520 --> 00:06:16,960
In the end you already know that all YouTube videos have automatically generated subtitles.

99
00:06:16,960 --> 00:06:17,960
Well no.

100
00:06:17,960 --> 00:06:22,800
This is precisely what OpenAI emphasises in their paper to explain that they have done

101
00:06:22,800 --> 00:06:28,200
a filtering process to eliminate from the dataset any text generated by

102
00:06:28,200 --> 00:06:31,000
automatic speech recognition systems.

103
00:06:31,000 --> 00:06:32,000
Why?

104
00:06:32,000 --> 00:06:36,480
Well, precisely to prevent Whisper from learning also those defects, those vices

105
00:06:36,480 --> 00:06:40,000
that the other automatic systems might also have.

106
00:06:40,000 --> 00:06:44,600
Having said that, now that we are talking about Whisper and YouTube, there is a theory that

107
00:06:44,600 --> 00:06:48,520
would like to tell you about that I find very interesting, it is nothing that is confirmed, but that

108
00:06:48,520 --> 00:06:53,560
could explain the reason for the existence of this tool and that could have a certain relationship

109
00:06:53,560 --> 00:06:55,760
with a future GPT-4.

110
00:06:55,760 --> 00:06:59,720
This is an idea I heard on Dr Alan Thompson's channel that says that at

111
00:06:59,720 --> 00:07:05,600
in the near future, where GPT-4 can start training, Whisper could offer the

112
00:07:05,600 --> 00:07:09,800
system a huge source of data that previous systems have not had.

113
00:07:09,800 --> 00:07:14,640
Let's think that a system like GPT-3 has been trained with a lot of Wikipedia articles,

114
00:07:14,640 --> 00:07:19,120
from books, from forums, from Internet conversations, but it has never been able to access all

115
00:07:19,120 --> 00:07:23,640
that spoken source that may be in databases like YouTube.

116
00:07:23,640 --> 00:07:28,240
A tool like Whisper could be used to completely sweep YouTube, transcribe

117
00:07:28,240 --> 00:07:33,200
many of its audios and obtain, unlock a new source of data that previously would not have

118
00:07:33,200 --> 00:07:37,400
been possible to use to train a future language model.

119
00:07:37,400 --> 00:07:41,560
This is the enormous value of a tool like Whisper that I think makes

120
00:07:41,560 --> 00:07:42,560
so interesting.

121
00:07:42,560 --> 00:07:47,680
No, it doesn't solve a task that is spectacular, like generating images or generating video, but

122
00:07:47,680 --> 00:07:52,280
solves a very useful task and almost solves it to perfection.

123
00:07:52,280 --> 00:07:57,640
I say almost, it is not perfect, sometimes some words are obviously wrong and it does not cover

124
00:07:57,640 --> 00:08:02,200
all the languages that exist on planet Earth and, well, to look for a limitation

125
00:08:02,200 --> 00:08:07,320
compared to other commercial tools, it does not work in real time yet.

126
00:08:07,320 --> 00:08:11,280
Processing the audio depending on the length can take a few seconds, sometimes

127
00:08:11,280 --> 00:08:17,080
a minute, but it is a solid tool, it is mature, it is useful and it is open source,

128
00:08:17,080 --> 00:08:21,040
allowing anyone to have access to a professional transcription tool

129
00:08:21,040 --> 00:08:25,160
and text translation better than any free alternative.

130
00:08:25,160 --> 00:08:26,160
What?

131
00:08:26,160 --> 00:08:28,600
Ah, that you also want to have access to this tool.

132
00:08:28,600 --> 00:08:32,720
Well, come on, I'll prepare an easy tutorial so you can all use it.

133
00:08:32,720 --> 00:08:37,640
We're going to do it on Google Colab, but first and since we're talking about programming,

134
00:08:37,640 --> 00:08:41,880
development, innovation, let me remind you that there are very few days left

135
00:08:41,880 --> 00:08:46,880
to celebrate the Samsung Dev Day, which is the technological event held every

136
00:08:46,880 --> 00:08:51,760
year by the Samsung Dev Spain community, which is the official Samsung community for developers

137
00:08:51,760 --> 00:08:52,840
Spanish.

138
00:08:52,840 --> 00:08:55,560
This will be a free event that you can't miss.

139
00:08:55,560 --> 00:09:00,640
If you are in Madrid you can attend in person on November 16th in the cloister of

140
00:09:00,640 --> 00:09:04,840
los Jerónimos of the Prado Museum and if not, you can connect online through

141
00:09:04,840 --> 00:09:05,840
its streaming.

142
00:09:05,840 --> 00:09:09,760
But yes, you have to register, I was lucky enough to participate last year with a

143
00:09:09,760 --> 00:09:14,280
presentation on code generation with artificial intelligence and the experience was

144
00:09:14,280 --> 00:09:15,280
great.

145
00:09:15,280 --> 00:09:18,800
So you can see, it will be an event full of great talks, talking about technology,

146
00:09:18,800 --> 00:09:23,280
innovation, applications and it will also be presented by my dudev, who probably

147
00:09:23,280 --> 00:09:26,560
many of you know, so you can not miss it.

148
00:09:26,560 --> 00:09:30,320
I'm going to leave below in the description box a link to the Samsung Dev website

149
00:09:30,320 --> 00:09:35,160
Spain, where you will find all the information regarding the agenda where to register and a

150
00:09:35,160 --> 00:09:37,040
lot of resources.

151
00:09:37,040 --> 00:09:38,720
See you on November 16.

152
00:09:38,720 --> 00:09:43,400
So let's see how we can use Whisper ourselves in our own code.

153
00:09:43,400 --> 00:09:47,240
For this we will use Google Colab, you know that Google here is giving us

154
00:09:47,240 --> 00:09:52,080
a free virtual machine that we can use and we will verify that we have

155
00:09:52,080 --> 00:09:56,560
enabled the type of environment with hardware acceleration GPU, okay, we will give here

156
00:09:56,560 --> 00:10:01,320
GPU, we will give save and now the first step will be to install Whisper.

157
00:10:01,320 --> 00:10:05,600
To do this we're going to use these two commands here, to install, you can find

158
00:10:05,600 --> 00:10:11,160
in the Whisper GitHub repository, I'm going to leave these commands below in the description box

159
00:10:11,160 --> 00:10:14,160
hit run and let it install.

160
00:10:14,160 --> 00:10:17,880
Once installed we are going to upload some audio that we want to transcribe, in this case

161
00:10:17,880 --> 00:10:21,920
I am going to try with the song of Rosalía de Chicken Teriyaki, we are going to place it here,

162
00:10:21,920 --> 00:10:26,800
we drag it and now the next step because we are going to take here and we are going to put the command

163
00:10:26,800 --> 00:10:31,640
necessary to be able to execute it, we are going to give here to song.mp3, it is called the file that

164
00:10:31,640 --> 00:10:37,680
we have uploaded, ok, song.mp3, the task is going to be to transcribe the size of the model,

165
00:10:37,680 --> 00:10:42,560
there are different sizes depending on whether you want more speed when doing the inference

166
00:10:42,560 --> 00:10:46,920
or if you want more precision in the results, I generally work with the Medium model

167
00:10:46,920 --> 00:10:50,600
which is the one that gives me good results, there are bigger models, there are smaller models, try

168
00:10:50,600 --> 00:10:55,360
and in this case simply where we are going to place the output file, we execute

169
00:10:55,360 --> 00:11:00,040
and that's it, that's it, we don't have to do anything else, OK, we are already using Whisper,

170
00:11:00,040 --> 00:11:03,660
the first time it will take a little while because it has to download the model but from this

171
00:11:03,660 --> 00:11:08,520
moment on you can use this system to transcribe any audio you want,

172
00:11:08,520 --> 00:11:13,640
cool, ok, we can see that in this case it has detected that the language is Spanish, it has made the inference

173
00:11:13,640 --> 00:11:16,800
automatically because we have not told it that we are going to transcribe from Spanish, you can do it

174
00:11:16,800 --> 00:11:20,960
if you want and when this cell is already executed we can come here, we see

175
00:11:20,960 --> 00:11:26,400
that the Audio Transcription folder has been generated and here we have the different options, we can

176
00:11:26,400 --> 00:11:32,360
open the sound.txt and here we open the file, we see that we have the whole song perfectly

177
00:11:32,360 --> 00:11:37,000
transcribed and in this case being Rosalía it has more merit and instead of wanting to

178
00:11:37,000 --> 00:11:41,680
do the transcription, you would like to do the translation, that is to say convert your

179
00:11:41,680 --> 00:11:45,640
voice, your audio to English, all you have to do is change here the

180
00:11:45,640 --> 00:11:51,480
task to Translate and in this case Whisper will work to translate what it has transcribed.

181
00:11:51,480 --> 00:11:54,880
In this case if you notice the command that we have used has been the one of console

182
00:11:54,880 --> 00:11:58,480
but maybe you want to use Whisper inside your code, then also

183
00:11:58,480 --> 00:12:02,000
you have the option to work with the own library of Whisper, it is simply this

184
00:12:02,000 --> 00:12:05,960
line of code of here, we import it, we load the model that we want, here then

185
00:12:05,960 --> 00:12:10,960
I would load the model Medium that is the one that as I say works better for my case and with

186
00:12:10,960 --> 00:12:17,520
the loaded model then here we call model.transcribe, we are going to put here song.mp3, we give to execute

187
00:12:17,520 --> 00:12:20,880
and in a matter of seconds we will have our transcription again.

188
00:12:20,880 --> 00:12:24,520
And here we have it, the Rosalía, rose without card, I send it to your cat, I have it to you

189
00:12:24,520 --> 00:12:27,600
with roulette, it was not necessary serenade, ok.

190
00:12:27,600 --> 00:12:31,480
Also to make your life easier I have prepared a notebook that you can use,

191
00:12:31,480 --> 00:12:35,000
is below in the description box, where you have all the code ready to

192
00:12:35,000 --> 00:12:39,200
start working, you just have to enter, check that the GPU is activated,

193
00:12:39,200 --> 00:12:43,080
we click this button here to install everything you need, here we choose the

194
00:12:43,080 --> 00:12:47,680
task we want to do, if it is transcribe to any language or translate to English

195
00:12:47,680 --> 00:12:48,800
and we click run.

196
00:12:48,800 --> 00:12:53,520
In this case the cell is prepared so that at the moment you start to execute it

197
00:12:53,520 --> 00:12:57,080
is recording right now your microphone, that is to say right now we would be generating

198
00:12:57,080 --> 00:13:00,960
an audio file that later we are going to use to transcribe with Whisper, this is for

199
00:13:00,960 --> 00:13:05,480
if you want to make a transcription in real time of any class or any thing

200
00:13:05,480 --> 00:13:06,480
that you need.

201
00:13:06,480 --> 00:13:10,800
We are going to give him to stop, we give him to this button and in a moment we have the result of the

202
00:13:10,800 --> 00:13:12,520
that we have said.

203
00:13:12,520 --> 00:13:16,800
Equally then below I add you the two necessary commands to be able to transcribe or to translate

204
00:13:16,800 --> 00:13:19,240
the audio that you upload.

205
00:13:19,240 --> 00:13:22,760
Finally, you should also know that if you want something simpler, there are

206
00:13:22,760 --> 00:13:27,240
web pages where you can try this system by uploading your own audio or recording

207
00:13:27,240 --> 00:13:28,240
from the microphone.

208
00:13:28,240 --> 00:13:32,960
And that's it, 2022 is shaping up to be a spectacular year in terms of the number

209
00:13:32,960 --> 00:13:37,360
of neural toys that are coming into our hands to build a lot

210
00:13:37,360 --> 00:13:39,320
of tools and to be able to play them.

211
00:13:39,320 --> 00:13:41,640
Now it's your turn, what can you do with these?

212
00:13:41,640 --> 00:13:45,080
Well, you can build a lot of super interesting things, you can connect for example

213
00:13:45,080 --> 00:13:49,960
Whisper with Stable Diffusion so that you can ask

214
00:13:49,960 --> 00:13:54,040
to generate a chart or you can for example take all your university classes or

215
00:13:54,040 --> 00:13:58,960
all your work meetings, transcribe them, create a huge bank of transcripts and

216
00:13:58,960 --> 00:14:03,680
then with the GPT-3 API make a chatbot that allows you to consult, ask questions

217
00:14:03,680 --> 00:14:06,160
and answers about all that source of information.

218
00:14:06,160 --> 00:14:10,040
For example, something I want to do is to take all the videos from my YouTube channel

219
00:14:10,040 --> 00:14:14,640
and transcribe them, generate good quality subtitles in both Spanish and English

220
00:14:14,640 --> 00:14:18,920
and be able to make statistics and queries on how many times I have said the word

221
00:14:18,920 --> 00:14:19,920
Machine Learning, for example.

222
00:14:19,920 --> 00:14:23,360
There are a lot of applications that you can start to build, that you can start to

223
00:14:23,360 --> 00:14:27,160
create by combining all these technologies.

224
00:14:27,160 --> 00:14:29,880
I had a dog barking in the background that was bothering me a lot.

225
00:14:29,880 --> 00:14:34,080
Well, what I was saying, you can create a lot of things and there is a lot to do.

226
00:14:34,080 --> 00:14:37,560
From here, from this channel we are going to keep experimenting with this technology,

227
00:14:37,560 --> 00:14:42,320
I will keep bringing you new tools so if you haven't subscribed yet,

228
00:14:42,320 --> 00:14:46,000
click on the little bell so you always get notifications of new videos

229
00:14:46,000 --> 00:14:50,440
and if you want to support all this content you know you can do it through Patreon

230
00:14:50,440 --> 00:14:52,080
below in the description box.

231
00:14:52,080 --> 00:14:55,080
You have a couple of videos here that are super interesting, I don't know what they are but

232
00:14:55,080 --> 00:14:58,960
are super interesting, take a look at them and see you with more artificial intelligence

233
00:14:58,960 --> 00:15:26,280
guys, girls, in the next video.

