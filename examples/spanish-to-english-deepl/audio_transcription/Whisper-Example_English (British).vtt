WEBVTT

00:00.001 --> 00:06.040
2022 will be remembered as the year of Stable Diffusion, of Dalí 2, of incredible models

00:06.040 --> 00:10.160
text generators like Palm or code generators like AlphaCode.

00:10.160 --> 00:13.920
And yet, chatting last month with Andrés Torrubia, he told me that the

00:13.920 --> 00:18.120
most interesting thing he had seen this year was an artificial intelligence that came

00:18.120 --> 00:21.880
from the OpenAI lab, an AI called Whisper.

00:21.880 --> 00:26.880
What do you think is the most impressive thing that has come out this year?

00:26.880 --> 00:31.800
Well, curiously, look, curiously, so far Whisper, I think.

00:31.800 --> 00:32.800
Do you know why?

00:32.800 --> 00:33.800
Curious, eh.

00:33.800 --> 00:39.760
What impresses me about Whisper is that Whisper works, it's like, for me, Whisper, if it were

00:39.760 --> 00:46.880
the autonomous car, would be the first self-driving car in dictation, in other words, it's the first one that looks like

00:46.880 --> 00:47.880
a person.

00:47.880 --> 00:51.000
Well, but in order for you to first understand what Whisper is, I am going to ask you to

00:51.000 --> 00:53.120
do the following exercise.

00:53.120 --> 00:57.800
I'm going to play you an audio in English and your task is to transcribe each of the words

00:57.800 --> 00:59.600
you hear.

00:59.600 --> 01:00.600
Are you ready?

01:00.600 --> 01:02.600
Three, two, one.

01:19.800 --> 01:21.280
Did you understand anything?

01:21.280 --> 01:22.760
Yeah, me neither.

01:22.760 --> 01:28.160
Well, to this artificial intelligence, this is the perfect transcription it has achieved.

01:28.160 --> 01:29.400
How's your Korean?

01:29.400 --> 01:33.680
Well, it's no problem for Whisper either and it can also transcribe this audio in

01:33.680 --> 01:35.520
perfect English.

01:44.440 --> 01:46.080
And well, it understands me too.

01:46.080 --> 01:50.040
What you are seeing on the screen now is the speech to text that Whisper gets when

01:50.040 --> 01:52.680
passes it the audio track you are listening to.

01:52.680 --> 01:57.440
Look closely, not only does it get an almost perfect transcription, understanding even specific words

01:57.440 --> 02:02.760
like Whisper or speech to text, but it is also able to generate full stops, commas

02:02.760 --> 02:06.560
and other punctuation marks that many other commercial models of speech recognition

02:06.560 --> 02:08.360
usually struggle to do.

02:08.360 --> 02:10.720
And this is very interesting.

02:10.720 --> 02:12.960
Well, not this, but Whisper.

02:12.960 --> 02:18.160
Whisper in general has many interesting things and the first interesting thing is the context

02:18.160 --> 02:20.120
in which this tool appears.

02:20.120 --> 02:23.640
After a year of incredible achievements by the Artificial Intelligence Lab

02:23.640 --> 02:29.680
of OpenAI, suddenly out of nowhere a collaborative initiative like Stability.ai

02:29.680 --> 02:34.320
emerges, which in September takes as a flag to make open source many of the technologies that

02:34.320 --> 02:40.240
OpenAI on its part has decided to keep to itself and share only under paid

02:40.240 --> 02:41.240
services.

02:41.240 --> 02:46.360
For me this is not a problem either, since in the end OpenAI as a company has

02:46.360 --> 02:50.720
to pay its bills and at least it is giving us a way to access these powerful

02:50.720 --> 02:52.360
artificial intelligences.

02:52.360 --> 02:53.920
Learn Google.

02:53.920 --> 02:57.880
But of course, a new kid comes to town and starts giving away candy to the

02:57.880 --> 03:01.920
children and suddenly the popular kid starts to be displaced.

03:01.920 --> 03:07.760
And at that precise moment OpenAI comes out of nowhere and gives us Whisper for the benefit

03:07.760 --> 03:08.760
of all.

03:08.760 --> 03:13.580
Because yes, friends, this is open source, I know you love to hear these words.

03:13.580 --> 03:17.160
At the end of the video I will show a mini tutorial so you can see how easy it is to use

03:17.160 --> 03:21.000
this tool and I will also share a notebook to make it super simple for

03:21.000 --> 03:22.000
you.

03:22.000 --> 03:25.800
And this is what makes Whisper a super interesting tool, but it is not the only thing.

03:25.800 --> 03:29.800
And this is where one of the things that has caught my attention the most is that Whisper

03:29.800 --> 03:34.440
is not a complex system that they have designed to process audio like they have never

03:34.440 --> 03:38.640
done before or a super complex system with a bunch of processing modules.

03:38.640 --> 03:45.840
No, Whisper is this right here, a transformer neural network of 2017, it has

03:45.840 --> 03:47.920
no changes, nothing new.

03:47.920 --> 03:51.280
It is an architecture that we are all already familiar with.

03:51.280 --> 03:55.800
So, if this is so, why didn't a technology like Whisper already exist?

03:55.800 --> 04:00.800
Well, the key that makes Whisper so powerful is in the data and in how

04:00.800 --> 04:02.920
has structured its training.

04:02.920 --> 04:09.040
To train it, OpenAI has used no less than 680,000 hours of audio with its corresponding text

04:09.040 --> 04:12.360
a brutality.

04:12.360 --> 04:17.200
And if you calculate 680,000 hours and start playing them now, you would end up

04:17.200 --> 04:19.880
listening to it in 77 years' time.

04:19.880 --> 04:24.160
You could be sure that at some point in the sky you would see Halley's comet fly.

04:24.160 --> 04:28.560
But what's more, a very interesting thing is that these audios come in multiple languages,

04:28.560 --> 04:32.200
allowing us to train a model that is multilingual, that can understand us

04:32.200 --> 04:36.560
if we speak to it in Spanish, English, Korean, whatever.

04:36.560 --> 04:38.240
But that's not all.

04:38.240 --> 04:43.720
And Whisper, as well as being a multilingual system, is also a multitasking system.

04:43.720 --> 04:47.520
This is a trend that, as we saw in the video on Gato, is becoming more and more frequent in the world of deep

04:47.520 --> 04:49.760
learning.

04:49.760 --> 04:54.680
Not to train artificial intelligence for a single task, but to train it for several different

04:54.680 --> 04:59.560
thus making its learning much more solid and robust.

04:59.560 --> 05:04.560
As we have seen, Whisper can take audio in English and transcribe it into English, or

05:04.560 --> 05:06.960
audio in Korean and transcribe it into Korean.

05:06.960 --> 05:11.200
But the same model can also identify which language is being spoken, or act

05:11.200 --> 05:15.360
as a voice detector to classify when a piece of audio is not

05:15.360 --> 05:16.360
listening to a person.

05:16.360 --> 05:20.960
Or, the most interesting task of all, you can speak

05:20.960 --> 05:25.720
to Whisper in any language and it will automatically transcribe it into English for you.

05:25.720 --> 05:29.800
And in this case I can't tell you why, but I find this functionality

05:29.800 --> 05:30.800
fascinating.

05:30.800 --> 05:32.880
It doesn't seem to offer us anything new either, does it?

05:32.880 --> 05:37.560
In the end you can take the text generated by any text transcriber in your language

05:37.560 --> 05:39.520
and pass it through a translator.

05:39.520 --> 05:43.520
But in this case I find it fascinating to see how something as simple as a single

05:43.520 --> 05:47.880
deep learning model allows you to speak to it in any language and it generates the text

05:47.880 --> 05:51.520
in English without having to combine any kind of tools.

05:51.520 --> 05:53.400
It's super simple.

05:53.400 --> 05:56.360
And the data that we mentioned before is also super interesting.

05:56.360 --> 06:00.480
Because my first intuition here is that OpenAI, in the search for a massive dataset

06:00.480 --> 06:05.280
of these 680,000 hours of audio that had a text transcription to be able to do

06:05.280 --> 06:09.800
this supervised learning, had possibly turned to one of the biggest sources

06:09.800 --> 06:12.520
that we can find on the Internet, which is YouTube.

06:12.520 --> 06:16.960
In the end you know that all YouTube videos have automatically generated subtitles.

06:16.960 --> 06:17.960
Well no.

06:17.960 --> 06:22.800
This is precisely what OpenAI emphasises in its paper to explain that they have done

06:22.800 --> 06:28.200
a filtering process to eliminate from the dataset any text generated by

06:28.200 --> 06:31.000
automatic speech recognition systems.

06:31.000 --> 06:32.000
Why?

06:32.000 --> 06:36.480
Well, precisely to prevent Whisper from learning also those defects, those vices

06:36.480 --> 06:40.000
that the other automatic systems might also have.

06:40.000 --> 06:44.600
Having said that, now that we are talking about Whisper and YouTube, there is a theory that

06:44.600 --> 06:48.520
would like to tell you about that I find very interesting, it is nothing that is confirmed, but that

06:48.520 --> 06:53.560
could explain the reason for the existence of this tool and that could have a certain relationship

06:53.560 --> 06:55.760
with a future GPT-4.

06:55.760 --> 06:59.720
This is an idea I heard on Dr Alan Thompson's channel that says that at

06:59.720 --> 07:05.600
in the near future, where GPT-4 can start training, Whisper could offer the

07:05.600 --> 07:09.800
system a huge source of data that previous systems have not had.

07:09.800 --> 07:14.640
Let's think that a system like GPT-3 has been trained with a lot of Wikipedia articles,

07:14.640 --> 07:19.120
from books, from forums, from Internet conversations, but it has never been able to access all

07:19.120 --> 07:23.640
that spoken source that may be in databases like YouTube.

07:23.640 --> 07:28.240
A tool like Whisper could be used to completely sweep YouTube, transcribe

07:28.240 --> 07:33.200
many of its audios and obtain, unlock a new source of data that previously would not have

07:33.200 --> 07:37.400
been possible to use to train a future language model.

07:37.400 --> 07:41.560
This is the enormous value of a tool like Whisper that I think makes

07:41.560 --> 07:42.560
so interesting.

07:42.560 --> 07:47.680
No, it doesn't solve a task that is spectacular, like generating images or generating video, but

07:47.680 --> 07:52.280
solves a very useful task and almost solves it to perfection.

07:52.280 --> 07:57.640
I say almost, it is not perfect, sometimes some words are obviously wrong and it does not cover

07:57.640 --> 08:02.200
all the languages that exist on planet Earth and, well, to look for a limitation

08:02.200 --> 08:07.320
compared to other commercial tools, it does not work in real time yet.

08:07.320 --> 08:11.280
Processing the audio depending on the length can take a few seconds, sometimes

08:11.280 --> 08:17.080
a few minutes, but it is a solid tool, it is mature, it is useful and it is open source,

08:17.080 --> 08:21.040
allowing anyone to have access to a professional transcription tool

08:21.040 --> 08:25.160
and text translation better than any free alternative.

08:25.160 --> 08:26.160
What?

08:26.160 --> 08:28.600
Ah, that you also want to have access to this tool.

08:28.600 --> 08:32.720
Well, come on, I'll prepare an easy tutorial so you can all use it.

08:32.720 --> 08:37.640
We're going to do it on Google Colab, but first and since we're talking about programming,

08:37.640 --> 08:41.880
development, innovation, let me remind you that there are very few days left

08:41.880 --> 08:46.880
to celebrate the Samsung Dev Day, which is the technological event held every

08:46.880 --> 08:51.760
year by the Samsung Dev Spain community, which is the official Samsung community for developers

08:51.760 --> 08:52.840
Spanish.

08:52.840 --> 08:55.560
This will be a free event that you can't miss.

08:55.560 --> 09:00.640
If you are in Madrid you can attend in person on November 16th in the cloister of

09:00.640 --> 09:04.840
los Jerónimos of the Prado Museum and if not, you can connect online through

09:04.840 --> 09:05.840
its streaming.

09:05.840 --> 09:09.760
But yes, you have to register, I was lucky enough to participate last year with a

09:09.760 --> 09:14.280
presentation on code generation with artificial intelligence and the experience was

09:14.280 --> 09:15.280
great.

09:15.280 --> 09:18.800
So you can see, it will be an event full of great talks, talking about technology,

09:18.800 --> 09:23.280
innovation, applications and it will also be presented by my dudev, who probably

09:23.280 --> 09:26.560
many of you know, so you can not miss it.

09:26.560 --> 09:30.320
I'm going to leave below in the description box a link to the Samsung Dev website

09:30.320 --> 09:35.160
Spain, where you will find all the information regarding the agenda where to register and a

09:35.160 --> 09:37.040
lot of resources.

09:37.040 --> 09:38.720
See you on November 16.

09:38.720 --> 09:43.400
So let's see how we can use Whisper ourselves in our own code.

09:43.400 --> 09:47.240
For this we will use Google Colab, you know that Google here is giving us

09:47.240 --> 09:52.080
a free virtual machine that we can use and we will verify that we have

09:52.080 --> 09:56.560
enabled the type of environment with hardware acceleration GPU, okay, we will give here

09:56.560 --> 10:01.320
GPU, we will give save and now the first step will be to install Whisper.

10:01.320 --> 10:05.600
To do this we're going to use these two commands here, to install, you can find

10:05.600 --> 10:11.160
in the Whisper GitHub repository, I'm going to leave these commands below in the description box

10:11.160 --> 10:14.160
hit run and let it install.

10:14.160 --> 10:17.880
Once installed we are going to upload some audio that we want to transcribe, in this case

10:17.880 --> 10:21.920
I am going to try with the song of Rosalía de Chicken Teriyaki, we are going to place it here,

10:21.920 --> 10:26.800
we drag it and now the next step because we are going to take here and we are going to put the command

10:26.800 --> 10:31.640
necessary to be able to execute it, we are going to give here to song.mp3, it is called the file that

10:31.640 --> 10:37.680
we have uploaded, ok, song.mp3, the task is going to be to transcribe the size of the model,

10:37.680 --> 10:42.560
there are different sizes depending on whether you want more speed when doing the inference

10:42.560 --> 10:46.920
or if you want more precision in the results, I generally work with the Medium model

10:46.920 --> 10:50.600
which is the one that gives me good results, there are bigger models, there are smaller models, try

10:50.600 --> 10:55.360
and in this case simply where we are going to place the output file, we execute

10:55.360 --> 11:00.040
and that's it, that's it, we don't have to do anything else, OK, we are already using Whisper,

11:00.040 --> 11:03.660
the first time it will take a little while because it has to download the model but from this

11:03.660 --> 11:08.520
moment on you can use this system to transcribe any audio you want,

11:08.520 --> 11:13.640
cool, ok, we can see that in this case it has detected that the language is Spanish, it has made the inference

11:13.640 --> 11:16.800
automatically because we have not told it that we are going to transcribe from Spanish, you can do it

11:16.800 --> 11:20.960
if you want and when this cell is already executed we can come here, we see

11:20.960 --> 11:26.400
that the Audio Transcription folder has been generated and here we have the different options, we can

11:26.400 --> 11:32.360
open the sound.txt and here we open the file, we see that we have the whole song perfectly

11:32.360 --> 11:37.000
transcribed and in this case being Rosalía it has more merit and instead of wanting to

11:37.000 --> 11:41.680
do the transcription, you would like to do the translation, that is to say convert your

11:41.680 --> 11:45.640
voice, your audio to English, all you have to do is change here the

11:45.640 --> 11:51.480
task to Translate and in this case Whisper will work to translate what it has transcribed.

11:51.480 --> 11:54.880
In this case if you notice the command that we have used has been the one of console

11:54.880 --> 11:58.480
but maybe you want to use Whisper inside your code, then also

11:58.480 --> 12:02.000
you have the option to work with the own library of Whisper, it is simply this

12:02.000 --> 12:05.960
line of code of here, we import it, we load the model that we want, here then

12:05.960 --> 12:10.960
I would load the model Medium that is the one that as I say works better for my case and with

12:10.960 --> 12:17.520
the loaded model then here we call model.transcribe, we are going to put here song.mp3, we give to execute

12:17.520 --> 12:20.880
and in a matter of seconds we will have our transcription again.

12:20.880 --> 12:24.520
And here we have it, the Rosalía, rose without card, I send it to your cat, I have it to you

12:24.520 --> 12:27.600
with roulette, it was not necessary serenade, ok.

12:27.600 --> 12:31.480
Also to make your life easier I have prepared a notebook that you can use,

12:31.480 --> 12:35.000
is below in the description box, where you have all the code ready to

12:35.000 --> 12:39.200
start working, you just have to enter, check that the GPU is activated,

12:39.200 --> 12:43.080
we click this button here to install everything you need, here we choose the

12:43.080 --> 12:47.680
task we want to do, if it is transcribe to any language or translate to English

12:47.680 --> 12:48.800
and we click run.

12:48.800 --> 12:53.520
In this case the cell is prepared so that at the moment you start to execute it

12:53.520 --> 12:57.080
is recording right now your microphone, that is to say right now we would be generating

12:57.080 --> 13:00.960
an audio file that later we are going to use to transcribe with Whisper, this is for

13:00.960 --> 13:05.480
if you want to make a transcription in real time of any class or any thing

13:05.480 --> 13:06.480
that you need.

13:06.480 --> 13:10.800
We are going to give him to stop, we give him to this button and in a moment we have the result of the

13:10.800 --> 13:12.520
that we have said.

13:12.520 --> 13:16.800
Equally then below I add you the two necessary commands to be able to transcribe or to translate

13:16.800 --> 13:19.240
the audio that you upload.

13:19.240 --> 13:22.760
Finally, you should also know that if you want something simpler, there are

13:22.760 --> 13:27.240
web pages where you can try this system by uploading your own audio or recording

13:27.240 --> 13:28.240
from the microphone.

13:28.240 --> 13:32.960
And that's it, 2022 is shaping up to be a spectacular year in terms of the number

13:32.960 --> 13:37.360
of neural toys that are coming into our hands to build a lot

13:37.360 --> 13:39.320
of tools and to be able to play them.

13:39.320 --> 13:41.640
Now it's your turn, what can you do with these?

13:41.640 --> 13:45.080
Well, you can build a lot of super interesting things, you can connect for example

13:45.080 --> 13:49.960
Whisper with Stable Diffusion so that you can ask

13:49.960 --> 13:54.040
to generate a chart or you can for example take all your university classes or

13:54.040 --> 13:58.960
all your work meetings, transcribe them, create a huge bank of transcripts and

13:58.960 --> 14:03.680
then with the GPT-3 API make a chatbot that allows you to consult, ask questions

14:03.680 --> 14:06.160
and answers about all that source of information.

14:06.160 --> 14:10.040
For example, something I want to do is to take all the videos from my YouTube channel

14:10.040 --> 14:14.640
and transcribe them, generate good quality subtitles in both Spanish and English

14:14.640 --> 14:18.920
and be able to make statistics and queries on how many times I have said the word

14:18.920 --> 14:19.920
Machine Learning, for example.

14:19.920 --> 14:23.360
There are a lot of applications that you can start to build, that you can start to

14:23.360 --> 14:27.160
create by combining all these technologies.

14:27.160 --> 14:29.880
I had a dog barking in the background that was bothering me a lot.

14:29.880 --> 14:34.080
Well, what I was saying, you can create a lot of things and there is a lot to do.

14:34.080 --> 14:37.560
From here, from this channel we are going to keep experimenting with this technology,

14:37.560 --> 14:42.320
I will keep bringing you new tools so if you haven't subscribed yet,

14:42.320 --> 14:46.000
click on the little bell so you always get notifications of new videos

14:46.000 --> 14:50.440
and if you want to support all this content you know you can do it through Patreon

14:50.440 --> 14:52.080
below in the description box.

14:52.080 --> 14:55.080
You have a couple of videos here that are super interesting, I don't know what they are but

14:55.080 --> 14:58.960
are super interesting, take a look at them and see you with more artificial intelligence

14:58.960 --> 15:26.280
guys, girls, in the next video.

