{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Carleslc/AudioToText/blob/master/examples/spanish-to-english-deepl/spanish-to-english-deepl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5hvo8QWN-a9"
      },
      "source": [
        "# üó£Ô∏è [**AudioToText**](https://github.com/Carleslc/AudioToText)\n",
        "\n",
        "### üõ† [Whisper by OpenAI (GitHub)](https://github.com/openai/whisper)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Step 1] ‚öôÔ∏è Install the required libraries\n",
        "\n",
        "Click ‚ñ∂Ô∏è button below to install the dependencies for this notebook."
      ],
      "metadata": {
        "id": "U_lylR1xWMxk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title { display-mode: \"form\" }\n",
        "!apt install ffmpeg\n",
        "!pip install git+https://github.com/openai/whisper.git deepl"
      ],
      "metadata": {
        "id": "SJl7HJOeo0-P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c3215ed-3237-4acf-bfc4-62d5e4ea7868"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.2.7-0ubuntu0.1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-510\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 27 not upgraded.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-svlsnrdt\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-svlsnrdt\n",
            "  Resolved https://github.com/openai/whisper.git to commit 7858aa9c08d98f75575035ecd6481f462d66ca27\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting deepl\n",
            "  Downloading deepl-1.13.0-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from openai-whisper==20230124) (1.21.6)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from openai-whisper==20230124) (1.13.1+cu116)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from openai-whisper==20230124) (4.64.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.8/dist-packages (from openai-whisper==20230124) (9.0.0)\n",
            "Collecting transformers>=4.19.0\n",
            "  Downloading transformers-4.26.0-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpeg-python==0.2.0\n",
            "  Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.8/dist-packages (from ffmpeg-python==0.2.0->openai-whisper==20230124) (0.16.0)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.8/dist-packages (from deepl) (2.25.1)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2->deepl) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2->deepl) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2->deepl) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2->deepl) (1.24.3)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.12.0-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.19.0->openai-whisper==20230124) (23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.19.0->openai-whisper==20230124) (2022.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers>=4.19.0->openai-whisper==20230124) (3.9.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.19.0->openai-whisper==20230124) (6.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch->openai-whisper==20230124) (4.4.0)\n",
            "Building wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20230124-py3-none-any.whl size=1179424 sha256=f228d74afe75db4bb347b4e3b00f948f8c6826dda01153d36147c9f8b4ddd807\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-hez1_f7p/wheels/a7/70/18/b7693c07b1d18b3dafb328f5d0496aa0d41a9c09ef332fd8e6\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: tokenizers, ffmpeg-python, huggingface-hub, deepl, transformers, openai-whisper\n",
            "Successfully installed deepl-1.13.0 ffmpeg-python-0.2.0 huggingface-hub-0.12.0 openai-whisper-20230124 tokenizers-0.13.2 transformers-4.26.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Step 2] üìÅ Upload your audio files to the Files folder\n",
        "\n",
        "‚¨ÖÔ∏è Files folder in Google Colab is on the left menu\n",
        "\n",
        "Almost any audio or video file format is [supported](https://gist.github.com/Carleslc/1d6b922c8bf4a7e9627a6970d178b3a6)."
      ],
      "metadata": {
        "id": "5A5bTMB8XmtI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9_I0W3tqTjr"
      },
      "source": [
        "## [Step 3] üëÇ Transcribe or Translate\n",
        "\n",
        "3.1. Choose a `task`:\n",
        "  - `Transcribe` speech to text in the same language of the source audio file.\n",
        "  - `Translate to English` speech to text in English.\n",
        "  \n",
        "Translation to other languages is not supported with _Whisper_ by default.\n",
        "You may try to choose the _Transcribe_ task and set your desired `language`, but translation is not guaranteed. However, you can use **_DeepL_** later in the Step 5 to translate the transcription to another language.\n",
        "\n",
        "3.2. Edit the `audio_file` to match your uploaded file name to transcribe.\n",
        "\n",
        "- If you want to transcribe multiple files with the same parameters you must separate their file names with commas `,`\n",
        "\n",
        "3.3. Run this cell and wait for the transcription to complete.\n",
        "\n",
        "  - You can try other parameters if the result with default parameters does not suit your needs.\n",
        "\n",
        "  If the execution takes too long to complete you can choose a smaller model in `use_model`, with an accuracy tradeoff.\n",
        "\n",
        "  [Available models and languages](https://github.com/openai/whisper#available-models-and-languages)\n",
        "\n",
        "  If the source audio file is entirely in English setting the `language` to English may provide better results when using a non-large model.\n",
        "  \n",
        "  More parameters are available in the code `options` object."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import modules\n",
        "\n",
        "import os\n",
        "\n",
        "import whisper\n",
        "from whisper.utils import format_timestamp, get_writer\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "try:\n",
        "  import tensorflow  # required in Colab to avoid protobuf compatibility issues\n",
        "except ImportError:\n",
        "  pass\n",
        "\n",
        "import torch\n",
        "\n",
        "# detect device\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "print(f\"Using {'GPU' if DEVICE == 'cuda' else 'CPU ‚ö†Ô∏è'}\")\n",
        "\n",
        "# https://medium.com/analytics-vidhya/the-google-colab-system-specification-check-69d159597417\n",
        "if DEVICE == \"cuda\":\n",
        "  !nvidia-smi -L\n",
        "else:\n",
        "  !lscpu | grep \"Model name\"\n",
        "  print(\"Not using GPU can result in a very slow execution\")\n",
        "  print(\"Ensure Hardware accelerator by GPU is enabled in Google Colab: Runtime > Change runtime type\")\n",
        "\n",
        "# select task\n",
        "\n",
        "task = \"Transcribe\" #@param [\"Transcribe\", \"Translate to English\"]\n",
        "\n",
        "task = \"transcribe\" if task == \"Transcribe\" else \"translate\"\n",
        "\n",
        "# select audio file\n",
        "\n",
        "audio_file = \"Whisper-Example.3gp\" #@param {type:\"string\"}\n",
        "\n",
        "audio_files = audio_file.split(',')\n",
        "\n",
        "# set model\n",
        "\n",
        "use_model = \"large-v2\" #@param [\"tiny\", \"base\", \"small\", \"medium\", \"large-v1\", \"large-v2\"]\n",
        "\n",
        "# select language\n",
        "\n",
        "WHISPER_LANGUAGES = [k.title() for k in whisper.tokenizer.TO_LANGUAGE_CODE.keys()]\n",
        "\n",
        "language = \"Auto-Detect\" #@param [\"Auto-Detect\", \"Afrikaans\", \"Albanian\", \"Amharic\", \"Arabic\", \"Armenian\", \"Assamese\", \"Azerbaijani\", \"Bashkir\", \"Basque\", \"Belarusian\", \"Bengali\", \"Bosnian\", \"Breton\", \"Bulgarian\", \"Burmese\", \"Castilian\", \"Catalan\", \"Chinese\", \"Croatian\", \"Czech\", \"Danish\", \"Dutch\", \"English\", \"Estonian\", \"Faroese\", \"Finnish\", \"Flemish\", \"French\", \"Galician\", \"Georgian\", \"German\", \"Greek\", \"Gujarati\", \"Haitian\", \"Haitian Creole\", \"Hausa\", \"Hawaiian\", \"Hebrew\", \"Hindi\", \"Hungarian\", \"Icelandic\", \"Indonesian\", \"Italian\", \"Japanese\", \"Javanese\", \"Kannada\", \"Kazakh\", \"Khmer\", \"Korean\", \"Lao\", \"Latin\", \"Latvian\", \"Letzeburgesch\", \"Lingala\", \"Lithuanian\", \"Luxembourgish\", \"Macedonian\", \"Malagasy\", \"Malay\", \"Malayalam\", \"Maltese\", \"Maori\", \"Marathi\", \"Moldavian\", \"Moldovan\", \"Mongolian\", \"Myanmar\", \"Nepali\", \"Norwegian\", \"Nynorsk\", \"Occitan\", \"Panjabi\", \"Pashto\", \"Persian\", \"Polish\", \"Portuguese\", \"Punjabi\", \"Pushto\", \"Romanian\", \"Russian\", \"Sanskrit\", \"Serbian\", \"Shona\", \"Sindhi\", \"Sinhala\", \"Sinhalese\", \"Slovak\", \"Slovenian\", \"Somali\", \"Spanish\", \"Sundanese\", \"Swahili\", \"Swedish\", \"Tagalog\", \"Tajik\", \"Tamil\", \"Tatar\", \"Telugu\", \"Thai\", \"Tibetan\", \"Turkish\", \"Turkmen\", \"Ukrainian\", \"Urdu\", \"Uzbek\", \"Valencian\", \"Vietnamese\", \"Welsh\", \"Yiddish\", \"Yoruba\"]\n",
        "\n",
        "if language == \"Auto-Detect\":\n",
        "  language = \"detect\"\n",
        "\n",
        "if language and language != \"detect\" and language not in WHISPER_LANGUAGES:\n",
        "  print(f\"Language '{language}' is invalid\")\n",
        "  language = \"detect\"\n",
        "\n",
        "if language and language != \"detect\":\n",
        "  print(f\"Language: {language}\\n\")\n",
        "\n",
        "# load model\n",
        "\n",
        "MODELS_WITH_ENGLISH_VERSION = [\"tiny\", \"base\", \"small\", \"medium\"]\n",
        "\n",
        "if language == \"English\" and use_model in MODELS_WITH_ENGLISH_VERSION:\n",
        "  use_model += \".en\"\n",
        "\n",
        "print(f\"\\nLoading {use_model} model...\")\n",
        "\n",
        "model = whisper.load_model(use_model, device=DEVICE)\n",
        "\n",
        "print(\n",
        "    f\"Model {use_model} is {'multilingual' if model.is_multilingual else 'English-only'} \"\n",
        "    f\"and has {sum(np.prod(p.shape) for p in model.parameters()):,d} parameters.\\n\"\n",
        ")\n",
        "\n",
        "# set options\n",
        "\n",
        "coherence_preference = \"More coherence, but may repeat text\" #@param [\"More coherence, but may repeat text\", \"Less repetitions, but may have less coherence\"]\n",
        "\n",
        "## Info: https://github.com/openai/whisper/blob/main/whisper/transcribe.py#L19\n",
        "options = {\n",
        "    'task': task,\n",
        "    'verbose': True,\n",
        "    'fp16': DEVICE == 'cuda',\n",
        "    'best_of': 5,\n",
        "    'beam_size': 5,\n",
        "    'patience': None,\n",
        "    'length_penalty': None,\n",
        "    'suppress_tokens': '-1',\n",
        "    'temperature': (0.0, 0.2, 0.4, 0.6, 0.8, 1.0),\n",
        "    'condition_on_previous_text': coherence_preference == \"More coherence, but may repeat text\",\n",
        "}\n",
        "\n",
        "if DEVICE == 'cpu':\n",
        "  torch.set_num_threads(os.cpu_count())\n",
        "\n",
        "# execute task\n",
        "# !whisper \"{audio_file}\" --task {task} --model {use_model} --output_dir {output_dir} --device {DEVICE} --verbose {options['verbose']}\n",
        "\n",
        "if task == \"translate\":\n",
        "  print(\"-- TRANSLATE TO ENGLISH --\\n\")\n",
        "else:\n",
        "  print(\"-- TRANSCRIPTION --\\n\")\n",
        "\n",
        "results = {} # audio_path to result\n",
        "\n",
        "for audio_path in audio_files:\n",
        "  print(f\"Processing: {audio_path}\")\n",
        "\n",
        "  # detect language\n",
        "  detect_language = not language or language == \"detect\"\n",
        "  if detect_language:\n",
        "    # load audio and pad/trim it to fit 30 seconds\n",
        "    audio = whisper.load_audio(audio_file)\n",
        "    audio = whisper.pad_or_trim(audio)\n",
        "\n",
        "    # make log-Mel spectrogram and move to the same device as the model\n",
        "    mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
        "\n",
        "    # detect the spoken language\n",
        "    _, probs = model.detect_language(mel)\n",
        "\n",
        "    language_code = max(probs, key=probs.get)\n",
        "    options['language'] = whisper.tokenizer.LANGUAGES[language_code].title()\n",
        "    \n",
        "    print(f\"Detected language: {options['language']}\")\n",
        "  else:\n",
        "    options['language'] = language\n",
        "\n",
        "  # transcribe\n",
        "  results[audio_path] = whisper.transcribe(model, audio_path, **options)"
      ],
      "metadata": {
        "id": "opNkn_Lgpat4",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ffcff09-d4e2-401f-8f5c-faf305fc1d47"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using GPU\n",
            "GPU 0: Tesla T4 (UUID: GPU-c31e48aa-4c4b-7720-afc9-48de26e1c511)\n",
            "\n",
            "Loading large-v2 model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2.87G/2.87G [00:14<00:00, 212MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model large-v2 is multilingual and has 1,541,384,960 parameters.\n",
            "\n",
            "-- TRANSCRIPTION --\n",
            "\n",
            "Processing: Whisper-Example.3gp\n",
            "Detected language: Spanish\n",
            "[00:00.000 --> 00:06.040]  2022 ser√° recordado como el a√±o de Stable Diffusion, de Dal√≠ 2, de incre√≠bles modelos\n",
            "[00:06.040 --> 00:10.160]  generadores de texto como Palm o generadores de c√≥digo como AlphaCode.\n",
            "[00:10.160 --> 00:13.920]  Y sin embargo, charlando el mes pasado con Andr√©s Torrubia, √©l me comentaba que lo\n",
            "[00:13.920 --> 00:18.120]  m√°s interesante que hab√≠a visto este a√±o era una inteligencia artificial que ven√≠a\n",
            "[00:18.120 --> 00:21.880]  del laboratorio OpenAI, una IA llamada Whisper.\n",
            "[00:21.880 --> 00:26.880]  ¬øQu√© es para ti de lo que ha salido este a√±o lo m√°s impresionante?\n",
            "[00:26.880 --> 00:31.800]  Pues curiosamente, f√≠jate, curiosamente, hasta ahora Whisper, yo creo.\n",
            "[00:31.800 --> 00:32.800]  ¬øSabes por qu√©?\n",
            "[00:32.800 --> 00:33.800]  Curioso, eh.\n",
            "[00:33.800 --> 00:39.760]  Por lo que me impresiona Whisper es que Whisper funciona, es como, para m√≠, Whisper, si fuera\n",
            "[00:39.760 --> 00:46.880]  el coche aut√≥nomo, ser√≠a el primer self-driving del dictado, o sea, es el primero que se parece\n",
            "[00:46.880 --> 00:47.880]  a una persona.\n",
            "[00:47.880 --> 00:51.000]  Bueno, pero para que entiendas t√∫ primero qu√© es esto de Whisper, te voy a pedir que\n",
            "[00:51.000 --> 00:53.120]  hagas el siguiente ejercicio.\n",
            "[00:53.120 --> 00:57.800]  Te voy a reproducir un audio en ingl√©s y tu tarea es transcribir cada una de las palabras\n",
            "[00:57.800 --> 00:59.600]  que est√©s escuchando.\n",
            "[00:59.600 --> 01:00.600]  ¬øEst√°s listo?\n",
            "[01:00.600 --> 01:02.600]  Tres, dos, uno.\n",
            "[01:19.800 --> 01:21.280]  ¬øHas entendido algo?\n",
            "[01:21.280 --> 01:22.760]  Ya, yo tampoco.\n",
            "[01:22.760 --> 01:28.160]  Pues a o√≠dos de esta inteligencia artificial, esta es la transcripci√≥n perfecta que ha conseguido.\n",
            "[01:28.160 --> 01:29.400]  ¬øY qu√© tal tu coreano?\n",
            "[01:29.400 --> 01:33.680]  Bueno, pues para Whisper tampoco es problema y tambi√©n puede transcribir este audio en\n",
            "[01:33.680 --> 01:35.520]  perfecto ingl√©s.\n",
            "[01:44.440 --> 01:46.080]  Y bueno, tambi√©n me entiende a m√≠.\n",
            "[01:46.080 --> 01:50.040]  Esto que est√°s viendo en pantalla ahora es el speech to text que consigue Whisper cuando\n",
            "[01:50.040 --> 01:52.680]  le paso la pista de audio que est√°s escuchando.\n",
            "[01:52.680 --> 01:57.440]  F√≠jate bien, no s√≥lo consigue una transcripci√≥n casi perfecta, entendiendo incluso palabras\n",
            "[01:57.440 --> 02:02.760]  concretas como Whisper o speech to text, sino que tambi√©n es capaz de generar puntos, comas\n",
            "[02:02.760 --> 02:06.560]  y otros signos de puntuaci√≥n que a otros muchos modelos comerciales de reconocimiento\n",
            "[02:06.560 --> 02:08.360]  del habla pues se le suele atragantar.\n",
            "[02:08.360 --> 02:10.720]  Y esto es muy interesante.\n",
            "[02:10.720 --> 02:12.960]  Bueno, no esto, sino Whisper.\n",
            "[02:12.960 --> 02:18.160]  Whisper en general tiene muchas cosas interesantes y la primera cosa interesante es el contexto\n",
            "[02:18.160 --> 02:20.120]  en el que esta herramienta aparece.\n",
            "[02:20.120 --> 02:23.640]  Tras un a√±o de incre√≠bles logros por parte del Laboratorio de Inteligencia Artificial\n",
            "[02:23.640 --> 02:29.680]  de OpenAI, de repente de la nada surge una iniciativa colaborativa como Stability.ai\n",
            "[02:29.680 --> 02:34.320]  que en septiembre toma por bandera el hacer open source muchas de las tecnolog√≠as que\n",
            "[02:34.320 --> 02:40.240]  OpenAI por su parte pues ha decidido guardarse para s√≠ y compartir s√≥lo bajo servicios\n",
            "[02:40.240 --> 02:41.240]  de pago.\n",
            "[02:41.240 --> 02:46.360]  Para m√≠ esto tampoco es un problema, puesto que al final OpenAI como empresa pues tiene\n",
            "[02:46.360 --> 02:50.720]  que pagar sus facturas y al menos nos est√° dando una forma de acceder a estas potentes\n",
            "[02:50.720 --> 02:52.360]  inteligencias artificiales.\n",
            "[02:52.360 --> 02:53.920]  Aprende Google.\n",
            "[02:53.920 --> 02:57.880]  Pero claro, llega un muchachito nuevo a la ciudad y empieza a regalar caramelos a los\n",
            "[02:57.880 --> 03:01.920]  ni√±os y de repente el chico popular pues empieza a ver desplazado.\n",
            "[03:01.920 --> 03:07.760]  Y en ese preciso momento llega OpenAI de la nada y nos regala a Whisper para beneficio\n",
            "[03:07.760 --> 03:08.760]  de todos.\n",
            "[03:08.760 --> 03:13.580]  Porque s√≠, amigos, esto es open source, que s√© que os encanta escuchar estas palabras.\n",
            "[03:13.580 --> 03:17.160]  Al final del v√≠deo voy a ense√±ar un mini tutorial para que ve√°is qu√© sencillo es utilizar\n",
            "[03:17.160 --> 03:21.000]  esta herramienta y tambi√©n os voy a compartir un notebook para que sea s√∫per sencillo para\n",
            "[03:21.000 --> 03:22.000]  vosotros.\n",
            "[03:22.000 --> 03:25.800]  Y esto es lo que hace a Whisper una herramienta s√∫per interesante, pero no es la √∫nica cosa.\n",
            "[03:25.800 --> 03:29.800]  Y aqu√≠ es donde viene una de las cosas que m√°s ha llamado mi atenci√≥n y es que Whisper\n",
            "[03:29.800 --> 03:34.440]  no es un complejo sistema que hayan dise√±ado para procesar audio como nunca antes hab√≠a\n",
            "[03:34.440 --> 03:38.640]  hecho o un sistema s√∫per complejo con un mont√≥n de m√≥dulos de procesamiento.\n",
            "[03:38.640 --> 03:45.840]  No, Whisper es esto de aqu√≠, una red neuronal de tipo transformer de las de 2017, no tiene\n",
            "[03:45.840 --> 03:47.920]  ning√∫n cambio, ninguna novedad.\n",
            "[03:47.920 --> 03:51.280]  Es una arquitectura que ya todos nosotros conocemos.\n",
            "[03:51.280 --> 03:55.800]  Entonces, si esto es as√≠, por qu√© no exist√≠a ya una tecnolog√≠a como Whisper?\n",
            "[03:55.800 --> 04:00.800]  Pues la clave que hace a Whisper algo tan potente est√° en los datos y en c√≥mo han\n",
            "[04:00.800 --> 04:02.920]  estructurado su entrenamiento.\n",
            "[04:02.920 --> 04:09.040]  Para entrenarlo, OpenAI ha utilizado ni m√°s ni menos que 680.000 horas de audio con su\n",
            "[04:09.040 --> 04:12.360]  correspondiente texto, una brutalidad.\n",
            "[04:12.360 --> 04:17.200]  Y es que si hac√©is el c√°lculo 680.000 horas y empezar a reproducirlas ahora, acabar√≠as\n",
            "[04:17.200 --> 04:19.880]  de escucharla dentro de 77 a√±os.\n",
            "[04:19.880 --> 04:24.160]  Te podr√≠as asegurar que en alg√∫n momento en el cielo ver√≠as surcar al cometa Halley.\n",
            "[04:24.160 --> 04:28.560]  Pero es que adem√°s, una cosa muy interesante es que estos audios vienen en m√∫ltiples idiomas,\n",
            "[04:28.560 --> 04:32.200]  permiti√©ndonos poder entrenar a un modelo que es multilinguaje, que puede entendernos\n",
            "[04:32.200 --> 04:36.560]  si le hablamos en espa√±ol, en ingl√©s, en coreano, da igual.\n",
            "[04:36.560 --> 04:38.240]  Pero la cosa no se queda solo ah√≠.\n",
            "[04:38.240 --> 04:43.720]  Y es que Whisper, adem√°s de ser un sistema multilinguaje, tambi√©n es un sistema multitarea.\n",
            "[04:43.720 --> 04:47.520]  Esta es una tendencia que, como ya vimos en el v√≠deo sobre Gato, en el mundo del deep\n",
            "[04:47.520 --> 04:49.760]  learning cada vez es m√°s frecuente.\n",
            "[04:49.760 --> 04:54.680]  No entrenar a la inteligencia artificial para una √∫nica tarea, sino entrenarla para varias\n",
            "[04:54.680 --> 04:59.560]  diferentes, haciendo as√≠ que su aprendizaje sea mucho m√°s s√≥lido y robusto.\n",
            "[04:59.560 --> 05:04.560]  Como hemos visto, Whisper puede tomar audios en ingl√©s y transcribirlos al ingl√©s, o\n",
            "[05:04.560 --> 05:06.960]  audio en coreano y transcribirlo al coreano.\n",
            "[05:06.960 --> 05:11.200]  Pero el mismo modelo tambi√©n puede identificar qu√© lenguaje se est√° hablando, o actuar\n",
            "[05:11.200 --> 05:15.360]  como un detector de voz para clasificar cuando en un trozo de audio se est√° escuchando no\n",
            "[05:15.360 --> 05:16.360]  a una persona.\n",
            "[05:16.360 --> 05:20.960]  O tambi√©n, la tarea que m√°s interesante me parece de todas, que t√∫ le puedas hablar\n",
            "[05:20.960 --> 05:25.720]  a Whisper en cualquier idioma y que √©l te lo transcriba autom√°ticamente al ingl√©s.\n",
            "[05:25.720 --> 05:29.800]  Y en este caso no sabr√≠a deciros por qu√©, pero para m√≠ esta me parece una funcionalidad\n",
            "[05:29.800 --> 05:30.800]  fascinante.\n",
            "[05:30.800 --> 05:32.880]  Parece que tampoco nos ofrece nada nuevo, ¬øno?\n",
            "[05:32.880 --> 05:37.560]  Al final t√∫ puedes coger el texto que genera cualquier transcriptor de texto en tu idioma\n",
            "[05:37.560 --> 05:39.520]  y pasarlo por un traductor.\n",
            "[05:39.520 --> 05:43.520]  Pero en este caso me parece fascinante el ver c√≥mo algo tan sencillo como un √∫nico\n",
            "[05:43.520 --> 05:47.880]  modelo de deep learning te permite poder hablarle en cualquier idioma y que te genere el texto\n",
            "[05:47.880 --> 05:51.520]  en ingl√©s sin tener que combinar ning√∫n tipo de herramientas.\n",
            "[05:51.520 --> 05:53.400]  Es s√∫per sencillo.\n",
            "[05:53.400 --> 05:56.360]  Y lo de los datos que hemos comentado antes tambi√©n es s√∫per interesante.\n",
            "[05:56.360 --> 06:00.480]  Porque mi primera intuici√≥n aqu√≠ es que OpenAI, en la b√∫squeda de un dataset masivo\n",
            "[06:00.480 --> 06:05.280]  de estas 680.000 horas de audio que tuviera una transcripci√≥n de texto para poder hacer\n",
            "[06:05.280 --> 06:09.800]  este aprendizaje supervisado, pues posiblemente hab√≠a acudido a una de las mayores fuentes\n",
            "[06:09.800 --> 06:12.520]  que podemos encontrar en Internet, que es YouTube.\n",
            "[06:12.520 --> 06:16.960]  Al final ya sab√©is que todos los v√≠deos de YouTube tienen generados subt√≠tulos autom√°ticamente.\n",
            "[06:16.960 --> 06:17.960]  Pues no.\n",
            "[06:17.960 --> 06:22.800]  Justamente en esto OpenAI hace mucho hincapi√© en su paper para explicarnos que han hecho\n",
            "[06:22.800 --> 06:28.200]  un proceso de filtrado para eliminar del dataset cualquier aparici√≥n de texto generado por\n",
            "[06:28.200 --> 06:31.000]  sistemas autom√°ticos de reconocimiento del habla.\n",
            "[06:31.000 --> 06:32.000]  ¬øPor qu√©?\n",
            "[06:32.000 --> 06:36.480]  Pues justamente para evitar que Whisper aprendiera tambi√©n aquellos defectos, aquellos vicios\n",
            "[06:36.480 --> 06:40.000]  que los otros sistemas autom√°ticos tambi√©n pudieran tener.\n",
            "[06:40.000 --> 06:44.600]  Dicho esto, ahora que estamos hablando de Whisper y de YouTube, hay una teor√≠a que\n",
            "[06:44.600 --> 06:48.520]  quiero contaros que me parece muy interesante, no es nada que est√© confirmado, pero que\n",
            "[06:48.520 --> 06:53.560]  podr√≠a explicar la raz√≥n de existir de esta herramienta y que podr√≠a tener cierta relaci√≥n\n",
            "[06:53.560 --> 06:55.760]  con un futuro GPT-4.\n",
            "[06:55.760 --> 06:59.720]  Esta es una idea que escuch√© en el canal del doctor Alan Thompson y que dice que en\n",
            "[06:59.720 --> 07:05.600]  un futuro pr√≥ximo, donde GPT-4 pueda empezar a entrenar, Whisper podr√≠a ofrecer al sistema\n",
            "[07:05.600 --> 07:09.800]  una enorme fuente de datos con la que sistemas anteriores no hab√≠an contado.\n",
            "[07:09.800 --> 07:14.640]  Pensemos que un sistema como GPT-3 se ha entrenado con un mont√≥n de art√≠culos de Wikipedia,\n",
            "[07:14.640 --> 07:19.120]  de libros, de foros, de conversaciones de Internet, pero nunca ha podido acceder a toda\n",
            "[07:19.120 --> 07:23.640]  esa fuente hablada que puede estar en bases de datos como YouTube.\n",
            "[07:23.640 --> 07:28.240]  Una herramienta como Whisper podr√≠a ser utilizada para barrer por completo a YouTube, transcribir\n",
            "[07:28.240 --> 07:33.200]  muchos de sus audios y obtener, desbloquear una nueva fuente de datos que antes no habr√≠a\n",
            "[07:33.200 --> 07:37.400]  sido posible utilizar para entrenar a un futuro modelo del lenguaje.\n",
            "[07:37.400 --> 07:41.560]  Este es el enorme valor que tiene una herramienta como Whisper y que creo que hace tan interesante\n",
            "[07:41.560 --> 07:42.560]  a esta tecnolog√≠a.\n",
            "[07:42.560 --> 07:47.680]  No, no resuelve una tarea que sea espectacular, como generar im√°genes o generar v√≠deo, pero\n",
            "[07:47.680 --> 07:52.280]  resuelve una tarea muy √∫til y casi la resuelve hasta la perfecci√≥n.\n",
            "[07:52.280 --> 07:57.640]  Ojo, digo casi, no es perfecta, a veces algunas palabras se equivocan evidentemente y no cubre\n",
            "[07:57.640 --> 08:02.200]  todos los lenguajes que existen en el planeta Tierra y bueno, por buscar alguna limitaci√≥n\n",
            "[08:02.200 --> 08:07.320]  frente a otras herramientas comerciales, pues tampoco funciona en tiempo real todav√≠a.\n",
            "[08:07.320 --> 08:11.280]  Procesar el audio dependiendo de la longitud te puede llevar unos cuantos segundos, a veces\n",
            "[08:11.280 --> 08:17.080]  alg√∫n minuto, pero es una herramienta s√≥lida, es madura, es √∫til y adem√°s open source,\n",
            "[08:17.080 --> 08:21.040]  permitiendo que ahora cualquiera pueda acceder a una herramienta profesional de transcripci√≥n\n",
            "[08:21.040 --> 08:25.160]  y traducci√≥n de texto mejor que cualquier alternativa gratis.\n",
            "[08:25.160 --> 08:26.160]  ¬øQu√©?\n",
            "[08:26.160 --> 08:28.600]  Ah, que tambi√©n vosotros quer√©is acceder a esta herramienta.\n",
            "[08:28.600 --> 08:32.720]  Bueno, venga va, os preparo un tutorial facilito para que todos pod√°is utilizarlo.\n",
            "[08:32.720 --> 08:37.640]  Vamos a hacerlo en Google Colab, pero antes y aprovechando que estamos hablando de programaci√≥n,\n",
            "[08:37.640 --> 08:41.880]  de desarrollo, de innovaci√≥n, dejadme que os recuerde que quedan muy poquitos d√≠as\n",
            "[08:41.880 --> 08:46.880]  para que se celebre el Samsung Dev Day, que es el evento tecnol√≥gico que celebra cada\n",
            "[08:46.880 --> 08:51.760]  a√±o la comunidad de Samsung Dev Spain, que es la comunidad oficial de Samsung para desarrolladores\n",
            "[08:51.760 --> 08:52.840]  espa√±oles.\n",
            "[08:52.840 --> 08:55.560]  Este ser√° un evento gratuito que no os pod√©is perder.\n",
            "[08:55.560 --> 09:00.640]  Si est√°is en Madrid pod√©is asistir presencialmente el d√≠a 16 de noviembre en el claustro de\n",
            "[09:00.640 --> 09:04.840]  los Jer√≥nimos del Museo del Prado y si no, pues pod√©is conectaros online a trav√©s de\n",
            "[09:04.840 --> 09:05.840]  su streaming.\n",
            "[09:05.840 --> 09:09.760]  Pero si, hay que registrarse, yo tuve la suerte el a√±o pasado de poder participar con una\n",
            "[09:09.760 --> 09:14.280]  ponencia sobre generaci√≥n de c√≥digo con inteligencia artificial y la experiencia fue\n",
            "[09:14.280 --> 09:15.280]  genial.\n",
            "[09:15.280 --> 09:18.800]  As√≠ que ya lo veis, ser√° un evento cargado de charlas geniales, hablando de tecnolog√≠a,\n",
            "[09:18.800 --> 09:23.280]  de innovaci√≥n, de aplicaciones y adem√°s va a estar presentado por mi dudev, que seguramente\n",
            "[09:23.280 --> 09:26.560]  muchos de vosotros le conozc√°is, as√≠ que no os lo pod√©is perder.\n",
            "[09:26.560 --> 09:30.320]  Os voy a dejar abajo en la cajita de descripci√≥n un enlace a la p√°gina web de Samsung Dev\n",
            "[09:30.320 --> 09:35.160]  Spain, donde vais a encontrar toda la informaci√≥n respecto a la agenda donde registraros y un\n",
            "[09:35.160 --> 09:37.040]  mont√≥n de recursos m√°s.\n",
            "[09:37.040 --> 09:38.720]  Nos vemos el 16 de noviembre.\n",
            "[09:38.720 --> 09:43.400]  Pues vamos a ver c√≥mo podemos utilizar Whisper nosotros en nuestro propio c√≥digo.\n",
            "[09:43.400 --> 09:47.240]  Para esto vamos a utilizar Google Colab, ya sab√©is que Google aqu√≠ nos est√° cediendo\n",
            "[09:47.240 --> 09:52.080]  una m√°quina virtual gratuita que podemos utilizar y vamos a verificar siempre que tengamos\n",
            "[09:52.080 --> 09:56.560]  activado el tipo de entorno con aceleraci√≥n por hardware GPU, vale, vamos a darle aqu√≠\n",
            "[09:56.560 --> 10:01.320]  GPU, vamos a darle a guardar y ahora el primer paso ser√° instalar a Whisper.\n",
            "[10:01.320 --> 10:05.600]  Para ello vamos a usar estos dos comandos de aqu√≠, a instalar, esto lo pod√©is encontrar\n",
            "[10:05.600 --> 10:11.160]  en el propio repositorio de GitHub de Whisper, os voy a dejar abajo en la cajita de descripci√≥n\n",
            "[10:11.160 --> 10:14.160]  estos comandos, le damos a ejecutar y dejamos que se instale.\n",
            "[10:14.160 --> 10:17.880]  Una vez instalado vamos a subir alg√∫n audio que queramos transcribir, yo en este caso\n",
            "[10:17.880 --> 10:21.920]  voy a probar con la canci√≥n de Rosal√≠a de Chicken Teriyaki, vamos a colocarla para ac√°,\n",
            "[10:21.920 --> 10:26.800]  la arrastramos y ahora el siguiente paso pues vamos a coger aqu√≠ y vamos a poner el comando\n",
            "[10:26.800 --> 10:31.640]  necesario para poder ejecutarlo, vamos a darle aqu√≠ a song.mp3, se llama el archivo que\n",
            "[10:31.640 --> 10:37.680]  hemos subido, vale, song.mp3, la tarea va a ser pues transcribir el tama√±o del modelo,\n",
            "[10:37.680 --> 10:42.560]  hay diferentes tama√±os seg√∫n si quieres m√°s velocidad a la hora de hacer la inferencia\n",
            "[10:42.560 --> 10:46.920]  o si quieres m√°s precisi√≥n en los resultados, yo por lo general trabajo con el modelo Medium\n",
            "[10:46.920 --> 10:50.600]  que es el que me da buenos resultados, hay modelos mayores, hay modelos menores, probad\n",
            "[10:50.600 --> 10:55.360]  y en este caso pues simplemente donde vamos a colocar el archivo de salida, ejecutamos\n",
            "[10:55.360 --> 11:00.040]  y ya est√°, ya est√°, no hay que hacer nada m√°s, vale, ya estamos utilizando Whisper,\n",
            "[11:00.040 --> 11:03.660]  la primera vez tardar√° un poco porque tiene que descargar el modelo pero a partir de este\n",
            "[11:03.660 --> 11:08.520]  momento pod√©is utilizar este sistema para transcribir cualquier audio que quer√°is,\n",
            "[11:08.520 --> 11:13.640]  mola, vale, vemos que en este caso ha detectado que el idioma es espa√±ol, ha hecho la inferencia\n",
            "[11:13.640 --> 11:16.800]  autom√°tica porque no le hemos dicho que vamos a transcribir del espa√±ol, lo pod√©is hacer\n",
            "[11:16.800 --> 11:20.960]  si quer√©is y cuando ya est√° ejecutada esta celda pues podemos venirnos para ac√°, vemos\n",
            "[11:20.960 --> 11:26.400]  que se ha generado la carpeta Audio Transcription y aqu√≠ tenemos las diferentes opciones, podemos\n",
            "[11:26.400 --> 11:32.360]  abrir el sound.txt y aqu√≠ abrimos el archivo, vemos que pues tenemos toda la canci√≥n perfectamente\n",
            "[11:32.360 --> 11:37.000]  transcrita que en este caso siendo la Rosal√≠a pues tiene m√°s m√©rito y en vez de querer\n",
            "[11:37.000 --> 11:41.680]  hacer la transcripci√≥n, quisierais hacer la traducci√≥n, es decir convertir vuestra\n",
            "[11:41.680 --> 11:45.640]  voz, vuestro audio al ingl√©s, pues lo √∫nico que ten√©is que hacer es cambiar aqu√≠ la\n",
            "[11:45.640 --> 11:51.480]  tarea por Translate y en este caso Whisper trabajar√° para traducir aquello que ha transcrito.\n",
            "[11:51.480 --> 11:54.880]  En este caso si os dais cuenta el comando que hemos utilizado ha sido el de consola\n",
            "[11:54.880 --> 11:58.480]  pero a lo mejor quer√©is utilizar Whisper dentro de vuestro c√≥digo, entonces tambi√©n\n",
            "[11:58.480 --> 12:02.000]  ten√©is la opci√≥n de trabajar con la propia librer√≠a de Whisper, es simplemente esta\n",
            "[12:02.000 --> 12:05.960]  l√≠nea de c√≥digo de aqu√≠, lo importamos, cargamos el modelo que queramos, aqu√≠ pues\n",
            "[12:05.960 --> 12:10.960]  yo cargar√≠a el modelo Medium que es el que como digo funciona mejor para mi caso y con\n",
            "[12:10.960 --> 12:17.520]  el modelo cargado luego aqu√≠ llamamos a model.transcribe, vamos a poner aqu√≠ song.mp3, le damos a ejecutar\n",
            "[12:17.520 --> 12:20.880]  y en cuesti√≥n de unos segundos pues ya tendremos de nuevo nuestra transcripci√≥n.\n",
            "[12:20.880 --> 12:24.520]  Y aqu√≠ lo tenemos, la Rosal√≠a, rosa sin tarjeta, se la mando a tu gata, te la tengo\n",
            "[12:24.520 --> 12:27.600]  con ruleta, no hizo falta serenata, pues ok.\n",
            "[12:27.600 --> 12:31.480]  Igualmente para haceros la vida m√°s f√°cil he preparado un notebook que pod√©is utilizar,\n",
            "[12:31.480 --> 12:35.000]  est√° abajo en la cajita de descripci√≥n, donde ten√©is ya todo el c√≥digo listo para\n",
            "[12:35.000 --> 12:39.200]  empezar a trabajar, simplemente ten√©is que entrar, comprobar que est√° la GPU activada,\n",
            "[12:39.200 --> 12:43.080]  le damos a este bot√≥n de aqu√≠ para instalar pues todo lo necesario, aqu√≠ elegimos la\n",
            "[12:43.080 --> 12:47.680]  tarea que queremos hacer, pues si es transcribir a cualquier idioma o traducir al ingl√©s\n",
            "[12:47.680 --> 12:48.800]  y le damos a ejecutar.\n",
            "[12:48.800 --> 12:53.520]  En este caso la celda est√° preparada para que en el momento en el que empieces a ejecutarla\n",
            "[12:53.520 --> 12:57.080]  est√° grabando ahora mismo tu micr√≥fono, es decir ahora mismo estar√≠amos generando\n",
            "[12:57.080 --> 13:00.960]  un archivo de audio que luego vamos a utilizar para transcribir con Whisper, esto es por\n",
            "[13:00.960 --> 13:05.480]  si quer√©is hacer una transcripci√≥n en tiempo real de cualquier clase o cualquier cosa\n",
            "[13:05.480 --> 13:06.480]  que necesit√©is.\n",
            "[13:06.480 --> 13:10.800]  Vamos a darle a parar, le damos a este bot√≥n y en un momento tenemos el resultado de lo\n",
            "[13:10.800 --> 13:12.520]  que hemos dicho.\n",
            "[13:12.520 --> 13:16.800]  Igualmente luego abajo os a√±ado los dos comandos necesarios para poder transcribir o traducir\n",
            "[13:16.800 --> 13:19.240]  el audio que vosotros sub√°is.\n",
            "[13:19.240 --> 13:22.760]  Por √∫ltimo tambi√©n ten√©is que saber que si quer√©is algo m√°s sencillo pues hay p√°ginas\n",
            "[13:22.760 --> 13:27.240]  web donde pod√©is probar este sistema pues subiendo vuestros propios audios o grabando\n",
            "[13:27.240 --> 13:28.240]  desde el micr√≥fono.\n",
            "[13:28.240 --> 13:32.960]  Y esto ser√≠a, 2022 se est√° quedando la verdad que un a√±o espectacular en cuanto al n√∫mero\n",
            "[13:32.960 --> 13:37.360]  de juguetes neuronales que est√°n llegando a nuestras manos para construir un mont√≥n\n",
            "[13:37.360 --> 13:39.320]  de herramientas y para poder toquetearlos.\n",
            "[13:39.320 --> 13:41.640]  Ahora os toca a vosotros, ¬øqu√© pod√©is hacer con esto?\n",
            "[13:41.640 --> 13:45.080]  Pues pod√©is construir un mont√≥n de cosas s√∫per interesantes, pod√©is conectar por\n",
            "[13:45.080 --> 13:49.960]  ejemplo Whisper con Stable Diffusion para que a viva voz t√∫ le puedas pedir que te\n",
            "[13:49.960 --> 13:54.040]  genere un cuadro o pod√©is por ejemplo coger todas vuestras clases en la universidad o\n",
            "[13:54.040 --> 13:58.960]  todas las reuniones de trabajo, transcribirlas, crear un enorme banco de transcripciones y\n",
            "[13:58.960 --> 14:03.680]  luego con la API de GPT-3 hacer un chatbot que te permita consultar, hacer preguntas\n",
            "[14:03.680 --> 14:06.160]  y respuestas sobre toda esa fuente de informaci√≥n.\n",
            "[14:06.160 --> 14:10.040]  Por ejemplo algo que yo quiero hacer es coger pues todos los v√≠deos de mi canal de YouTube\n",
            "[14:10.040 --> 14:14.640]  y transcribirlo, generar subt√≠tulos de buena calidad tanto en espa√±ol como en ingl√©s\n",
            "[14:14.640 --> 14:18.920]  y poder hacer estad√≠sticas y consultas de cu√°ntas veces he dicho por ejemplo la palabra\n",
            "[14:18.920 --> 14:19.920]  Machine Learning.\n",
            "[14:19.920 --> 14:23.360]  Hay un mont√≥n de aplicaciones que pod√©is empezar a construir, que pod√©is empezar a\n",
            "[14:23.360 --> 14:27.160]  crear combinando todas estas tecnolog√≠as.\n",
            "[14:27.160 --> 14:29.880]  Ten√≠a un perro ladrando de fondo que me estaba molestando bastante.\n",
            "[14:29.880 --> 14:34.080]  Bueno, lo que os dec√≠a, que pod√©is crear un mont√≥n de cosas y hay mucho por hacer.\n",
            "[14:34.080 --> 14:37.560]  Desde aqu√≠, desde este canal vamos a seguir haciendo experimentos con esta tecnolog√≠a,\n",
            "[14:37.560 --> 14:42.320]  voy a seguir trayendo nuevas herramientas as√≠ que si no lo has hecho todav√≠a suscr√≠bete,\n",
            "[14:42.320 --> 14:46.000]  dale a la campanita para que te lleguen siempre las notificaciones de que hay v√≠deo nuevo\n",
            "[14:46.000 --> 14:50.440]  y si quieres apoyar todo este contenido ya sab√©is que pod√©is hacerlo a trav√©s de Patreon\n",
            "[14:50.440 --> 14:52.080]  abajo en la cajita de descripci√≥n.\n",
            "[14:52.080 --> 14:55.080]  Ten√©is un par de v√≠deos por aqu√≠ que son s√∫per interesantes, no s√© cu√°les son pero\n",
            "[14:55.080 --> 14:58.960]  son s√∫per interesantes, echadle un ojo y nos vemos con m√°s inteligencia artificial\n",
            "[14:58.960 --> 15:26.280]  chicos, chicas, en el pr√≥ximo v√≠deo.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Step 4] üíæ **Save results**\n",
        "\n",
        "Run this cell to write the transcription as a file output.\n",
        "\n",
        "Results will be available in the **audio_transcription** folder in the formats selected in `output_formats`.\n",
        "\n",
        "If you don't see that folder, you may need to refresh üîÑ the Files folder.\n",
        "\n",
        "Available formats: `txt,vtt,srt,tsv,json`"
      ],
      "metadata": {
        "id": "hTrxbUivk_h3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set output folder\n",
        "output_dir = \"audio_transcription\"\n",
        "\n",
        "# set output formats: https://github.com/openai/whisper/blob/7858aa9c08d98f75575035ecd6481f462d66ca27/whisper/utils.py#L145\n",
        "output_formats = \"txt,vtt,srt\" #@param [\"txt,vtt,srt,tsv,json\", \"txt,vtt,srt\", \"txt,vtt\", \"txt,srt\", \"txt\", \"vtt\", \"srt\", \"tsv\", \"json\"] {allow-input: true}\n",
        "output_formats = output_formats.split(',')\n",
        "\n",
        "def write_result(result, output_format, output_file_name):\n",
        "  output_format = output_format.strip()\n",
        "\n",
        "  # start captions in non-zero timestamp (some media players does not detect the first caption)\n",
        "  fix_vtt = output_format == 'vtt' and result[\"segments\"] and result[\"segments\"][0].get('start') == 0\n",
        "  \n",
        "  if fix_vtt:\n",
        "    result[\"segments\"][0]['start'] += 1/1000 # +1ms\n",
        "\n",
        "  # write result in the desired format\n",
        "  writer = get_writer(output_format, output_dir)\n",
        "  writer(result, output_file_name)\n",
        "\n",
        "  if fix_vtt:\n",
        "    result[\"segments\"][0]['start'] = 0 # reset change\n",
        "\n",
        "  output_file_path = os.path.join(output_dir, f\"{output_file_name}.{output_format}\")\n",
        "  print(output_file_path)\n",
        "\n",
        "# save results\n",
        "\n",
        "print(\"Writing results...\\n\")\n",
        "\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "for audio_path, result in results.items():\n",
        "  output_file_name = os.path.splitext(os.path.basename(audio_path))[0]\n",
        "\n",
        "  for output_format in output_formats:\n",
        "    write_result(result, output_format, output_file_name)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "wNsrB45_lCIl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02aa45e5-5da8-463c-acdd-64f5d43215c8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing results...\n",
            "\n",
            "audio_transcription/Whisper-Example.txt\n",
            "audio_transcription/Whisper-Example.vtt\n",
            "audio_transcription/Whisper-Example.srt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Step 5] üí¨ Translate results with DeepL (API key needed)\n",
        "\n",
        "This is an **optional** step to translate the transcription to another language using the **DeepL** API.\n",
        "\n",
        "[Get a DeepL Developer Account API Key](https://www.deepl.com/pro-api?cta=header-pro-api)\n",
        "\n",
        "Set the `deepl_api_key` to translate the transcription to a supported language in `deepl_target_language`."
      ],
      "metadata": {
        "id": "ZfkDhNMMvY8s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install dependencies\n",
        "import deepl\n",
        "\n",
        "# translation service options (DeepL Developer Account)\n",
        "\n",
        "deepl_api_key = \"\" #@param {type:\"string\"}\n",
        "deepl_target_language = \"English (British)\" #@param [\"\", \"Bulgarian\", \"Chinese (simplified)\", \"Czech\", \"Danish\", \"Dutch\", \"English (American)\", \"English (British)\", \"Estonian\", \"Finnish\", \"French\", \"German\", \"Greek\", \"Hungarian\", \"Indonesian\", \"Italian\", \"Japanese\", \"Latvian\", \"Lithuanian\", \"Polish\", \"Portuguese (Brazilian)\", \"Portuguese (European)\", \"Romanian\", \"Russian\", \"Slovak\", \"Slovenian\", \"Spanish\", \"Swedish\", \"Turkish\", \"Ukrainian\"]\n",
        "\n",
        "use_deepl_translation = deepl_api_key and deepl_target_language\n",
        "\n",
        "if not use_deepl_translation:\n",
        "  if not deepl_api_key:\n",
        "    print(\"Required: deepl_api_key\")\n",
        "    print(\"Get a DeepL Developer Account API Key: https://www.deepl.com/pro-api?cta=header-pro-api\")\n",
        "  if not deepl_target_language:\n",
        "    print(\"Required: deepl_target_language\")\n",
        "else:\n",
        "  translated_results = {} # audio_path to translated segments results\n",
        "\n",
        "  try:\n",
        "    deepl_translator = deepl.Translator(deepl_api_key)\n",
        "\n",
        "    deepl_source_languages = [lang.code.upper() for lang in deepl_translator.get_source_languages()]\n",
        "    \n",
        "    deepl_target_languages_dict = deepl_translator.get_target_languages()\n",
        "    deepl_target_languages = [lang.name for lang in deepl_target_languages_dict]\n",
        "\n",
        "    deepl_target_language_code = next(lang.code for lang in deepl_target_languages_dict if lang.name == deepl_target_language).upper()\n",
        "\n",
        "    source_language_code = whisper.tokenizer.TO_LANGUAGE_CODE.get(options['language'].lower()).upper()\n",
        "    target_language_code = deepl_target_language_code.split('-')[0]\n",
        "\n",
        "    if (task == 'translate' and target_language_code != 'EN') or (task == 'transcribe' and source_language_code in deepl_source_languages and source_language_code != target_language_code):\n",
        "      source_lang = source_language_code if task == 'transcribe' else None\n",
        "      translate_from = f\"from {options['language']} [{source_language_code}] \" if source_lang else ''\n",
        "      print(f\"DeepL: Translate results {translate_from}to {deepl_target_language} [{deepl_target_language_code}]\\n\")\n",
        "      \n",
        "      for audio_path, result in results.items():\n",
        "        deepl_usage = deepl_translator.get_usage()\n",
        "        \n",
        "        if deepl_usage.any_limit_reached:\n",
        "            print('DeepL: Translation limit reached.')\n",
        "            use_deepl_translation = False\n",
        "\n",
        "        # translate results (DeepL)\n",
        "        if use_deepl_translation:\n",
        "          print(audio_path + '\\n')\n",
        "\n",
        "          translated_results[audio_path] = { \"segments\": [] }\n",
        "\n",
        "          segments = result[\"segments\"]\n",
        "          deepl_batch_requests_size = 10\n",
        "          \n",
        "          for batch_segments in [segments[i:i + deepl_batch_requests_size] for i in range(0, len(segments), deepl_batch_requests_size)]:\n",
        "            deepl_results = deepl_translator.translate_text([segment['text'] for segment in batch_segments], source_lang=source_lang, target_lang=deepl_target_language_code, split_sentences='off')\n",
        "            \n",
        "            for j, deepl_result in enumerate(deepl_results):\n",
        "              segment = batch_segments[j]\n",
        "              translated_text = deepl_result.text\n",
        "              translated_results[audio_path][\"segments\"].append(dict(id=segment['id'], start=segment['start'], end=segment['end'], text=translated_text))\n",
        "\n",
        "              if options['verbose']:\n",
        "                print(f\"[{format_timestamp(segment['start'])} --> {format_timestamp(segment['end'])}] {translated_text}\")\n",
        "\n",
        "          deepl_usage = deepl_translator.get_usage()\n",
        "          \n",
        "          if deepl_usage.character.valid:\n",
        "            print(f\"\\nDeepL: Character usage: {deepl_usage.character.count} / {deepl_usage.character.limit} ({100*(deepl_usage.character.count/deepl_usage.character.limit):.1f}%)\\n\")\n",
        "    elif task == 'transcribe' and source_language_code not in deepl_source_languages:\n",
        "      print(f\"DeepL: {options['language']} is not yet supported\")\n",
        "  except deepl.DeepLException as e:\n",
        "    if isinstance(e, deepl.AuthorizationException) and str(e) == \"Authorization failure, check auth_key\":\n",
        "      e = \"Authorization failure, check deepl_api_key\"\n",
        "    print(f\"DeepL: [Error] {e}\")\n",
        "  \n",
        "  # save translated results (if any)\n",
        "\n",
        "  if translated_results:\n",
        "    print(\"Writing translated results...\\n\")\n",
        "\n",
        "    for audio_path, translated_result in translated_results.items():\n",
        "      output_file_name = os.path.splitext(os.path.basename(audio_path))[0]\n",
        "      translated_output_file_name = f\"{output_file_name}_{deepl_target_language}\"\n",
        "\n",
        "      for output_format in output_formats:\n",
        "        write_result(translated_result, output_format, translated_output_file_name)"
      ],
      "metadata": {
        "id": "28f7EIP-rez0",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1691100-d97c-4c23-d7b4-95f8bbabd518"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DeepL: Translate results from Spanish [ES] to English (British) [EN-GB]\n",
            "\n",
            "Whisper-Example.3gp\n",
            "\n",
            "[00:00.000 --> 00:06.040] 2022 will be remembered as the year of Stable Diffusion, of Dali 2, of incredible models.\n",
            "[00:06.040 --> 00:10.160] text generators such as Palm or code generators such as AlphaCode.\n",
            "[00:10.160 --> 00:13.920] And yet, chatting last month with Andr√©s Torrubia, he told me that the\n",
            "[00:13.920 --> 00:18.120] most interesting thing I'd seen this year was an artificial intelligence that was coming\n",
            "[00:18.120 --> 00:21.880] from the OpenAI lab, an AI called Whisper.\n",
            "[00:21.880 --> 00:26.880] What do you think is the most impressive thing that has come out this year?\n",
            "[00:26.880 --> 00:31.800] Well, oddly enough, look, oddly enough, so far Whisper, I think.\n",
            "[00:31.800 --> 00:32.800] Do you know why?\n",
            "[00:32.800 --> 00:33.800] Curious, eh.\n",
            "[00:33.800 --> 00:39.760] What impresses me about Whisper is that Whisper works, it's like, for me, Whisper, if it were\n",
            "[00:39.760 --> 00:46.880] the autonomous car, it would be the first self-driving car in the dictation, i.e. it is the first one that resembles\n",
            "[00:46.880 --> 00:47.880] to a person.\n",
            "[00:47.880 --> 00:51.000] Well, but in order for you to first understand what this Whisper thing is, I'm going to ask you to\n",
            "[00:51.000 --> 00:53.120] do the following exercise.\n",
            "[00:53.120 --> 00:57.800] I am going to play you an audio in English and your task is to transcribe each of the words.\n",
            "[00:57.800 --> 00:59.600] you are listening to.\n",
            "[00:59.600 --> 01:00.600] Are you ready?\n",
            "[01:00.600 --> 01:02.600] Three, two, one.\n",
            "[01:19.800 --> 01:21.280] Have you understood anything?\n",
            "[01:21.280 --> 01:22.760] Yeah, me neither.\n",
            "[01:22.760 --> 01:28.160] Well, in the ears of this artificial intelligence, this is the perfect transcription it has achieved.\n",
            "[01:28.160 --> 01:29.400] How is your Korean?\n",
            "[01:29.400 --> 01:33.680] Well, it's no problem for Whisper either, and he can also transcribe this audio at\n",
            "[01:33.680 --> 01:35.520] perfect English.\n",
            "[01:44.440 --> 01:46.080] And well, he understands me too.\n",
            "[01:46.080 --> 01:50.040] What you're seeing on the screen now is the speech to text that Whisper gets when it\n",
            "[01:50.040 --> 01:52.680] I pass on the audio track you are listening to.\n",
            "[01:52.680 --> 01:57.440] Look closely, not only does he get a near-perfect transcription, understanding even words\n",
            "[01:57.440 --> 02:02.760] such as Whisper or speech to text, but it is also capable of generating full stops, commas\n",
            "[02:02.760 --> 02:06.560] and other punctuation marks than many other commercial recognition models.\n",
            "[02:06.560 --> 02:08.360] of speech as he tends to choke on it.\n",
            "[02:08.360 --> 02:10.720] And this is very interesting.\n",
            "[02:10.720 --> 02:12.960] Well, not this, but Whisper.\n",
            "[02:12.960 --> 02:18.160] Whisper in general has a lot of interesting things and the first interesting thing is the context.\n",
            "[02:18.160 --> 02:20.120] in which this tool appears.\n",
            "[02:20.120 --> 02:23.640] After a year of amazing achievements by the Artificial Intelligence Lab\n",
            "[02:23.640 --> 02:29.680] OpenAI, suddenly out of the blue a collaborative initiative like Stability.ai\n",
            "[02:29.680 --> 02:34.320] which in September takes as its banner the open sourcing of many of the technologies that\n",
            "[02:34.320 --> 02:40.240] OpenAI for its part has decided to keep to itself and share only under services\n",
            "[02:40.240 --> 02:41.240] payment.\n",
            "[02:41.240 --> 02:46.360] This is not a problem for me either, because in the end OpenAI as a company has\n",
            "[02:46.360 --> 02:50.720] to pay their bills and at least it is giving us a way to access these powerful\n",
            "[02:50.720 --> 02:52.360] artificial intelligences.\n",
            "[02:52.360 --> 02:53.920] Learn Google.\n",
            "[02:53.920 --> 02:57.880] But of course, a new little boy arrives in town and starts giving out sweets to the\n",
            "[02:57.880 --> 03:01.920] children and all of a sudden the popular kid starts to be displaced.\n",
            "[03:01.920 --> 03:07.760] And at that very moment OpenAI comes out of nowhere and gives us Whisper for our benefit.\n",
            "[03:07.760 --> 03:08.760] of all.\n",
            "[03:08.760 --> 03:13.580] Because yes, my friends, this is open source, and I know you love to hear these words.\n",
            "[03:13.580 --> 03:17.160] At the end of the video I'm going to show you a mini tutorial so you can see how easy it is to use\n",
            "[03:17.160 --> 03:21.000] this tool and I'm also going to share a notebook to make it super easy for you to\n",
            "[03:21.000 --> 03:22.000] you.\n",
            "[03:22.000 --> 03:25.800] And this is what makes Whisper a super interesting tool, but it is not the only thing.\n",
            "[03:25.800 --> 03:29.800] And this is where one of the things that has caught my attention is that Whisper\n",
            "[03:29.800 --> 03:34.440] is not a complex system that has been designed to process audio in a way that has never been done before.\n",
            "[03:34.440 --> 03:38.640] made or a super-complex system with a lot of processing modules.\n",
            "[03:38.640 --> 03:45.840] No, Whisper is this here, a transformer neural network of 2017, it has no\n",
            "[03:45.840 --> 03:47.920] no change, nothing new.\n",
            "[03:47.920 --> 03:51.280] It is an architecture with which we are all familiar.\n",
            "[03:51.280 --> 03:55.800] So, if this is the case, why didn't a technology like Whisper already exist?\n",
            "[03:55.800 --> 04:00.800] Well, the key to what makes Whisper so powerful is in the data and how it has been used.\n",
            "[04:00.800 --> 04:02.920] structured their training.\n",
            "[04:02.920 --> 04:09.040] To train him, OpenAI has used no less than 680,000 hours of audio with his\n",
            "[04:09.040 --> 04:12.360] corresponding text, a brutality.\n",
            "[04:12.360 --> 04:17.200] And if you calculate 680,000 hours and start reproducing them now, you would end up with\n",
            "[04:17.200 --> 04:19.880] to listen to it 77 years from now.\n",
            "[04:19.880 --> 04:24.160] You could be sure that at some point in the sky you would see Halley's comet streaking across the sky.\n",
            "[04:24.160 --> 04:28.560] But what's more, a very interesting thing is that these audios come in multiple languages,\n",
            "[04:28.560 --> 04:32.200] allowing us to be able to train a model that is multilingual, that can understand us\n",
            "[04:32.200 --> 04:36.560] whether we speak to him in Spanish, English, Korean, it doesn't matter.\n",
            "[04:36.560 --> 04:38.240] But it doesn't stop there.\n",
            "[04:38.240 --> 04:43.720] Whisper is not only a multilingual system, but also a multitasking system.\n",
            "[04:43.720 --> 04:47.520] This is a trend that, as we saw in the video on Gato, in the world of deep\n",
            "[04:47.520 --> 04:49.760] learning is becoming more and more frequent.\n",
            "[04:49.760 --> 04:54.680] Do not train artificial intelligence for a single task, but train it for several tasks.\n",
            "[04:54.680 --> 04:59.560] different, thus making their learning much more solid and robust.\n",
            "[04:59.560 --> 05:04.560] As we have seen, Whisper can take audios in English and transcribe them into English, or\n",
            "[05:04.560 --> 05:06.960] audio in Korean and transcribe it into Korean.\n",
            "[05:06.960 --> 05:11.200] But the same model can also identify which language is being spoken, or acted upon.\n",
            "[05:11.200 --> 05:15.360] as a speech detector to classify when a piece of audio is not being listened to\n",
            "[05:15.360 --> 05:16.360] to a person.\n",
            "[05:16.360 --> 05:20.960] Or also, the task that I find most interesting of all, that you can talk to him or her.\n",
            "[05:20.960 --> 05:25.720] to Whisper in any language and it will automatically transcribe it into English for you.\n",
            "[05:25.720 --> 05:29.800] And in this case I can't tell you why, but for me this seems to me to be one of the most important functions.\n",
            "[05:29.800 --> 05:30.800] fascinating.\n",
            "[05:30.800 --> 05:32.880] It doesn't seem to offer us anything new either, does it?\n",
            "[05:32.880 --> 05:37.560] In the end you can take the text generated by any text transcriber in your language.\n",
            "[05:37.560 --> 05:39.520] and run it through a translator.\n",
            "[05:39.520 --> 05:43.520] But in this case, I find it fascinating to see how something as simple as a single\n",
            "[05:43.520 --> 05:47.880] deep learning model allows you to speak to it in any language and have it generate the text for you.\n",
            "[05:47.880 --> 05:51.520] in English without having to combine any tools.\n",
            "[05:51.520 --> 05:53.400] It's super simple.\n",
            "[05:53.400 --> 05:56.360] And the data we discussed earlier is also very interesting.\n",
            "[05:56.360 --> 06:00.480] Because my first intuition here is that OpenAI, in the search for a massive dataset\n",
            "[06:00.480 --> 06:05.280] of these 680,000 hours of audio to have a text transcription in order to be able to make\n",
            "[06:05.280 --> 06:09.800] this supervised apprenticeship, as he had possibly gone to one of the biggest sources of\n",
            "[06:09.800 --> 06:12.520] that we can find on the Internet, which is YouTube.\n",
            "[06:12.520 --> 06:16.960] In the end you know that all YouTube videos are automatically generated with subtitles.\n",
            "[06:16.960 --> 06:17.960] Well, no.\n",
            "[06:17.960 --> 06:22.800] This is precisely what OpenAI puts a lot of emphasis on in its paper to explain what they have done.\n",
            "[06:22.800 --> 06:28.200] a filtering process to remove from the dataset any occurrences of text generated by\n",
            "[06:28.200 --> 06:31.000] automatic speech recognition systems.\n",
            "[06:31.000 --> 06:32.000] Why?\n",
            "[06:32.000 --> 06:36.480] It was precisely to prevent Whisper from learning those defects, those vices, too.\n",
            "[06:36.480 --> 06:40.000] that other automatic systems may also have.\n",
            "[06:40.000 --> 06:44.600] That said, now that we're talking about Whisper and YouTube, there's a theory that\n",
            "[06:44.600 --> 06:48.520] I want to tell you that I think it's very interesting, it's nothing that is confirmed, but that\n",
            "[06:48.520 --> 06:53.560] could explain the reason for the existence of this tool and that it could have a certain relationship with the\n",
            "[06:53.560 --> 06:55.760] with a future GPT-4.\n",
            "[06:55.760 --> 06:59.720] This is an idea that I heard on Dr. Alan Thompson's channel that says that in\n",
            "[06:59.720 --> 07:05.600] the near future, where GPT-4 can begin training, Whisper could offer the system\n",
            "[07:05.600 --> 07:09.800] a huge source of data that previous systems had not been able to count on.\n",
            "[07:09.800 --> 07:14.640] Think of a system like GPT-3 as having been trained with a lot of Wikipedia articles,\n",
            "[07:14.640 --> 07:19.120] of books, of forums, of Internet conversations, but he has never been able to access all of the\n",
            "[07:19.120 --> 07:23.640] that spoken source that may be in databases such as YouTube.\n",
            "[07:23.640 --> 07:28.240] A tool such as Whisper could be used to sweep YouTube completely, transcribe\n",
            "[07:28.240 --> 07:33.200] many of their audios and get, unlock a new source of data that previously would not have\n",
            "[07:33.200 --> 07:37.400] It has been possible to use it to train a future language model.\n",
            "[07:37.400 --> 07:41.560] This is the enormous value of a tool like Whisper that I think makes it so interesting.\n",
            "[07:41.560 --> 07:42.560] to this technology.\n",
            "[07:42.560 --> 07:47.680] No, it does not solve a task that is spectacular, such as generating images or generating video, but it does\n",
            "[07:47.680 --> 07:52.280] solves a very useful task and almost solves it to perfection.\n",
            "[07:52.280 --> 07:57.640] I say almost, it's not perfect, sometimes some words are obviously wrong and it doesn't cover it.\n",
            "[07:57.640 --> 08:02.200] all the languages that exist on planet Earth, and well, to look for some limitation\n",
            "[08:02.200 --> 08:07.320] compared to other commercial tools, as it does not yet work in real time either.\n",
            "[08:07.320 --> 08:11.280] Depending on the length of the audio, it can take a few seconds to process it, sometimes\n",
            "[08:11.280 --> 08:17.080] It's a solid tool, it's mature, it's useful and it's open source,\n",
            "[08:17.080 --> 08:21.040] now allowing anyone to have access to a professional transcription tool.\n",
            "[08:21.040 --> 08:25.160] and text translation better than any free alternative.\n",
            "[08:25.160 --> 08:26.160] What?\n",
            "[08:26.160 --> 08:28.600] Oh, that you too would like to have access to this tool.\n",
            "[08:28.600 --> 08:32.720] Well, come on, I'll prepare an easy tutorial for all of you to use.\n",
            "[08:32.720 --> 08:37.640] We are going to do it in Google Colab, but first and taking advantage of the fact that we are talking about programming,\n",
            "[08:37.640 --> 08:41.880] of development, of innovation, let me remind you that there are very few days left\n",
            "[08:41.880 --> 08:46.880] for Samsung Dev Day, which is the technology event held every year, to be held at the\n",
            "[08:46.880 --> 08:51.760] year the Samsung Dev Spain community, which is Samsung's official community for developers.\n",
            "[08:51.760 --> 08:52.840] Spanish.\n",
            "[08:52.840 --> 08:55.560] This will be a free event not to be missed.\n",
            "[08:55.560 --> 09:00.640] If you are in Madrid you can attend in person on 16 November at the cloister of\n",
            "[09:00.640 --> 09:04.840] the Hieronymites of the Museo del Prado and if not, you can connect online at\n",
            "[09:04.840 --> 09:05.840] its streaming.\n",
            "[09:05.840 --> 09:09.760] But yes, you have to register, I was lucky enough to be able to participate last year with one of my own.\n",
            "[09:09.760 --> 09:14.280] presentation on code generation with artificial intelligence, and the experience was\n",
            "[09:14.280 --> 09:15.280] great.\n",
            "[09:15.280 --> 09:18.800] So you see, it's going to be an event full of great talks, talking about technology,\n",
            "[09:18.800 --> 09:23.280] of innovation, of applications, and it will also be presented by my dudev, who will surely\n",
            "[09:23.280 --> 09:26.560] Many of you will know him, so you can't miss him.\n",
            "[09:26.560 --> 09:30.320] I'll leave a link to the Samsung Dev website below in the description box.\n",
            "[09:30.320 --> 09:35.160] Spain, where you will find all the information regarding the agenda where you can register and a\n",
            "[09:35.160 --> 09:37.040] a lot of other resources.\n",
            "[09:37.040 --> 09:38.720] See you on 16 November.\n",
            "[09:38.720 --> 09:43.400] Let's see how we can use Whisper in our own code.\n",
            "[09:43.400 --> 09:47.240] For this we are going to use Google Colab, you know that Google is giving us here\n",
            "[09:47.240 --> 09:52.080] a free virtual machine that we can use and we will verify whenever we have\n",
            "[09:52.080 --> 09:56.560] enabled the GPU hardware accelerated environment type, OK, let's hit it here.\n",
            "[09:56.560 --> 10:01.320] GPU, let's hit save and now the first step will be to install Whisper.\n",
            "[10:01.320 --> 10:05.600] To do this we are going to use these two commands here, to install, you can find this here\n",
            "[10:05.600 --> 10:11.160] in Whisper's own GitHub repository, I'm going to leave you below in the little description box\n",
            "[10:11.160 --> 10:14.160] these commands, hit run and let it install.\n",
            "[10:14.160 --> 10:17.880] Once installed, we are going to upload some audio that we want to transcribe, in this case\n",
            "[10:17.880 --> 10:21.920] I'm going to try Rosal√≠a's song from Chicken Teriyaki, let's put it here,\n",
            "[10:21.920 --> 10:26.800] we drag it and now the next step we are going to take it here and we are going to put the command\n",
            "[10:26.800 --> 10:31.640] necessary to be able to run it, we're going to hit song.mp3 here, it's called the file that we\n",
            "[10:31.640 --> 10:37.680] we have uploaded, okay, song.mp3, so the task is going to be to transcribe the size of the model,\n",
            "[10:37.680 --> 10:42.560] there are different sizes depending on whether you want more speed when making the inference\n",
            "[10:42.560 --> 10:46.920] or if you want more precision in the results, I usually work with the Medium model.\n",
            "[10:46.920 --> 10:50.600] which is the one that gives me good results, there are bigger models, there are smaller models, try it.\n",
            "[10:50.600 --> 10:55.360] and in this case simply where we are going to place the output file, we run\n",
            "[10:55.360 --> 11:00.040] and that's it, that's it, no more to do, okay, we're already using Whisper,\n",
            "[11:00.040 --> 11:03.660] the first time it will take a little while because you have to download the model but after this\n",
            "[11:03.660 --> 11:08.520] At the moment you can use this system to transcribe any audio you want,\n",
            "[11:08.520 --> 11:13.640] mola, ok, we can see that in this case it has detected that the language is Spanish, it has made the inference\n",
            "[11:13.640 --> 11:16.800] automatic because we haven't told you that we are going to transcribe from Spanish, you can do it\n",
            "[11:16.800 --> 11:20.960] if you want and when this cell is already executed, we can come here, we'll see\n",
            "[11:20.960 --> 11:26.400] that the Audio Transcription folder has been generated and here we have the different options, we can\n",
            "[11:26.400 --> 11:32.360] open the sound.txt and here we open the file, we can see that we have the whole song perfectly.\n",
            "[11:32.360 --> 11:37.000] transcribed, which in this case, being Rosalia, has more merit and instead of wanting to\n",
            "[11:37.000 --> 11:41.680] transcription, you would like to make the translation, i.e. to convert your\n",
            "[11:41.680 --> 11:45.640] voice, your audio to English, so all you have to do is change here the\n",
            "[11:45.640 --> 11:51.480] task by Translate and in this case Whisper will work to translate what it has transcribed.\n",
            "[11:51.480 --> 11:54.880] In this case, if you notice, the command we have used is the console command\n",
            "[11:54.880 --> 11:58.480] but you may want to use Whisper within your code, then you can also use\n",
            "[11:58.480 --> 12:02.000] you have the option to work with Whisper's own library, it's simply this one\n",
            "[12:02.000 --> 12:05.960] line of code from here, import it, load the model we want, here then\n",
            "[12:05.960 --> 12:10.960] I would load the Medium model which is the one that, as I said, works best for my case, and with\n",
            "[12:10.960 --> 12:17.520] the loaded model then here we call model.transcribe, let's put here song.mp3, we hit run\n",
            "[12:17.520 --> 12:20.880] and in a matter of seconds we will have our transcript back.\n",
            "[12:20.880 --> 12:24.520] And here it is, the Rosalia, pink without a card, I send it to your cat, I have it for you.\n",
            "[12:24.520 --> 12:27.600] with roulette, no need for a serenade, ok.\n",
            "[12:27.600 --> 12:31.480] However, to make your life easier I have prepared a notebook that you can use,\n",
            "[12:31.480 --> 12:35.000] is below in the description box, where you already have all the code ready for\n",
            "[12:35.000 --> 12:39.200] to start working, just log in, check that the GPU is activated,\n",
            "[12:39.200 --> 12:43.080] click on this button here to install everything necessary, here we choose the\n",
            "[12:43.080 --> 12:47.680] task we want to do, whether it is transcribing into any language or translating into English.\n",
            "[12:47.680 --> 12:48.800] and click on run.\n",
            "[12:48.800 --> 12:53.520] In this case the cell is prepared so that the moment you start to run it\n",
            "[12:53.520 --> 12:57.080] your microphone is recording right now, i.e. right now we would be generating\n",
            "[12:57.080 --> 13:00.960] an audio file that we will then use for transcribing with Whisper, this is by\n",
            "[13:00.960 --> 13:05.480] if you want to make a real time transcript of any class or anything else\n",
            "[13:05.480 --> 13:06.480] you need.\n",
            "[13:06.480 --> 13:10.800] We're going to hit stop, we hit this button and in a moment we have the result of what we've done.\n",
            "[13:10.800 --> 13:12.520] we have said.\n",
            "[13:12.520 --> 13:16.800] Below you will find the two commands needed to be able to transcribe or translate\n",
            "[13:16.800 --> 13:19.240] the audio you upload.\n",
            "[13:19.240 --> 13:22.760] Finally, you should also know that if you want something simpler, then there are pages\n",
            "[13:22.760 --> 13:27.240] website where you can try out this system by uploading your own audios or by recording\n",
            "[13:27.240 --> 13:28.240] from the microphone.\n",
            "[13:28.240 --> 13:32.960] And this would be, 2022 is shaping up to be a spectacular year in terms of numbers.\n",
            "[13:32.960 --> 13:37.360] of neural toys that are coming into our hands to build a whole bunch\n",
            "[13:37.360 --> 13:39.320] and to be able to touch them.\n",
            "[13:39.320 --> 13:41.640] Now it's your turn, what can you do about it?\n",
            "[13:41.640 --> 13:45.080] Well, you can build a lot of super interesting things, you can connect by\n",
            "[13:45.080 --> 13:49.960] example Whisper with Stable Diffusion so that you can loudly ask it to\n",
            "[13:49.960 --> 13:54.040] generate a table or you can for example take all your classes at the university or\n",
            "[13:54.040 --> 13:58.960] all working meetings, transcribing them, creating a huge bank of transcripts and\n",
            "[13:58.960 --> 14:03.680] then with the GPT-3 API to make a chatbot that allows you to query, ask questions\n",
            "[14:03.680 --> 14:06.160] and answers on all these sources of information.\n",
            "[14:06.160 --> 14:10.040] For example, something I want to do is to take all the videos from my YouTube channel.\n",
            "[14:10.040 --> 14:14.640] and transcribe it, generate good quality subtitles in both Spanish and English\n",
            "[14:14.640 --> 14:18.920] and to be able to make statistics and queries on how many times I have said for example the word\n",
            "[14:18.920 --> 14:19.920] Machine Learning.\n",
            "[14:19.920 --> 14:23.360] There are a lot of applications that you can start to build, that you can start to build, that you can start to\n",
            "[14:23.360 --> 14:27.160] create by combining all these technologies.\n",
            "[14:27.160 --> 14:29.880] I had a dog barking in the background that was bothering me a lot.\n",
            "[14:29.880 --> 14:34.080] Well, as I was saying, you can create a lot of things and there is a lot to do.\n",
            "[14:34.080 --> 14:37.560] From here, from this channel, we will continue to experiment with this technology,\n",
            "[14:37.560 --> 14:42.320] I'll keep bringing you new tools so if you haven't done so yet subscribe,\n",
            "[14:42.320 --> 14:46.000] click on the little bell so that you always receive notifications of new videos\n",
            "[14:46.000 --> 14:50.440] and if you want to support all this content you know you can do so through Patreon\n",
            "[14:50.440 --> 14:52.080] below in the description box.\n",
            "[14:52.080 --> 14:55.080] You have a couple of videos around here that are super interesting, I don't know what they are but\n",
            "[14:55.080 --> 14:58.960] are super interesting, keep an eye on them and we'll see you with more artificial intelligence.\n",
            "[14:58.960 --> 15:26.280] guys, girls, in the next video.\n",
            "\n",
            "DeepL: Character usage: 16409 / 500000 (3.3%)\n",
            "\n",
            "Writing translated results...\n",
            "\n",
            "audio_transcription/Whisper-Example_English (British).txt\n",
            "audio_transcription/Whisper-Example_English (British).vtt\n",
            "audio_transcription/Whisper-Example_English (British).srt\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}