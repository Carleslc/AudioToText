{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Carleslc/AudioToText/blob/master/examples/spanish-to-english-deepl/spanish-to-english-deepl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5hvo8QWN-a9"
      },
      "source": [
        "# ðŸ—£ï¸ [**AudioToText**](https://github.com/Carleslc/AudioToText)\n",
        "\n",
        "### ðŸ›  [Whisper by OpenAI (GitHub)](https://github.com/openai/whisper)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Step 1] âš™ï¸ Install the required libraries\n",
        "\n",
        "Click â–¶ï¸ button below to install the dependencies for this notebook."
      ],
      "metadata": {
        "id": "U_lylR1xWMxk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title { display-mode: \"form\" }\n",
        "!apt install ffmpeg\n",
        "!pip install git+https://github.com/openai/whisper.git deepl"
      ],
      "metadata": {
        "id": "SJl7HJOeo0-P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c3215ed-3237-4acf-bfc4-62d5e4ea7868"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.2.7-0ubuntu0.1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-510\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 27 not upgraded.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-svlsnrdt\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-svlsnrdt\n",
            "  Resolved https://github.com/openai/whisper.git to commit 7858aa9c08d98f75575035ecd6481f462d66ca27\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting deepl\n",
            "  Downloading deepl-1.13.0-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from openai-whisper==20230124) (1.21.6)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from openai-whisper==20230124) (1.13.1+cu116)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from openai-whisper==20230124) (4.64.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.8/dist-packages (from openai-whisper==20230124) (9.0.0)\n",
            "Collecting transformers>=4.19.0\n",
            "  Downloading transformers-4.26.0-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpeg-python==0.2.0\n",
            "  Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.8/dist-packages (from ffmpeg-python==0.2.0->openai-whisper==20230124) (0.16.0)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.8/dist-packages (from deepl) (2.25.1)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2->deepl) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2->deepl) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2->deepl) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2->deepl) (1.24.3)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.12.0-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.19.0->openai-whisper==20230124) (23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.19.0->openai-whisper==20230124) (2022.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers>=4.19.0->openai-whisper==20230124) (3.9.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.19.0->openai-whisper==20230124) (6.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch->openai-whisper==20230124) (4.4.0)\n",
            "Building wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20230124-py3-none-any.whl size=1179424 sha256=f228d74afe75db4bb347b4e3b00f948f8c6826dda01153d36147c9f8b4ddd807\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-hez1_f7p/wheels/a7/70/18/b7693c07b1d18b3dafb328f5d0496aa0d41a9c09ef332fd8e6\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: tokenizers, ffmpeg-python, huggingface-hub, deepl, transformers, openai-whisper\n",
            "Successfully installed deepl-1.13.0 ffmpeg-python-0.2.0 huggingface-hub-0.12.0 openai-whisper-20230124 tokenizers-0.13.2 transformers-4.26.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Step 2] ðŸ“ Upload your audio files to the Files folder\n",
        "\n",
        "â¬…ï¸ Files folder in Google Colab is on the left menu\n",
        "\n",
        "Almost any audio or video file format is [supported](https://gist.github.com/Carleslc/1d6b922c8bf4a7e9627a6970d178b3a6)."
      ],
      "metadata": {
        "id": "5A5bTMB8XmtI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9_I0W3tqTjr"
      },
      "source": [
        "## [Step 3] ðŸ‘‚ Transcribe or Translate\n",
        "\n",
        "3.1. Choose a `task`:\n",
        "  - `Transcribe` speech to text in the same language of the source audio file.\n",
        "  - `Translate to English` speech to text in English.\n",
        "  \n",
        "Translation to other languages is not supported with _Whisper_ by default.\n",
        "You may try to choose the _Transcribe_ task and set your desired `language`, but translation is not guaranteed. However, you can use **_DeepL_** later in the Step 5 to translate the transcription to another language.\n",
        "\n",
        "3.2. Edit the `audio_file` to match your uploaded file name to transcribe.\n",
        "\n",
        "- If you want to transcribe multiple files with the same parameters you must separate their file names with commas `,`\n",
        "\n",
        "3.3. Run this cell and wait for the transcription to complete.\n",
        "\n",
        "  - You can try other parameters if the result with default parameters does not suit your needs.\n",
        "\n",
        "  If the execution takes too long to complete you can choose a smaller model in `use_model`, with an accuracy tradeoff.\n",
        "\n",
        "  [Available models and languages](https://github.com/openai/whisper#available-models-and-languages)\n",
        "\n",
        "  If the source audio file is entirely in English setting the `language` to English may provide better results when using a non-large model.\n",
        "  \n",
        "  More parameters are available in the code `options` object."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import modules\n",
        "\n",
        "import os\n",
        "\n",
        "import whisper\n",
        "from whisper.utils import format_timestamp, get_writer\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "try:\n",
        "  import tensorflow  # required in Colab to avoid protobuf compatibility issues\n",
        "except ImportError:\n",
        "  pass\n",
        "\n",
        "import torch\n",
        "\n",
        "# detect device\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "print(f\"Using {'GPU' if DEVICE == 'cuda' else 'CPU âš ï¸'}\")\n",
        "\n",
        "# https://medium.com/analytics-vidhya/the-google-colab-system-specification-check-69d159597417\n",
        "if DEVICE == \"cuda\":\n",
        "  !nvidia-smi -L\n",
        "else:\n",
        "  !lscpu | grep \"Model name\"\n",
        "  print(\"Not using GPU can result in a very slow execution\")\n",
        "  print(\"Ensure Hardware accelerator by GPU is enabled in Google Colab: Runtime > Change runtime type\")\n",
        "\n",
        "# select task\n",
        "\n",
        "task = \"Transcribe\" #@param [\"Transcribe\", \"Translate to English\"]\n",
        "\n",
        "task = \"transcribe\" if task == \"Transcribe\" else \"translate\"\n",
        "\n",
        "# select audio file\n",
        "\n",
        "audio_file = \"Whisper-Example.3gp\" #@param {type:\"string\"}\n",
        "\n",
        "audio_files = audio_file.split(',')\n",
        "\n",
        "# set model\n",
        "\n",
        "use_model = \"large-v2\" #@param [\"tiny\", \"base\", \"small\", \"medium\", \"large-v1\", \"large-v2\"]\n",
        "\n",
        "# select language\n",
        "\n",
        "WHISPER_LANGUAGES = [k.title() for k in whisper.tokenizer.TO_LANGUAGE_CODE.keys()]\n",
        "\n",
        "language = \"Auto-Detect\" #@param [\"Auto-Detect\", \"Afrikaans\", \"Albanian\", \"Amharic\", \"Arabic\", \"Armenian\", \"Assamese\", \"Azerbaijani\", \"Bashkir\", \"Basque\", \"Belarusian\", \"Bengali\", \"Bosnian\", \"Breton\", \"Bulgarian\", \"Burmese\", \"Castilian\", \"Catalan\", \"Chinese\", \"Croatian\", \"Czech\", \"Danish\", \"Dutch\", \"English\", \"Estonian\", \"Faroese\", \"Finnish\", \"Flemish\", \"French\", \"Galician\", \"Georgian\", \"German\", \"Greek\", \"Gujarati\", \"Haitian\", \"Haitian Creole\", \"Hausa\", \"Hawaiian\", \"Hebrew\", \"Hindi\", \"Hungarian\", \"Icelandic\", \"Indonesian\", \"Italian\", \"Japanese\", \"Javanese\", \"Kannada\", \"Kazakh\", \"Khmer\", \"Korean\", \"Lao\", \"Latin\", \"Latvian\", \"Letzeburgesch\", \"Lingala\", \"Lithuanian\", \"Luxembourgish\", \"Macedonian\", \"Malagasy\", \"Malay\", \"Malayalam\", \"Maltese\", \"Maori\", \"Marathi\", \"Moldavian\", \"Moldovan\", \"Mongolian\", \"Myanmar\", \"Nepali\", \"Norwegian\", \"Nynorsk\", \"Occitan\", \"Panjabi\", \"Pashto\", \"Persian\", \"Polish\", \"Portuguese\", \"Punjabi\", \"Pushto\", \"Romanian\", \"Russian\", \"Sanskrit\", \"Serbian\", \"Shona\", \"Sindhi\", \"Sinhala\", \"Sinhalese\", \"Slovak\", \"Slovenian\", \"Somali\", \"Spanish\", \"Sundanese\", \"Swahili\", \"Swedish\", \"Tagalog\", \"Tajik\", \"Tamil\", \"Tatar\", \"Telugu\", \"Thai\", \"Tibetan\", \"Turkish\", \"Turkmen\", \"Ukrainian\", \"Urdu\", \"Uzbek\", \"Valencian\", \"Vietnamese\", \"Welsh\", \"Yiddish\", \"Yoruba\"]\n",
        "\n",
        "if language == \"Auto-Detect\":\n",
        "  language = \"detect\"\n",
        "\n",
        "if language and language != \"detect\" and language not in WHISPER_LANGUAGES:\n",
        "  print(f\"Language '{language}' is invalid\")\n",
        "  language = \"detect\"\n",
        "\n",
        "if language and language != \"detect\":\n",
        "  print(f\"Language: {language}\\n\")\n",
        "\n",
        "# load model\n",
        "\n",
        "MODELS_WITH_ENGLISH_VERSION = [\"tiny\", \"base\", \"small\", \"medium\"]\n",
        "\n",
        "if language == \"English\" and use_model in MODELS_WITH_ENGLISH_VERSION:\n",
        "  use_model += \".en\"\n",
        "\n",
        "print(f\"\\nLoading {use_model} model...\")\n",
        "\n",
        "model = whisper.load_model(use_model, device=DEVICE)\n",
        "\n",
        "print(\n",
        "    f\"Model {use_model} is {'multilingual' if model.is_multilingual else 'English-only'} \"\n",
        "    f\"and has {sum(np.prod(p.shape) for p in model.parameters()):,d} parameters.\\n\"\n",
        ")\n",
        "\n",
        "# set options\n",
        "\n",
        "coherence_preference = \"More coherence, but may repeat text\" #@param [\"More coherence, but may repeat text\", \"Less repetitions, but may have less coherence\"]\n",
        "\n",
        "## Info: https://github.com/openai/whisper/blob/main/whisper/transcribe.py#L19\n",
        "options = {\n",
        "    'task': task,\n",
        "    'verbose': True,\n",
        "    'fp16': DEVICE == 'cuda',\n",
        "    'best_of': 5,\n",
        "    'beam_size': 5,\n",
        "    'patience': None,\n",
        "    'length_penalty': None,\n",
        "    'suppress_tokens': '-1',\n",
        "    'temperature': (0.0, 0.2, 0.4, 0.6, 0.8, 1.0),\n",
        "    'condition_on_previous_text': coherence_preference == \"More coherence, but may repeat text\",\n",
        "}\n",
        "\n",
        "if DEVICE == 'cpu':\n",
        "  torch.set_num_threads(os.cpu_count())\n",
        "\n",
        "# execute task\n",
        "# !whisper \"{audio_file}\" --task {task} --model {use_model} --output_dir {output_dir} --device {DEVICE} --verbose {options['verbose']}\n",
        "\n",
        "if task == \"translate\":\n",
        "  print(\"-- TRANSLATE TO ENGLISH --\\n\")\n",
        "else:\n",
        "  print(\"-- TRANSCRIPTION --\\n\")\n",
        "\n",
        "results = {} # audio_path to result\n",
        "\n",
        "for audio_path in audio_files:\n",
        "  print(f\"Processing: {audio_path}\")\n",
        "\n",
        "  # detect language\n",
        "  detect_language = not language or language == \"detect\"\n",
        "  if detect_language:\n",
        "    # load audio and pad/trim it to fit 30 seconds\n",
        "    audio = whisper.load_audio(audio_file)\n",
        "    audio = whisper.pad_or_trim(audio)\n",
        "\n",
        "    # make log-Mel spectrogram and move to the same device as the model\n",
        "    mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
        "\n",
        "    # detect the spoken language\n",
        "    _, probs = model.detect_language(mel)\n",
        "\n",
        "    language_code = max(probs, key=probs.get)\n",
        "    options['language'] = whisper.tokenizer.LANGUAGES[language_code].title()\n",
        "    \n",
        "    print(f\"Detected language: {options['language']}\")\n",
        "  else:\n",
        "    options['language'] = language\n",
        "\n",
        "  # transcribe\n",
        "  results[audio_path] = whisper.transcribe(model, audio_path, **options)"
      ],
      "metadata": {
        "id": "opNkn_Lgpat4",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ffcff09-d4e2-401f-8f5c-faf305fc1d47"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using GPU\n",
            "GPU 0: Tesla T4 (UUID: GPU-c31e48aa-4c4b-7720-afc9-48de26e1c511)\n",
            "\n",
            "Loading large-v2 model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.87G/2.87G [00:14<00:00, 212MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model large-v2 is multilingual and has 1,541,384,960 parameters.\n",
            "\n",
            "-- TRANSCRIPTION --\n",
            "\n",
            "Processing: Whisper-Example.3gp\n",
            "Detected language: Spanish\n",
            "[00:00.000 --> 00:06.040]  2022 serÃ¡ recordado como el aÃ±o de Stable Diffusion, de DalÃ­ 2, de increÃ­bles modelos\n",
            "[00:06.040 --> 00:10.160]  generadores de texto como Palm o generadores de cÃ³digo como AlphaCode.\n",
            "[00:10.160 --> 00:13.920]  Y sin embargo, charlando el mes pasado con AndrÃ©s Torrubia, Ã©l me comentaba que lo\n",
            "[00:13.920 --> 00:18.120]  mÃ¡s interesante que habÃ­a visto este aÃ±o era una inteligencia artificial que venÃ­a\n",
            "[00:18.120 --> 00:21.880]  del laboratorio OpenAI, una IA llamada Whisper.\n",
            "[00:21.880 --> 00:26.880]  Â¿QuÃ© es para ti de lo que ha salido este aÃ±o lo mÃ¡s impresionante?\n",
            "[00:26.880 --> 00:31.800]  Pues curiosamente, fÃ­jate, curiosamente, hasta ahora Whisper, yo creo.\n",
            "[00:31.800 --> 00:32.800]  Â¿Sabes por quÃ©?\n",
            "[00:32.800 --> 00:33.800]  Curioso, eh.\n",
            "[00:33.800 --> 00:39.760]  Por lo que me impresiona Whisper es que Whisper funciona, es como, para mÃ­, Whisper, si fuera\n",
            "[00:39.760 --> 00:46.880]  el coche autÃ³nomo, serÃ­a el primer self-driving del dictado, o sea, es el primero que se parece\n",
            "[00:46.880 --> 00:47.880]  a una persona.\n",
            "[00:47.880 --> 00:51.000]  Bueno, pero para que entiendas tÃº primero quÃ© es esto de Whisper, te voy a pedir que\n",
            "[00:51.000 --> 00:53.120]  hagas el siguiente ejercicio.\n",
            "[00:53.120 --> 00:57.800]  Te voy a reproducir un audio en inglÃ©s y tu tarea es transcribir cada una de las palabras\n",
            "[00:57.800 --> 00:59.600]  que estÃ©s escuchando.\n",
            "[00:59.600 --> 01:00.600]  Â¿EstÃ¡s listo?\n",
            "[01:00.600 --> 01:02.600]  Tres, dos, uno.\n",
            "[01:19.800 --> 01:21.280]  Â¿Has entendido algo?\n",
            "[01:21.280 --> 01:22.760]  Ya, yo tampoco.\n",
            "[01:22.760 --> 01:28.160]  Pues a oÃ­dos de esta inteligencia artificial, esta es la transcripciÃ³n perfecta que ha conseguido.\n",
            "[01:28.160 --> 01:29.400]  Â¿Y quÃ© tal tu coreano?\n",
            "[01:29.400 --> 01:33.680]  Bueno, pues para Whisper tampoco es problema y tambiÃ©n puede transcribir este audio en\n",
            "[01:33.680 --> 01:35.520]  perfecto inglÃ©s.\n",
            "[01:44.440 --> 01:46.080]  Y bueno, tambiÃ©n me entiende a mÃ­.\n",
            "[01:46.080 --> 01:50.040]  Esto que estÃ¡s viendo en pantalla ahora es el speech to text que consigue Whisper cuando\n",
            "[01:50.040 --> 01:52.680]  le paso la pista de audio que estÃ¡s escuchando.\n",
            "[01:52.680 --> 01:57.440]  FÃ­jate bien, no sÃ³lo consigue una transcripciÃ³n casi perfecta, entendiendo incluso palabras\n",
            "[01:57.440 --> 02:02.760]  concretas como Whisper o speech to text, sino que tambiÃ©n es capaz de generar puntos, comas\n",
            "[02:02.760 --> 02:06.560]  y otros signos de puntuaciÃ³n que a otros muchos modelos comerciales de reconocimiento\n",
            "[02:06.560 --> 02:08.360]  del habla pues se le suele atragantar.\n",
            "[02:08.360 --> 02:10.720]  Y esto es muy interesante.\n",
            "[02:10.720 --> 02:12.960]  Bueno, no esto, sino Whisper.\n",
            "[02:12.960 --> 02:18.160]  Whisper en general tiene muchas cosas interesantes y la primera cosa interesante es el contexto\n",
            "[02:18.160 --> 02:20.120]  en el que esta herramienta aparece.\n",
            "[02:20.120 --> 02:23.640]  Tras un aÃ±o de increÃ­bles logros por parte del Laboratorio de Inteligencia Artificial\n",
            "[02:23.640 --> 02:29.680]  de OpenAI, de repente de la nada surge una iniciativa colaborativa como Stability.ai\n",
            "[02:29.680 --> 02:34.320]  que en septiembre toma por bandera el hacer open source muchas de las tecnologÃ­as que\n",
            "[02:34.320 --> 02:40.240]  OpenAI por su parte pues ha decidido guardarse para sÃ­ y compartir sÃ³lo bajo servicios\n",
            "[02:40.240 --> 02:41.240]  de pago.\n",
            "[02:41.240 --> 02:46.360]  Para mÃ­ esto tampoco es un problema, puesto que al final OpenAI como empresa pues tiene\n",
            "[02:46.360 --> 02:50.720]  que pagar sus facturas y al menos nos estÃ¡ dando una forma de acceder a estas potentes\n",
            "[02:50.720 --> 02:52.360]  inteligencias artificiales.\n",
            "[02:52.360 --> 02:53.920]  Aprende Google.\n",
            "[02:53.920 --> 02:57.880]  Pero claro, llega un muchachito nuevo a la ciudad y empieza a regalar caramelos a los\n",
            "[02:57.880 --> 03:01.920]  niÃ±os y de repente el chico popular pues empieza a ver desplazado.\n",
            "[03:01.920 --> 03:07.760]  Y en ese preciso momento llega OpenAI de la nada y nos regala a Whisper para beneficio\n",
            "[03:07.760 --> 03:08.760]  de todos.\n",
            "[03:08.760 --> 03:13.580]  Porque sÃ­, amigos, esto es open source, que sÃ© que os encanta escuchar estas palabras.\n",
            "[03:13.580 --> 03:17.160]  Al final del vÃ­deo voy a enseÃ±ar un mini tutorial para que veÃ¡is quÃ© sencillo es utilizar\n",
            "[03:17.160 --> 03:21.000]  esta herramienta y tambiÃ©n os voy a compartir un notebook para que sea sÃºper sencillo para\n",
            "[03:21.000 --> 03:22.000]  vosotros.\n",
            "[03:22.000 --> 03:25.800]  Y esto es lo que hace a Whisper una herramienta sÃºper interesante, pero no es la Ãºnica cosa.\n",
            "[03:25.800 --> 03:29.800]  Y aquÃ­ es donde viene una de las cosas que mÃ¡s ha llamado mi atenciÃ³n y es que Whisper\n",
            "[03:29.800 --> 03:34.440]  no es un complejo sistema que hayan diseÃ±ado para procesar audio como nunca antes habÃ­a\n",
            "[03:34.440 --> 03:38.640]  hecho o un sistema sÃºper complejo con un montÃ³n de mÃ³dulos de procesamiento.\n",
            "[03:38.640 --> 03:45.840]  No, Whisper es esto de aquÃ­, una red neuronal de tipo transformer de las de 2017, no tiene\n",
            "[03:45.840 --> 03:47.920]  ningÃºn cambio, ninguna novedad.\n",
            "[03:47.920 --> 03:51.280]  Es una arquitectura que ya todos nosotros conocemos.\n",
            "[03:51.280 --> 03:55.800]  Entonces, si esto es asÃ­, por quÃ© no existÃ­a ya una tecnologÃ­a como Whisper?\n",
            "[03:55.800 --> 04:00.800]  Pues la clave que hace a Whisper algo tan potente estÃ¡ en los datos y en cÃ³mo han\n",
            "[04:00.800 --> 04:02.920]  estructurado su entrenamiento.\n",
            "[04:02.920 --> 04:09.040]  Para entrenarlo, OpenAI ha utilizado ni mÃ¡s ni menos que 680.000 horas de audio con su\n",
            "[04:09.040 --> 04:12.360]  correspondiente texto, una brutalidad.\n",
            "[04:12.360 --> 04:17.200]  Y es que si hacÃ©is el cÃ¡lculo 680.000 horas y empezar a reproducirlas ahora, acabarÃ­as\n",
            "[04:17.200 --> 04:19.880]  de escucharla dentro de 77 aÃ±os.\n",
            "[04:19.880 --> 04:24.160]  Te podrÃ­as asegurar que en algÃºn momento en el cielo verÃ­as surcar al cometa Halley.\n",
            "[04:24.160 --> 04:28.560]  Pero es que ademÃ¡s, una cosa muy interesante es que estos audios vienen en mÃºltiples idiomas,\n",
            "[04:28.560 --> 04:32.200]  permitiÃ©ndonos poder entrenar a un modelo que es multilinguaje, que puede entendernos\n",
            "[04:32.200 --> 04:36.560]  si le hablamos en espaÃ±ol, en inglÃ©s, en coreano, da igual.\n",
            "[04:36.560 --> 04:38.240]  Pero la cosa no se queda solo ahÃ­.\n",
            "[04:38.240 --> 04:43.720]  Y es que Whisper, ademÃ¡s de ser un sistema multilinguaje, tambiÃ©n es un sistema multitarea.\n",
            "[04:43.720 --> 04:47.520]  Esta es una tendencia que, como ya vimos en el vÃ­deo sobre Gato, en el mundo del deep\n",
            "[04:47.520 --> 04:49.760]  learning cada vez es mÃ¡s frecuente.\n",
            "[04:49.760 --> 04:54.680]  No entrenar a la inteligencia artificial para una Ãºnica tarea, sino entrenarla para varias\n",
            "[04:54.680 --> 04:59.560]  diferentes, haciendo asÃ­ que su aprendizaje sea mucho mÃ¡s sÃ³lido y robusto.\n",
            "[04:59.560 --> 05:04.560]  Como hemos visto, Whisper puede tomar audios en inglÃ©s y transcribirlos al inglÃ©s, o\n",
            "[05:04.560 --> 05:06.960]  audio en coreano y transcribirlo al coreano.\n",
            "[05:06.960 --> 05:11.200]  Pero el mismo modelo tambiÃ©n puede identificar quÃ© lenguaje se estÃ¡ hablando, o actuar\n",
            "[05:11.200 --> 05:15.360]  como un detector de voz para clasificar cuando en un trozo de audio se estÃ¡ escuchando no\n",
            "[05:15.360 --> 05:16.360]  a una persona.\n",
            "[05:16.360 --> 05:20.960]  O tambiÃ©n, la tarea que mÃ¡s interesante me parece de todas, que tÃº le puedas hablar\n",
            "[05:20.960 --> 05:25.720]  a Whisper en cualquier idioma y que Ã©l te lo transcriba automÃ¡ticamente al inglÃ©s.\n",
            "[05:25.720 --> 05:29.800]  Y en este caso no sabrÃ­a deciros por quÃ©, pero para mÃ­ esta me parece una funcionalidad\n",
            "[05:29.800 --> 05:30.800]  fascinante.\n",
            "[05:30.800 --> 05:32.880]  Parece que tampoco nos ofrece nada nuevo, Â¿no?\n",
            "[05:32.880 --> 05:37.560]  Al final tÃº puedes coger el texto que genera cualquier transcriptor de texto en tu idioma\n",
            "[05:37.560 --> 05:39.520]  y pasarlo por un traductor.\n",
            "[05:39.520 --> 05:43.520]  Pero en este caso me parece fascinante el ver cÃ³mo algo tan sencillo como un Ãºnico\n",
            "[05:43.520 --> 05:47.880]  modelo de deep learning te permite poder hablarle en cualquier idioma y que te genere el texto\n",
            "[05:47.880 --> 05:51.520]  en inglÃ©s sin tener que combinar ningÃºn tipo de herramientas.\n",
            "[05:51.520 --> 05:53.400]  Es sÃºper sencillo.\n",
            "[05:53.400 --> 05:56.360]  Y lo de los datos que hemos comentado antes tambiÃ©n es sÃºper interesante.\n",
            "[05:56.360 --> 06:00.480]  Porque mi primera intuiciÃ³n aquÃ­ es que OpenAI, en la bÃºsqueda de un dataset masivo\n",
            "[06:00.480 --> 06:05.280]  de estas 680.000 horas de audio que tuviera una transcripciÃ³n de texto para poder hacer\n",
            "[06:05.280 --> 06:09.800]  este aprendizaje supervisado, pues posiblemente habÃ­a acudido a una de las mayores fuentes\n",
            "[06:09.800 --> 06:12.520]  que podemos encontrar en Internet, que es YouTube.\n",
            "[06:12.520 --> 06:16.960]  Al final ya sabÃ©is que todos los vÃ­deos de YouTube tienen generados subtÃ­tulos automÃ¡ticamente.\n",
            "[06:16.960 --> 06:17.960]  Pues no.\n",
            "[06:17.960 --> 06:22.800]  Justamente en esto OpenAI hace mucho hincapiÃ© en su paper para explicarnos que han hecho\n",
            "[06:22.800 --> 06:28.200]  un proceso de filtrado para eliminar del dataset cualquier apariciÃ³n de texto generado por\n",
            "[06:28.200 --> 06:31.000]  sistemas automÃ¡ticos de reconocimiento del habla.\n",
            "[06:31.000 --> 06:32.000]  Â¿Por quÃ©?\n",
            "[06:32.000 --> 06:36.480]  Pues justamente para evitar que Whisper aprendiera tambiÃ©n aquellos defectos, aquellos vicios\n",
            "[06:36.480 --> 06:40.000]  que los otros sistemas automÃ¡ticos tambiÃ©n pudieran tener.\n",
            "[06:40.000 --> 06:44.600]  Dicho esto, ahora que estamos hablando de Whisper y de YouTube, hay una teorÃ­a que\n",
            "[06:44.600 --> 06:48.520]  quiero contaros que me parece muy interesante, no es nada que estÃ© confirmado, pero que\n",
            "[06:48.520 --> 06:53.560]  podrÃ­a explicar la razÃ³n de existir de esta herramienta y que podrÃ­a tener cierta relaciÃ³n\n",
            "[06:53.560 --> 06:55.760]  con un futuro GPT-4.\n",
            "[06:55.760 --> 06:59.720]  Esta es una idea que escuchÃ© en el canal del doctor Alan Thompson y que dice que en\n",
            "[06:59.720 --> 07:05.600]  un futuro prÃ³ximo, donde GPT-4 pueda empezar a entrenar, Whisper podrÃ­a ofrecer al sistema\n",
            "[07:05.600 --> 07:09.800]  una enorme fuente de datos con la que sistemas anteriores no habÃ­an contado.\n",
            "[07:09.800 --> 07:14.640]  Pensemos que un sistema como GPT-3 se ha entrenado con un montÃ³n de artÃ­culos de Wikipedia,\n",
            "[07:14.640 --> 07:19.120]  de libros, de foros, de conversaciones de Internet, pero nunca ha podido acceder a toda\n",
            "[07:19.120 --> 07:23.640]  esa fuente hablada que puede estar en bases de datos como YouTube.\n",
            "[07:23.640 --> 07:28.240]  Una herramienta como Whisper podrÃ­a ser utilizada para barrer por completo a YouTube, transcribir\n",
            "[07:28.240 --> 07:33.200]  muchos de sus audios y obtener, desbloquear una nueva fuente de datos que antes no habrÃ­a\n",
            "[07:33.200 --> 07:37.400]  sido posible utilizar para entrenar a un futuro modelo del lenguaje.\n",
            "[07:37.400 --> 07:41.560]  Este es el enorme valor que tiene una herramienta como Whisper y que creo que hace tan interesante\n",
            "[07:41.560 --> 07:42.560]  a esta tecnologÃ­a.\n",
            "[07:42.560 --> 07:47.680]  No, no resuelve una tarea que sea espectacular, como generar imÃ¡genes o generar vÃ­deo, pero\n",
            "[07:47.680 --> 07:52.280]  resuelve una tarea muy Ãºtil y casi la resuelve hasta la perfecciÃ³n.\n",
            "[07:52.280 --> 07:57.640]  Ojo, digo casi, no es perfecta, a veces algunas palabras se equivocan evidentemente y no cubre\n",
            "[07:57.640 --> 08:02.200]  todos los lenguajes que existen en el planeta Tierra y bueno, por buscar alguna limitaciÃ³n\n",
            "[08:02.200 --> 08:07.320]  frente a otras herramientas comerciales, pues tampoco funciona en tiempo real todavÃ­a.\n",
            "[08:07.320 --> 08:11.280]  Procesar el audio dependiendo de la longitud te puede llevar unos cuantos segundos, a veces\n",
            "[08:11.280 --> 08:17.080]  algÃºn minuto, pero es una herramienta sÃ³lida, es madura, es Ãºtil y ademÃ¡s open source,\n",
            "[08:17.080 --> 08:21.040]  permitiendo que ahora cualquiera pueda acceder a una herramienta profesional de transcripciÃ³n\n",
            "[08:21.040 --> 08:25.160]  y traducciÃ³n de texto mejor que cualquier alternativa gratis.\n",
            "[08:25.160 --> 08:26.160]  Â¿QuÃ©?\n",
            "[08:26.160 --> 08:28.600]  Ah, que tambiÃ©n vosotros querÃ©is acceder a esta herramienta.\n",
            "[08:28.600 --> 08:32.720]  Bueno, venga va, os preparo un tutorial facilito para que todos podÃ¡is utilizarlo.\n",
            "[08:32.720 --> 08:37.640]  Vamos a hacerlo en Google Colab, pero antes y aprovechando que estamos hablando de programaciÃ³n,\n",
            "[08:37.640 --> 08:41.880]  de desarrollo, de innovaciÃ³n, dejadme que os recuerde que quedan muy poquitos dÃ­as\n",
            "[08:41.880 --> 08:46.880]  para que se celebre el Samsung Dev Day, que es el evento tecnolÃ³gico que celebra cada\n",
            "[08:46.880 --> 08:51.760]  aÃ±o la comunidad de Samsung Dev Spain, que es la comunidad oficial de Samsung para desarrolladores\n",
            "[08:51.760 --> 08:52.840]  espaÃ±oles.\n",
            "[08:52.840 --> 08:55.560]  Este serÃ¡ un evento gratuito que no os podÃ©is perder.\n",
            "[08:55.560 --> 09:00.640]  Si estÃ¡is en Madrid podÃ©is asistir presencialmente el dÃ­a 16 de noviembre en el claustro de\n",
            "[09:00.640 --> 09:04.840]  los JerÃ³nimos del Museo del Prado y si no, pues podÃ©is conectaros online a travÃ©s de\n",
            "[09:04.840 --> 09:05.840]  su streaming.\n",
            "[09:05.840 --> 09:09.760]  Pero si, hay que registrarse, yo tuve la suerte el aÃ±o pasado de poder participar con una\n",
            "[09:09.760 --> 09:14.280]  ponencia sobre generaciÃ³n de cÃ³digo con inteligencia artificial y la experiencia fue\n",
            "[09:14.280 --> 09:15.280]  genial.\n",
            "[09:15.280 --> 09:18.800]  AsÃ­ que ya lo veis, serÃ¡ un evento cargado de charlas geniales, hablando de tecnologÃ­a,\n",
            "[09:18.800 --> 09:23.280]  de innovaciÃ³n, de aplicaciones y ademÃ¡s va a estar presentado por mi dudev, que seguramente\n",
            "[09:23.280 --> 09:26.560]  muchos de vosotros le conozcÃ¡is, asÃ­ que no os lo podÃ©is perder.\n",
            "[09:26.560 --> 09:30.320]  Os voy a dejar abajo en la cajita de descripciÃ³n un enlace a la pÃ¡gina web de Samsung Dev\n",
            "[09:30.320 --> 09:35.160]  Spain, donde vais a encontrar toda la informaciÃ³n respecto a la agenda donde registraros y un\n",
            "[09:35.160 --> 09:37.040]  montÃ³n de recursos mÃ¡s.\n",
            "[09:37.040 --> 09:38.720]  Nos vemos el 16 de noviembre.\n",
            "[09:38.720 --> 09:43.400]  Pues vamos a ver cÃ³mo podemos utilizar Whisper nosotros en nuestro propio cÃ³digo.\n",
            "[09:43.400 --> 09:47.240]  Para esto vamos a utilizar Google Colab, ya sabÃ©is que Google aquÃ­ nos estÃ¡ cediendo\n",
            "[09:47.240 --> 09:52.080]  una mÃ¡quina virtual gratuita que podemos utilizar y vamos a verificar siempre que tengamos\n",
            "[09:52.080 --> 09:56.560]  activado el tipo de entorno con aceleraciÃ³n por hardware GPU, vale, vamos a darle aquÃ­\n",
            "[09:56.560 --> 10:01.320]  GPU, vamos a darle a guardar y ahora el primer paso serÃ¡ instalar a Whisper.\n",
            "[10:01.320 --> 10:05.600]  Para ello vamos a usar estos dos comandos de aquÃ­, a instalar, esto lo podÃ©is encontrar\n",
            "[10:05.600 --> 10:11.160]  en el propio repositorio de GitHub de Whisper, os voy a dejar abajo en la cajita de descripciÃ³n\n",
            "[10:11.160 --> 10:14.160]  estos comandos, le damos a ejecutar y dejamos que se instale.\n",
            "[10:14.160 --> 10:17.880]  Una vez instalado vamos a subir algÃºn audio que queramos transcribir, yo en este caso\n",
            "[10:17.880 --> 10:21.920]  voy a probar con la canciÃ³n de RosalÃ­a de Chicken Teriyaki, vamos a colocarla para acÃ¡,\n",
            "[10:21.920 --> 10:26.800]  la arrastramos y ahora el siguiente paso pues vamos a coger aquÃ­ y vamos a poner el comando\n",
            "[10:26.800 --> 10:31.640]  necesario para poder ejecutarlo, vamos a darle aquÃ­ a song.mp3, se llama el archivo que\n",
            "[10:31.640 --> 10:37.680]  hemos subido, vale, song.mp3, la tarea va a ser pues transcribir el tamaÃ±o del modelo,\n",
            "[10:37.680 --> 10:42.560]  hay diferentes tamaÃ±os segÃºn si quieres mÃ¡s velocidad a la hora de hacer la inferencia\n",
            "[10:42.560 --> 10:46.920]  o si quieres mÃ¡s precisiÃ³n en los resultados, yo por lo general trabajo con el modelo Medium\n",
            "[10:46.920 --> 10:50.600]  que es el que me da buenos resultados, hay modelos mayores, hay modelos menores, probad\n",
            "[10:50.600 --> 10:55.360]  y en este caso pues simplemente donde vamos a colocar el archivo de salida, ejecutamos\n",
            "[10:55.360 --> 11:00.040]  y ya estÃ¡, ya estÃ¡, no hay que hacer nada mÃ¡s, vale, ya estamos utilizando Whisper,\n",
            "[11:00.040 --> 11:03.660]  la primera vez tardarÃ¡ un poco porque tiene que descargar el modelo pero a partir de este\n",
            "[11:03.660 --> 11:08.520]  momento podÃ©is utilizar este sistema para transcribir cualquier audio que querÃ¡is,\n",
            "[11:08.520 --> 11:13.640]  mola, vale, vemos que en este caso ha detectado que el idioma es espaÃ±ol, ha hecho la inferencia\n",
            "[11:13.640 --> 11:16.800]  automÃ¡tica porque no le hemos dicho que vamos a transcribir del espaÃ±ol, lo podÃ©is hacer\n",
            "[11:16.800 --> 11:20.960]  si querÃ©is y cuando ya estÃ¡ ejecutada esta celda pues podemos venirnos para acÃ¡, vemos\n",
            "[11:20.960 --> 11:26.400]  que se ha generado la carpeta Audio Transcription y aquÃ­ tenemos las diferentes opciones, podemos\n",
            "[11:26.400 --> 11:32.360]  abrir el sound.txt y aquÃ­ abrimos el archivo, vemos que pues tenemos toda la canciÃ³n perfectamente\n",
            "[11:32.360 --> 11:37.000]  transcrita que en este caso siendo la RosalÃ­a pues tiene mÃ¡s mÃ©rito y en vez de querer\n",
            "[11:37.000 --> 11:41.680]  hacer la transcripciÃ³n, quisierais hacer la traducciÃ³n, es decir convertir vuestra\n",
            "[11:41.680 --> 11:45.640]  voz, vuestro audio al inglÃ©s, pues lo Ãºnico que tenÃ©is que hacer es cambiar aquÃ­ la\n",
            "[11:45.640 --> 11:51.480]  tarea por Translate y en este caso Whisper trabajarÃ¡ para traducir aquello que ha transcrito.\n",
            "[11:51.480 --> 11:54.880]  En este caso si os dais cuenta el comando que hemos utilizado ha sido el de consola\n",
            "[11:54.880 --> 11:58.480]  pero a lo mejor querÃ©is utilizar Whisper dentro de vuestro cÃ³digo, entonces tambiÃ©n\n",
            "[11:58.480 --> 12:02.000]  tenÃ©is la opciÃ³n de trabajar con la propia librerÃ­a de Whisper, es simplemente esta\n",
            "[12:02.000 --> 12:05.960]  lÃ­nea de cÃ³digo de aquÃ­, lo importamos, cargamos el modelo que queramos, aquÃ­ pues\n",
            "[12:05.960 --> 12:10.960]  yo cargarÃ­a el modelo Medium que es el que como digo funciona mejor para mi caso y con\n",
            "[12:10.960 --> 12:17.520]  el modelo cargado luego aquÃ­ llamamos a model.transcribe, vamos a poner aquÃ­ song.mp3, le damos a ejecutar\n",
            "[12:17.520 --> 12:20.880]  y en cuestiÃ³n de unos segundos pues ya tendremos de nuevo nuestra transcripciÃ³n.\n",
            "[12:20.880 --> 12:24.520]  Y aquÃ­ lo tenemos, la RosalÃ­a, rosa sin tarjeta, se la mando a tu gata, te la tengo\n",
            "[12:24.520 --> 12:27.600]  con ruleta, no hizo falta serenata, pues ok.\n",
            "[12:27.600 --> 12:31.480]  Igualmente para haceros la vida mÃ¡s fÃ¡cil he preparado un notebook que podÃ©is utilizar,\n",
            "[12:31.480 --> 12:35.000]  estÃ¡ abajo en la cajita de descripciÃ³n, donde tenÃ©is ya todo el cÃ³digo listo para\n",
            "[12:35.000 --> 12:39.200]  empezar a trabajar, simplemente tenÃ©is que entrar, comprobar que estÃ¡ la GPU activada,\n",
            "[12:39.200 --> 12:43.080]  le damos a este botÃ³n de aquÃ­ para instalar pues todo lo necesario, aquÃ­ elegimos la\n",
            "[12:43.080 --> 12:47.680]  tarea que queremos hacer, pues si es transcribir a cualquier idioma o traducir al inglÃ©s\n",
            "[12:47.680 --> 12:48.800]  y le damos a ejecutar.\n",
            "[12:48.800 --> 12:53.520]  En este caso la celda estÃ¡ preparada para que en el momento en el que empieces a ejecutarla\n",
            "[12:53.520 --> 12:57.080]  estÃ¡ grabando ahora mismo tu micrÃ³fono, es decir ahora mismo estarÃ­amos generando\n",
            "[12:57.080 --> 13:00.960]  un archivo de audio que luego vamos a utilizar para transcribir con Whisper, esto es por\n",
            "[13:00.960 --> 13:05.480]  si querÃ©is hacer una transcripciÃ³n en tiempo real de cualquier clase o cualquier cosa\n",
            "[13:05.480 --> 13:06.480]  que necesitÃ©is.\n",
            "[13:06.480 --> 13:10.800]  Vamos a darle a parar, le damos a este botÃ³n y en un momento tenemos el resultado de lo\n",
            "[13:10.800 --> 13:12.520]  que hemos dicho.\n",
            "[13:12.520 --> 13:16.800]  Igualmente luego abajo os aÃ±ado los dos comandos necesarios para poder transcribir o traducir\n",
            "[13:16.800 --> 13:19.240]  el audio que vosotros subÃ¡is.\n",
            "[13:19.240 --> 13:22.760]  Por Ãºltimo tambiÃ©n tenÃ©is que saber que si querÃ©is algo mÃ¡s sencillo pues hay pÃ¡ginas\n",
            "[13:22.760 --> 13:27.240]  web donde podÃ©is probar este sistema pues subiendo vuestros propios audios o grabando\n",
            "[13:27.240 --> 13:28.240]  desde el micrÃ³fono.\n",
            "[13:28.240 --> 13:32.960]  Y esto serÃ­a, 2022 se estÃ¡ quedando la verdad que un aÃ±o espectacular en cuanto al nÃºmero\n",
            "[13:32.960 --> 13:37.360]  de juguetes neuronales que estÃ¡n llegando a nuestras manos para construir un montÃ³n\n",
            "[13:37.360 --> 13:39.320]  de herramientas y para poder toquetearlos.\n",
            "[13:39.320 --> 13:41.640]  Ahora os toca a vosotros, Â¿quÃ© podÃ©is hacer con esto?\n",
            "[13:41.640 --> 13:45.080]  Pues podÃ©is construir un montÃ³n de cosas sÃºper interesantes, podÃ©is conectar por\n",
            "[13:45.080 --> 13:49.960]  ejemplo Whisper con Stable Diffusion para que a viva voz tÃº le puedas pedir que te\n",
            "[13:49.960 --> 13:54.040]  genere un cuadro o podÃ©is por ejemplo coger todas vuestras clases en la universidad o\n",
            "[13:54.040 --> 13:58.960]  todas las reuniones de trabajo, transcribirlas, crear un enorme banco de transcripciones y\n",
            "[13:58.960 --> 14:03.680]  luego con la API de GPT-3 hacer un chatbot que te permita consultar, hacer preguntas\n",
            "[14:03.680 --> 14:06.160]  y respuestas sobre toda esa fuente de informaciÃ³n.\n",
            "[14:06.160 --> 14:10.040]  Por ejemplo algo que yo quiero hacer es coger pues todos los vÃ­deos de mi canal de YouTube\n",
            "[14:10.040 --> 14:14.640]  y transcribirlo, generar subtÃ­tulos de buena calidad tanto en espaÃ±ol como en inglÃ©s\n",
            "[14:14.640 --> 14:18.920]  y poder hacer estadÃ­sticas y consultas de cuÃ¡ntas veces he dicho por ejemplo la palabra\n",
            "[14:18.920 --> 14:19.920]  Machine Learning.\n",
            "[14:19.920 --> 14:23.360]  Hay un montÃ³n de aplicaciones que podÃ©is empezar a construir, que podÃ©is empezar a\n",
            "[14:23.360 --> 14:27.160]  crear combinando todas estas tecnologÃ­as.\n",
            "[14:27.160 --> 14:29.880]  TenÃ­a un perro ladrando de fondo que me estaba molestando bastante.\n",
            "[14:29.880 --> 14:34.080]  Bueno, lo que os decÃ­a, que podÃ©is crear un montÃ³n de cosas y hay mucho por hacer.\n",
            "[14:34.080 --> 14:37.560]  Desde aquÃ­, desde este canal vamos a seguir haciendo experimentos con esta tecnologÃ­a,\n",
            "[14:37.560 --> 14:42.320]  voy a seguir trayendo nuevas herramientas asÃ­ que si no lo has hecho todavÃ­a suscrÃ­bete,\n",
            "[14:42.320 --> 14:46.000]  dale a la campanita para que te lleguen siempre las notificaciones de que hay vÃ­deo nuevo\n",
            "[14:46.000 --> 14:50.440]  y si quieres apoyar todo este contenido ya sabÃ©is que podÃ©is hacerlo a travÃ©s de Patreon\n",
            "[14:50.440 --> 14:52.080]  abajo en la cajita de descripciÃ³n.\n",
            "[14:52.080 --> 14:55.080]  TenÃ©is un par de vÃ­deos por aquÃ­ que son sÃºper interesantes, no sÃ© cuÃ¡les son pero\n",
            "[14:55.080 --> 14:58.960]  son sÃºper interesantes, echadle un ojo y nos vemos con mÃ¡s inteligencia artificial\n",
            "[14:58.960 --> 15:26.280]  chicos, chicas, en el prÃ³ximo vÃ­deo.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Step 4] ðŸ’¾ **Save results**\n",
        "\n",
        "Run this cell to write the transcription as a file output.\n",
        "\n",
        "Results will be available in the **audio_transcription** folder in the formats selected in `output_formats`.\n",
        "\n",
        "If you don't see that folder, you may need to refresh ðŸ”„ the Files folder.\n",
        "\n",
        "Available formats: `txt,vtt,srt,tsv,json`"
      ],
      "metadata": {
        "id": "hTrxbUivk_h3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set output folder\n",
        "output_dir = \"audio_transcription\"\n",
        "\n",
        "# set output formats: https://github.com/openai/whisper/blob/7858aa9c08d98f75575035ecd6481f462d66ca27/whisper/utils.py#L145\n",
        "output_formats = \"txt,vtt,srt\" #@param [\"txt,vtt,srt,tsv,json\", \"txt,vtt,srt\", \"txt,vtt\", \"txt,srt\", \"txt\", \"vtt\", \"srt\", \"tsv\", \"json\"] {allow-input: true}\n",
        "output_formats = output_formats.split(',')\n",
        "\n",
        "def write_result(result, output_format, output_file_name):\n",
        "  output_format = output_format.strip()\n",
        "\n",
        "  # start captions in non-zero timestamp (some media players does not detect the first caption)\n",
        "  fix_vtt = output_format == 'vtt' and result[\"segments\"] and result[\"segments\"][0].get('start') == 0\n",
        "  \n",
        "  if fix_vtt:\n",
        "    result[\"segments\"][0]['start'] += 1/1000 # +1ms\n",
        "\n",
        "  # write result in the desired format\n",
        "  writer = get_writer(output_format, output_dir)\n",
        "  writer(result, output_file_name)\n",
        "\n",
        "  if fix_vtt:\n",
        "    result[\"segments\"][0]['start'] = 0 # reset change\n",
        "\n",
        "  output_file_path = os.path.join(output_dir, f\"{output_file_name}.{output_format}\")\n",
        "  print(output_file_path)\n",
        "\n",
        "# save results\n",
        "\n",
        "print(\"Writing results...\\n\")\n",
        "\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "for audio_path, result in results.items():\n",
        "  output_file_name = os.path.splitext(os.path.basename(audio_path))[0]\n",
        "\n",
        "  for output_format in output_formats:\n",
        "    write_result(result, output_format, output_file_name)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "wNsrB45_lCIl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02aa45e5-5da8-463c-acdd-64f5d43215c8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing results...\n",
            "\n",
            "audio_transcription/Whisper-Example.txt\n",
            "audio_transcription/Whisper-Example.vtt\n",
            "audio_transcription/Whisper-Example.srt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Step 5] ðŸ’¬ Translate results with DeepL (API key needed)\n",
        "\n",
        "This is an **optional** step to translate the transcription to another language using the **DeepL** API.\n",
        "\n",
        "[Get a DeepL Developer Account API Key](https://www.deepl.com/pro-api?cta=header-pro-api)\n",
        "\n",
        "Set the `deepl_api_key` to translate the transcription to a supported language in `deepl_target_language`."
      ],
      "metadata": {
        "id": "ZfkDhNMMvY8s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install dependencies\n",
        "import deepl\n",
        "\n",
        "# translation service options (DeepL Developer Account)\n",
        "\n",
        "deepl_api_key = \"\" #@param {type:\"string\"}\n",
        "deepl_target_language = \"English (British)\" #@param [\"\", \"Bulgarian\", \"Chinese (simplified)\", \"Czech\", \"Danish\", \"Dutch\", \"English (American)\", \"English (British)\", \"Estonian\", \"Finnish\", \"French\", \"German\", \"Greek\", \"Hungarian\", \"Indonesian\", \"Italian\", \"Japanese\", \"Latvian\", \"Lithuanian\", \"Polish\", \"Portuguese (Brazilian)\", \"Portuguese (European)\", \"Romanian\", \"Russian\", \"Slovak\", \"Slovenian\", \"Spanish\", \"Swedish\", \"Turkish\", \"Ukrainian\"]\n",
        "\n",
        "use_deepl_translation = deepl_api_key and deepl_target_language\n",
        "\n",
        "if not use_deepl_translation:\n",
        "  if not deepl_api_key:\n",
        "    print(\"Required: deepl_api_key\")\n",
        "    print(\"Get a DeepL Developer Account API Key: https://www.deepl.com/pro-api?cta=header-pro-api\")\n",
        "  if not deepl_target_language:\n",
        "    print(\"Required: deepl_target_language\")\n",
        "else:\n",
        "  translated_results = {} # audio_path to translated segments results\n",
        "\n",
        "  try:\n",
        "    deepl_translator = deepl.Translator(deepl_api_key)\n",
        "\n",
        "    deepl_source_languages = [lang.code.upper() for lang in deepl_translator.get_source_languages()]\n",
        "    \n",
        "    deepl_target_languages_dict = deepl_translator.get_target_languages()\n",
        "    deepl_target_languages = [lang.name for lang in deepl_target_languages_dict]\n",
        "\n",
        "    deepl_target_language_code = next(lang.code for lang in deepl_target_languages_dict if lang.name == deepl_target_language).upper()\n",
        "\n",
        "    source_language_code = whisper.tokenizer.TO_LANGUAGE_CODE.get(options['language'].lower()).upper()\n",
        "    target_language_code = deepl_target_language_code.split('-')[0]\n",
        "\n",
        "    if (task == 'translate' and target_language_code != 'EN') or (task == 'transcribe' and source_language_code in deepl_source_languages and source_language_code != target_language_code):\n",
        "      source_lang = source_language_code if task == 'transcribe' else None\n",
        "      translate_from = f\"from {options['language']} [{source_language_code}] \" if source_lang else ''\n",
        "      print(f\"DeepL: Translate results {translate_from}to {deepl_target_language} [{deepl_target_language_code}]\\n\")\n",
        "      \n",
        "      for audio_path, result in results.items():\n",
        "        deepl_usage = deepl_translator.get_usage()\n",
        "        \n",
        "        if deepl_usage.any_limit_reached:\n",
        "            print('DeepL: Translation limit reached.')\n",
        "            use_deepl_translation = False\n",
        "\n",
        "        # translate results (DeepL)\n",
        "        if use_deepl_translation:\n",
        "          print(audio_path + '\\n')\n",
        "\n",
        "          translated_results[audio_path] = { \"segments\": [] }\n",
        "\n",
        "          segments = result[\"segments\"]\n",
        "          deepl_batch_requests_size = 10\n",
        "          \n",
        "          for batch_segments in [segments[i:i + deepl_batch_requests_size] for i in range(0, len(segments), deepl_batch_requests_size)]:\n",
        "            deepl_results = deepl_translator.translate_text([segment['text'] for segment in batch_segments], source_lang=source_lang, target_lang=deepl_target_language_code, split_sentences='off')\n",
        "            \n",
        "            for j, deepl_result in enumerate(deepl_results):\n",
        "              segment = batch_segments[j]\n",
        "              translated_text = deepl_result.text\n",
        "              translated_results[audio_path][\"segments\"].append(dict(id=segment['id'], start=segment['start'], end=segment['end'], text=translated_text))\n",
        "\n",
        "              if options['verbose']:\n",
        "                print(f\"[{format_timestamp(segment['start'])} --> {format_timestamp(segment['end'])}] {translated_text}\")\n",
        "\n",
        "          deepl_usage = deepl_translator.get_usage()\n",
        "          \n",
        "          if deepl_usage.character.valid:\n",
        "            print(f\"\\nDeepL: Character usage: {deepl_usage.character.count} / {deepl_usage.character.limit} ({100*(deepl_usage.character.count/deepl_usage.character.limit):.1f}%)\\n\")\n",
        "    elif task == 'transcribe' and source_language_code not in deepl_source_languages:\n",
        "      print(f\"DeepL: {options['language']} is not yet supported\")\n",
        "  except deepl.DeepLException as e:\n",
        "    if isinstance(e, deepl.AuthorizationException) and str(e) == \"Authorization failure, check auth_key\":\n",
        "      e = \"Authorization failure, check deepl_api_key\"\n",
        "    print(f\"DeepL: [Error] {e}\")\n",
        "  \n",
        "  # save translated results (if any)\n",
        "\n",
        "  if translated_results:\n",
        "    print(\"Writing translated results...\\n\")\n",
        "\n",
        "    for audio_path, translated_result in translated_results.items():\n",
        "      output_file_name = os.path.splitext(os.path.basename(audio_path))[0]\n",
        "      translated_output_file_name = f\"{output_file_name}_{deepl_target_language}\"\n",
        "\n",
        "      for output_format in output_formats:\n",
        "        write_result(translated_result, output_format, translated_output_file_name)"
      ],
      "metadata": {
        "id": "28f7EIP-rez0",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1691100-d97c-4c23-d7b4-95f8bbabd518"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DeepL: Translate results from Spanish [ES] to English (British) [EN-GB]\n",
            "\n",
            "Whisper-Example.3gp\n",
            "\n",
            "[00:00.000 --> 00:06.040] 2022 will be remembered as the year of Stable Diffusion, of Dali 2, of incredible models.\n",
            "[00:06.040 --> 00:10.160] text generators such as Palm or code generators such as AlphaCode.\n",
            "[00:10.160 --> 00:13.920] And yet, chatting last month with AndrÃ©s Torrubia, he told me that the\n",
            "[00:13.920 --> 00:18.120] most interesting thing I'd seen this year was an artificial intelligence that was coming\n",
            "[00:18.120 --> 00:21.880] from the OpenAI lab, an AI called Whisper.\n",
            "[00:21.880 --> 00:26.880] What do you think is the most impressive thing that has come out this year?\n",
            "[00:26.880 --> 00:31.800] Well, oddly enough, look, oddly enough, so far Whisper, I think.\n",
            "[00:31.800 --> 00:32.800] Do you know why?\n",
            "[00:32.800 --> 00:33.800] Curious, eh.\n",
            "[00:33.800 --> 00:39.760] What impresses me about Whisper is that Whisper works, it's like, for me, Whisper, if it were\n",
            "[00:39.760 --> 00:46.880] the autonomous car, it would be the first self-driving car in the dictation, i.e. it is the first one that resembles\n",
            "[00:46.880 --> 00:47.880] to a person.\n",
            "[00:47.880 --> 00:51.000] Well, but in order for you to first understand what this Whisper thing is, I'm going to ask you to\n",
            "[00:51.000 --> 00:53.120] do the following exercise.\n",
            "[00:53.120 --> 00:57.800] I am going to play you an audio in English and your task is to transcribe each of the words.\n",
            "[00:57.800 --> 00:59.600] you are listening to.\n",
            "[00:59.600 --> 01:00.600] Are you ready?\n",
            "[01:00.600 --> 01:02.600] Three, two, one.\n",
            "[01:19.800 --> 01:21.280] Have you understood anything?\n",
            "[01:21.280 --> 01:22.760] Yeah, me neither.\n",
            "[01:22.760 --> 01:28.160] Well, in the ears of this artificial intelligence, this is the perfect transcription it has achieved.\n",
            "[01:28.160 --> 01:29.400] How is your Korean?\n",
            "[01:29.400 --> 01:33.680] Well, it's no problem for Whisper either, and he can also transcribe this audio at\n",
            "[01:33.680 --> 01:35.520] perfect English.\n",
            "[01:44.440 --> 01:46.080] And well, he understands me too.\n",
            "[01:46.080 --> 01:50.040] What you're seeing on the screen now is the speech to text that Whisper gets when it\n",
            "[01:50.040 --> 01:52.680] I pass on the audio track you are listening to.\n",
            "[01:52.680 --> 01:57.440] Look closely, not only does he get a near-perfect transcription, understanding even words\n",
            "[01:57.440 --> 02:02.760] such as Whisper or speech to text, but it is also capable of generating full stops, commas\n",
            "[02:02.760 --> 02:06.560] and other punctuation marks than many other commercial recognition models.\n",
            "[02:06.560 --> 02:08.360] of speech as he tends to choke on it.\n",
            "[02:08.360 --> 02:10.720] And this is very interesting.\n",
            "[02:10.720 --> 02:12.960] Well, not this, but Whisper.\n",
            "[02:12.960 --> 02:18.160] Whisper in general has a lot of interesting things and the first interesting thing is the context.\n",
            "[02:18.160 --> 02:20.120] in which this tool appears.\n",
            "[02:20.120 --> 02:23.640] After a year of amazing achievements by the Artificial Intelligence Lab\n",
            "[02:23.640 --> 02:29.680] OpenAI, suddenly out of the blue a collaborative initiative like Stability.ai\n",
            "[02:29.680 --> 02:34.320] which in September takes as its banner the open sourcing of many of the technologies that\n",
            "[02:34.320 --> 02:40.240] OpenAI for its part has decided to keep to itself and share only under services\n",
            "[02:40.240 --> 02:41.240] payment.\n",
            "[02:41.240 --> 02:46.360] This is not a problem for me either, because in the end OpenAI as a company has\n",
            "[02:46.360 --> 02:50.720] to pay their bills and at least it is giving us a way to access these powerful\n",
            "[02:50.720 --> 02:52.360] artificial intelligences.\n",
            "[02:52.360 --> 02:53.920] Learn Google.\n",
            "[02:53.920 --> 02:57.880] But of course, a new little boy arrives in town and starts giving out sweets to the\n",
            "[02:57.880 --> 03:01.920] children and all of a sudden the popular kid starts to be displaced.\n",
            "[03:01.920 --> 03:07.760] And at that very moment OpenAI comes out of nowhere and gives us Whisper for our benefit.\n",
            "[03:07.760 --> 03:08.760] of all.\n",
            "[03:08.760 --> 03:13.580] Because yes, my friends, this is open source, and I know you love to hear these words.\n",
            "[03:13.580 --> 03:17.160] At the end of the video I'm going to show you a mini tutorial so you can see how easy it is to use\n",
            "[03:17.160 --> 03:21.000] this tool and I'm also going to share a notebook to make it super easy for you to\n",
            "[03:21.000 --> 03:22.000] you.\n",
            "[03:22.000 --> 03:25.800] And this is what makes Whisper a super interesting tool, but it is not the only thing.\n",
            "[03:25.800 --> 03:29.800] And this is where one of the things that has caught my attention is that Whisper\n",
            "[03:29.800 --> 03:34.440] is not a complex system that has been designed to process audio in a way that has never been done before.\n",
            "[03:34.440 --> 03:38.640] made or a super-complex system with a lot of processing modules.\n",
            "[03:38.640 --> 03:45.840] No, Whisper is this here, a transformer neural network of 2017, it has no\n",
            "[03:45.840 --> 03:47.920] no change, nothing new.\n",
            "[03:47.920 --> 03:51.280] It is an architecture with which we are all familiar.\n",
            "[03:51.280 --> 03:55.800] So, if this is the case, why didn't a technology like Whisper already exist?\n",
            "[03:55.800 --> 04:00.800] Well, the key to what makes Whisper so powerful is in the data and how it has been used.\n",
            "[04:00.800 --> 04:02.920] structured their training.\n",
            "[04:02.920 --> 04:09.040] To train him, OpenAI has used no less than 680,000 hours of audio with his\n",
            "[04:09.040 --> 04:12.360] corresponding text, a brutality.\n",
            "[04:12.360 --> 04:17.200] And if you calculate 680,000 hours and start reproducing them now, you would end up with\n",
            "[04:17.200 --> 04:19.880] to listen to it 77 years from now.\n",
            "[04:19.880 --> 04:24.160] You could be sure that at some point in the sky you would see Halley's comet streaking across the sky.\n",
            "[04:24.160 --> 04:28.560] But what's more, a very interesting thing is that these audios come in multiple languages,\n",
            "[04:28.560 --> 04:32.200] allowing us to be able to train a model that is multilingual, that can understand us\n",
            "[04:32.200 --> 04:36.560] whether we speak to him in Spanish, English, Korean, it doesn't matter.\n",
            "[04:36.560 --> 04:38.240] But it doesn't stop there.\n",
            "[04:38.240 --> 04:43.720] Whisper is not only a multilingual system, but also a multitasking system.\n",
            "[04:43.720 --> 04:47.520] This is a trend that, as we saw in the video on Gato, in the world of deep\n",
            "[04:47.520 --> 04:49.760] learning is becoming more and more frequent.\n",
            "[04:49.760 --> 04:54.680] Do not train artificial intelligence for a single task, but train it for several tasks.\n",
            "[04:54.680 --> 04:59.560] different, thus making their learning much more solid and robust.\n",
            "[04:59.560 --> 05:04.560] As we have seen, Whisper can take audios in English and transcribe them into English, or\n",
            "[05:04.560 --> 05:06.960] audio in Korean and transcribe it into Korean.\n",
            "[05:06.960 --> 05:11.200] But the same model can also identify which language is being spoken, or acted upon.\n",
            "[05:11.200 --> 05:15.360] as a speech detector to classify when a piece of audio is not being listened to\n",
            "[05:15.360 --> 05:16.360] to a person.\n",
            "[05:16.360 --> 05:20.960] Or also, the task that I find most interesting of all, that you can talk to him or her.\n",
            "[05:20.960 --> 05:25.720] to Whisper in any language and it will automatically transcribe it into English for you.\n",
            "[05:25.720 --> 05:29.800] And in this case I can't tell you why, but for me this seems to me to be one of the most important functions.\n",
            "[05:29.800 --> 05:30.800] fascinating.\n",
            "[05:30.800 --> 05:32.880] It doesn't seem to offer us anything new either, does it?\n",
            "[05:32.880 --> 05:37.560] In the end you can take the text generated by any text transcriber in your language.\n",
            "[05:37.560 --> 05:39.520] and run it through a translator.\n",
            "[05:39.520 --> 05:43.520] But in this case, I find it fascinating to see how something as simple as a single\n",
            "[05:43.520 --> 05:47.880] deep learning model allows you to speak to it in any language and have it generate the text for you.\n",
            "[05:47.880 --> 05:51.520] in English without having to combine any tools.\n",
            "[05:51.520 --> 05:53.400] It's super simple.\n",
            "[05:53.400 --> 05:56.360] And the data we discussed earlier is also very interesting.\n",
            "[05:56.360 --> 06:00.480] Because my first intuition here is that OpenAI, in the search for a massive dataset\n",
            "[06:00.480 --> 06:05.280] of these 680,000 hours of audio to have a text transcription in order to be able to make\n",
            "[06:05.280 --> 06:09.800] this supervised apprenticeship, as he had possibly gone to one of the biggest sources of\n",
            "[06:09.800 --> 06:12.520] that we can find on the Internet, which is YouTube.\n",
            "[06:12.520 --> 06:16.960] In the end you know that all YouTube videos are automatically generated with subtitles.\n",
            "[06:16.960 --> 06:17.960] Well, no.\n",
            "[06:17.960 --> 06:22.800] This is precisely what OpenAI puts a lot of emphasis on in its paper to explain what they have done.\n",
            "[06:22.800 --> 06:28.200] a filtering process to remove from the dataset any occurrences of text generated by\n",
            "[06:28.200 --> 06:31.000] automatic speech recognition systems.\n",
            "[06:31.000 --> 06:32.000] Why?\n",
            "[06:32.000 --> 06:36.480] It was precisely to prevent Whisper from learning those defects, those vices, too.\n",
            "[06:36.480 --> 06:40.000] that other automatic systems may also have.\n",
            "[06:40.000 --> 06:44.600] That said, now that we're talking about Whisper and YouTube, there's a theory that\n",
            "[06:44.600 --> 06:48.520] I want to tell you that I think it's very interesting, it's nothing that is confirmed, but that\n",
            "[06:48.520 --> 06:53.560] could explain the reason for the existence of this tool and that it could have a certain relationship with the\n",
            "[06:53.560 --> 06:55.760] with a future GPT-4.\n",
            "[06:55.760 --> 06:59.720] This is an idea that I heard on Dr. Alan Thompson's channel that says that in\n",
            "[06:59.720 --> 07:05.600] the near future, where GPT-4 can begin training, Whisper could offer the system\n",
            "[07:05.600 --> 07:09.800] a huge source of data that previous systems had not been able to count on.\n",
            "[07:09.800 --> 07:14.640] Think of a system like GPT-3 as having been trained with a lot of Wikipedia articles,\n",
            "[07:14.640 --> 07:19.120] of books, of forums, of Internet conversations, but he has never been able to access all of the\n",
            "[07:19.120 --> 07:23.640] that spoken source that may be in databases such as YouTube.\n",
            "[07:23.640 --> 07:28.240] A tool such as Whisper could be used to sweep YouTube completely, transcribe\n",
            "[07:28.240 --> 07:33.200] many of their audios and get, unlock a new source of data that previously would not have\n",
            "[07:33.200 --> 07:37.400] It has been possible to use it to train a future language model.\n",
            "[07:37.400 --> 07:41.560] This is the enormous value of a tool like Whisper that I think makes it so interesting.\n",
            "[07:41.560 --> 07:42.560] to this technology.\n",
            "[07:42.560 --> 07:47.680] No, it does not solve a task that is spectacular, such as generating images or generating video, but it does\n",
            "[07:47.680 --> 07:52.280] solves a very useful task and almost solves it to perfection.\n",
            "[07:52.280 --> 07:57.640] I say almost, it's not perfect, sometimes some words are obviously wrong and it doesn't cover it.\n",
            "[07:57.640 --> 08:02.200] all the languages that exist on planet Earth, and well, to look for some limitation\n",
            "[08:02.200 --> 08:07.320] compared to other commercial tools, as it does not yet work in real time either.\n",
            "[08:07.320 --> 08:11.280] Depending on the length of the audio, it can take a few seconds to process it, sometimes\n",
            "[08:11.280 --> 08:17.080] It's a solid tool, it's mature, it's useful and it's open source,\n",
            "[08:17.080 --> 08:21.040] now allowing anyone to have access to a professional transcription tool.\n",
            "[08:21.040 --> 08:25.160] and text translation better than any free alternative.\n",
            "[08:25.160 --> 08:26.160] What?\n",
            "[08:26.160 --> 08:28.600] Oh, that you too would like to have access to this tool.\n",
            "[08:28.600 --> 08:32.720] Well, come on, I'll prepare an easy tutorial for all of you to use.\n",
            "[08:32.720 --> 08:37.640] We are going to do it in Google Colab, but first and taking advantage of the fact that we are talking about programming,\n",
            "[08:37.640 --> 08:41.880] of development, of innovation, let me remind you that there are very few days left\n",
            "[08:41.880 --> 08:46.880] for Samsung Dev Day, which is the technology event held every year, to be held at the\n",
            "[08:46.880 --> 08:51.760] year the Samsung Dev Spain community, which is Samsung's official community for developers.\n",
            "[08:51.760 --> 08:52.840] Spanish.\n",
            "[08:52.840 --> 08:55.560] This will be a free event not to be missed.\n",
            "[08:55.560 --> 09:00.640] If you are in Madrid you can attend in person on 16 November at the cloister of\n",
            "[09:00.640 --> 09:04.840] the Hieronymites of the Museo del Prado and if not, you can connect online at\n",
            "[09:04.840 --> 09:05.840] its streaming.\n",
            "[09:05.840 --> 09:09.760] But yes, you have to register, I was lucky enough to be able to participate last year with one of my own.\n",
            "[09:09.760 --> 09:14.280] presentation on code generation with artificial intelligence, and the experience was\n",
            "[09:14.280 --> 09:15.280] great.\n",
            "[09:15.280 --> 09:18.800] So you see, it's going to be an event full of great talks, talking about technology,\n",
            "[09:18.800 --> 09:23.280] of innovation, of applications, and it will also be presented by my dudev, who will surely\n",
            "[09:23.280 --> 09:26.560] Many of you will know him, so you can't miss him.\n",
            "[09:26.560 --> 09:30.320] I'll leave a link to the Samsung Dev website below in the description box.\n",
            "[09:30.320 --> 09:35.160] Spain, where you will find all the information regarding the agenda where you can register and a\n",
            "[09:35.160 --> 09:37.040] a lot of other resources.\n",
            "[09:37.040 --> 09:38.720] See you on 16 November.\n",
            "[09:38.720 --> 09:43.400] Let's see how we can use Whisper in our own code.\n",
            "[09:43.400 --> 09:47.240] For this we are going to use Google Colab, you know that Google is giving us here\n",
            "[09:47.240 --> 09:52.080] a free virtual machine that we can use and we will verify whenever we have\n",
            "[09:52.080 --> 09:56.560] enabled the GPU hardware accelerated environment type, OK, let's hit it here.\n",
            "[09:56.560 --> 10:01.320] GPU, let's hit save and now the first step will be to install Whisper.\n",
            "[10:01.320 --> 10:05.600] To do this we are going to use these two commands here, to install, you can find this here\n",
            "[10:05.600 --> 10:11.160] in Whisper's own GitHub repository, I'm going to leave you below in the little description box\n",
            "[10:11.160 --> 10:14.160] these commands, hit run and let it install.\n",
            "[10:14.160 --> 10:17.880] Once installed, we are going to upload some audio that we want to transcribe, in this case\n",
            "[10:17.880 --> 10:21.920] I'm going to try RosalÃ­a's song from Chicken Teriyaki, let's put it here,\n",
            "[10:21.920 --> 10:26.800] we drag it and now the next step we are going to take it here and we are going to put the command\n",
            "[10:26.800 --> 10:31.640] necessary to be able to run it, we're going to hit song.mp3 here, it's called the file that we\n",
            "[10:31.640 --> 10:37.680] we have uploaded, okay, song.mp3, so the task is going to be to transcribe the size of the model,\n",
            "[10:37.680 --> 10:42.560] there are different sizes depending on whether you want more speed when making the inference\n",
            "[10:42.560 --> 10:46.920] or if you want more precision in the results, I usually work with the Medium model.\n",
            "[10:46.920 --> 10:50.600] which is the one that gives me good results, there are bigger models, there are smaller models, try it.\n",
            "[10:50.600 --> 10:55.360] and in this case simply where we are going to place the output file, we run\n",
            "[10:55.360 --> 11:00.040] and that's it, that's it, no more to do, okay, we're already using Whisper,\n",
            "[11:00.040 --> 11:03.660] the first time it will take a little while because you have to download the model but after this\n",
            "[11:03.660 --> 11:08.520] At the moment you can use this system to transcribe any audio you want,\n",
            "[11:08.520 --> 11:13.640] mola, ok, we can see that in this case it has detected that the language is Spanish, it has made the inference\n",
            "[11:13.640 --> 11:16.800] automatic because we haven't told you that we are going to transcribe from Spanish, you can do it\n",
            "[11:16.800 --> 11:20.960] if you want and when this cell is already executed, we can come here, we'll see\n",
            "[11:20.960 --> 11:26.400] that the Audio Transcription folder has been generated and here we have the different options, we can\n",
            "[11:26.400 --> 11:32.360] open the sound.txt and here we open the file, we can see that we have the whole song perfectly.\n",
            "[11:32.360 --> 11:37.000] transcribed, which in this case, being Rosalia, has more merit and instead of wanting to\n",
            "[11:37.000 --> 11:41.680] transcription, you would like to make the translation, i.e. to convert your\n",
            "[11:41.680 --> 11:45.640] voice, your audio to English, so all you have to do is change here the\n",
            "[11:45.640 --> 11:51.480] task by Translate and in this case Whisper will work to translate what it has transcribed.\n",
            "[11:51.480 --> 11:54.880] In this case, if you notice, the command we have used is the console command\n",
            "[11:54.880 --> 11:58.480] but you may want to use Whisper within your code, then you can also use\n",
            "[11:58.480 --> 12:02.000] you have the option to work with Whisper's own library, it's simply this one\n",
            "[12:02.000 --> 12:05.960] line of code from here, import it, load the model we want, here then\n",
            "[12:05.960 --> 12:10.960] I would load the Medium model which is the one that, as I said, works best for my case, and with\n",
            "[12:10.960 --> 12:17.520] the loaded model then here we call model.transcribe, let's put here song.mp3, we hit run\n",
            "[12:17.520 --> 12:20.880] and in a matter of seconds we will have our transcript back.\n",
            "[12:20.880 --> 12:24.520] And here it is, the Rosalia, pink without a card, I send it to your cat, I have it for you.\n",
            "[12:24.520 --> 12:27.600] with roulette, no need for a serenade, ok.\n",
            "[12:27.600 --> 12:31.480] However, to make your life easier I have prepared a notebook that you can use,\n",
            "[12:31.480 --> 12:35.000] is below in the description box, where you already have all the code ready for\n",
            "[12:35.000 --> 12:39.200] to start working, just log in, check that the GPU is activated,\n",
            "[12:39.200 --> 12:43.080] click on this button here to install everything necessary, here we choose the\n",
            "[12:43.080 --> 12:47.680] task we want to do, whether it is transcribing into any language or translating into English.\n",
            "[12:47.680 --> 12:48.800] and click on run.\n",
            "[12:48.800 --> 12:53.520] In this case the cell is prepared so that the moment you start to run it\n",
            "[12:53.520 --> 12:57.080] your microphone is recording right now, i.e. right now we would be generating\n",
            "[12:57.080 --> 13:00.960] an audio file that we will then use for transcribing with Whisper, this is by\n",
            "[13:00.960 --> 13:05.480] if you want to make a real time transcript of any class or anything else\n",
            "[13:05.480 --> 13:06.480] you need.\n",
            "[13:06.480 --> 13:10.800] We're going to hit stop, we hit this button and in a moment we have the result of what we've done.\n",
            "[13:10.800 --> 13:12.520] we have said.\n",
            "[13:12.520 --> 13:16.800] Below you will find the two commands needed to be able to transcribe or translate\n",
            "[13:16.800 --> 13:19.240] the audio you upload.\n",
            "[13:19.240 --> 13:22.760] Finally, you should also know that if you want something simpler, then there are pages\n",
            "[13:22.760 --> 13:27.240] website where you can try out this system by uploading your own audios or by recording\n",
            "[13:27.240 --> 13:28.240] from the microphone.\n",
            "[13:28.240 --> 13:32.960] And this would be, 2022 is shaping up to be a spectacular year in terms of numbers.\n",
            "[13:32.960 --> 13:37.360] of neural toys that are coming into our hands to build a whole bunch\n",
            "[13:37.360 --> 13:39.320] and to be able to touch them.\n",
            "[13:39.320 --> 13:41.640] Now it's your turn, what can you do about it?\n",
            "[13:41.640 --> 13:45.080] Well, you can build a lot of super interesting things, you can connect by\n",
            "[13:45.080 --> 13:49.960] example Whisper with Stable Diffusion so that you can loudly ask it to\n",
            "[13:49.960 --> 13:54.040] generate a table or you can for example take all your classes at the university or\n",
            "[13:54.040 --> 13:58.960] all working meetings, transcribing them, creating a huge bank of transcripts and\n",
            "[13:58.960 --> 14:03.680] then with the GPT-3 API to make a chatbot that allows you to query, ask questions\n",
            "[14:03.680 --> 14:06.160] and answers on all these sources of information.\n",
            "[14:06.160 --> 14:10.040] For example, something I want to do is to take all the videos from my YouTube channel.\n",
            "[14:10.040 --> 14:14.640] and transcribe it, generate good quality subtitles in both Spanish and English\n",
            "[14:14.640 --> 14:18.920] and to be able to make statistics and queries on how many times I have said for example the word\n",
            "[14:18.920 --> 14:19.920] Machine Learning.\n",
            "[14:19.920 --> 14:23.360] There are a lot of applications that you can start to build, that you can start to build, that you can start to\n",
            "[14:23.360 --> 14:27.160] create by combining all these technologies.\n",
            "[14:27.160 --> 14:29.880] I had a dog barking in the background that was bothering me a lot.\n",
            "[14:29.880 --> 14:34.080] Well, as I was saying, you can create a lot of things and there is a lot to do.\n",
            "[14:34.080 --> 14:37.560] From here, from this channel, we will continue to experiment with this technology,\n",
            "[14:37.560 --> 14:42.320] I'll keep bringing you new tools so if you haven't done so yet subscribe,\n",
            "[14:42.320 --> 14:46.000] click on the little bell so that you always receive notifications of new videos\n",
            "[14:46.000 --> 14:50.440] and if you want to support all this content you know you can do so through Patreon\n",
            "[14:50.440 --> 14:52.080] below in the description box.\n",
            "[14:52.080 --> 14:55.080] You have a couple of videos around here that are super interesting, I don't know what they are but\n",
            "[14:55.080 --> 14:58.960] are super interesting, keep an eye on them and we'll see you with more artificial intelligence.\n",
            "[14:58.960 --> 15:26.280] guys, girls, in the next video.\n",
            "\n",
            "DeepL: Character usage: 16409 / 500000 (3.3%)\n",
            "\n",
            "Writing translated results...\n",
            "\n",
            "audio_transcription/Whisper-Example_English (British).txt\n",
            "audio_transcription/Whisper-Example_English (British).vtt\n",
            "audio_transcription/Whisper-Example_English (British).srt\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}