{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Carleslc/AudioToText/blob/master/AudioToText%20(Example).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5hvo8QWN-a9"
      },
      "source": [
        "# üó£Ô∏è [**AudioToText**](https://github.com/Carleslc/AudioToText)\n",
        "\n",
        "[![Donate](https://www.ko-fi.com/img/githubbutton_sm.svg)](https://ko-fi.com/carleslc)\n",
        "\n",
        "### üõ† [Whisper by OpenAI](https://github.com/openai/whisper)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_lylR1xWMxk"
      },
      "source": [
        "## [Step 1] ‚öôÔ∏è Install the required libraries\n",
        "\n",
        "Click ‚ñ∂Ô∏è button below to install the dependencies for this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SJl7HJOeo0-P",
        "outputId": "a4c4a399-af68-4b89-f74f-fc91f105a36c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.2.7-0ubuntu0.1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-510\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 28 not upgraded.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-x9bbewbt\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-x9bbewbt\n",
            "  Resolved https://github.com/openai/whisper.git to commit 7858aa9c08d98f75575035ecd6481f462d66ca27\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting deepl\n",
            "  Downloading deepl-1.13.0-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from openai-whisper==20230124) (1.21.6)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from openai-whisper==20230124) (1.13.1+cu116)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from openai-whisper==20230124) (4.64.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.8/dist-packages (from openai-whisper==20230124) (9.0.0)\n",
            "Collecting transformers>=4.19.0\n",
            "  Downloading transformers-4.26.0-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpeg-python==0.2.0\n",
            "  Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.8/dist-packages (from ffmpeg-python==0.2.0->openai-whisper==20230124) (0.16.0)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.8/dist-packages (from deepl) (2.25.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2->deepl) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2->deepl) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2->deepl) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2->deepl) (4.0.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m70.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.19.0->openai-whisper==20230124) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.19.0->openai-whisper==20230124) (23.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.12.0-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers>=4.19.0->openai-whisper==20230124) (3.9.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.19.0->openai-whisper==20230124) (2022.6.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch->openai-whisper==20230124) (4.4.0)\n",
            "Building wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20230124-py3-none-any.whl size=1179424 sha256=0a59172a6930ecc8a57a35707524c2070695f179b2f05110c6a327e24b958920\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-iqdvkje1/wheels/a7/70/18/b7693c07b1d18b3dafb328f5d0496aa0d41a9c09ef332fd8e6\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: tokenizers, ffmpeg-python, huggingface-hub, deepl, transformers, openai-whisper\n",
            "Successfully installed deepl-1.13.0 ffmpeg-python-0.2.0 huggingface-hub-0.12.0 openai-whisper-20230124 tokenizers-0.13.2 transformers-4.26.0\n"
          ]
        }
      ],
      "source": [
        "#@title { display-mode: \"form\" }\n",
        "!apt install ffmpeg\n",
        "!pip install git+https://github.com/openai/whisper.git deepl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5A5bTMB8XmtI"
      },
      "source": [
        "## [Step 2] üìÅ Upload your audio files to the Files folder\n",
        "\n",
        "‚¨ÖÔ∏è Files folder in Google Colab is on the left menu\n",
        "\n",
        "Almost any audio or video file format is [supported](https://gist.github.com/Carleslc/1d6b922c8bf4a7e9627a6970d178b3a6)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9_I0W3tqTjr"
      },
      "source": [
        "## [Step 3] üëÇ Transcribe or Translate\n",
        "\n",
        "3.1. Choose a `task`:\n",
        "  - `Transcribe` speech to text in the same language of the source audio file.\n",
        "  - `Translate to English` speech to text in English.\n",
        "  \n",
        "Translation to other languages is not supported with _Whisper_ by default.\n",
        "You may try to choose the _Transcribe_ task and set your desired `language`, but translation is not guaranteed. However, you can use **_DeepL_** later in the Step 5 to translate the transcription to another language.\n",
        "\n",
        "3.2. Edit the `audio_file` to match your uploaded file name to transcribe.\n",
        "\n",
        "- If you want to transcribe multiple files with the same parameters you must separate their file names with commas `,`\n",
        "\n",
        "3.3. Run this cell and wait for the transcription to complete.\n",
        "\n",
        "  - You can try other parameters if the result with default parameters does not suit your needs.\n",
        "\n",
        "  If the execution takes too long to complete you can choose a smaller model in `use_model`, with an accuracy tradeoff.\n",
        "\n",
        "  [Available models and languages](https://github.com/openai/whisper#available-models-and-languages)\n",
        "\n",
        "  If the source audio file is entirely in English setting the `language` to English may provide better results when using a non-large model.\n",
        "  \n",
        "  More parameters are available in the code `options` object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "opNkn_Lgpat4",
        "outputId": "933d011b-1634-40a7-c750-a2b7476f8f57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using GPU\n",
            "GPU 0: Tesla T4 (UUID: GPU-1f48ac03-e5ba-922f-30a0-6ba2496c1c1c)\n",
            "\n",
            "Loading large-v2 model...\n",
            "Model large-v2 is multilingual and has 1,541,384,960 parameters.\n",
            "\n",
            "-- TRANSCRIPTION --\n",
            "\n",
            "Processing: chinese.wav\n",
            "\n",
            "Detected language: Chinese\n",
            "\n",
            "[00:00.600 --> 00:04.120] Èô¢Â≠êÈó®Âè£‰∏çËøúÂ§ÑÂ∞±ÊòØ‰∏Ä‰∏™Âú∞ÈìÅÁ´ô\n",
            "[00:04.140 --> 00:06.720] ËøôÊòØ‰∏Ä‰∏™Áæé‰∏ΩËÄåÁ•ûÂ•áÁöÑÊôØË±°\n",
            "[00:07.760 --> 00:10.520] Ê†ë‰∏äÈïøÊª°‰∫ÜÂèàÂ§ßÂèàÁîúÁöÑÊ°ÉÂ≠ê\n",
            "[00:11.520 --> 00:14.600] Êµ∑Ë±öÂíåÈ≤∏È±ºÁöÑË°®ÊºîÊòØÂæàÂ•ΩÁúãÁöÑËäÇÁõÆ\n",
            "[00:14.600 --> 00:19.400] ÈÇÆÂ±ÄÈó®ÂâçÁöÑÊ£±ÂΩ¢ÈÅì‰∏äÊúâ‰∏Ä‰∏™ËìùËâ≤ÁöÑÈÇÆÁÆ±\n",
            "\n",
            "Processing: bruce.mp3\n",
            "\n",
            "Detected language: English\n",
            "\n",
            "[00:00.000 --> 00:03.000]  I get up in the evening\n",
            "[00:04.000 --> 00:07.000]  And I ain't got nothing to say\n",
            "[00:07.000 --> 00:09.000]  I come home in the morning\n",
            "[00:10.000 --> 00:13.000]  I go to bed feeling the same way\n",
            "[00:13.000 --> 00:16.000]  I ain't nothing but tired\n",
            "[00:17.000 --> 00:20.000]  Man, I'm just tired and bored with myself\n",
            "[00:20.000 --> 00:22.000]  Hey there, baby\n",
            "[00:24.000 --> 00:26.000]  I could use just a little help\n",
            "[00:26.000 --> 00:29.000]  You can't start a fire\n",
            "[00:29.000 --> 00:33.000]  You can't start a fire without a spark\n",
            "[00:33.000 --> 00:35.000]  There's guns for hire\n",
            "[00:35.000 --> 00:39.000]  Even if we're just dancing in the dark\n",
            "\n",
            "Processing: english_japanese.mp3\n",
            "\n",
            "Detected language: Japanese\n",
            "\n",
            "[00:00.000 --> 00:07.000] This is Unit 1 of Pimsleur's Japanese 1. Listen to this Japanese conversation.\n",
            "[00:07.000 --> 00:10.000] „Åô„Åø„Åæ„Åõ„Çì„ÄÇËã±Ë™û„Åå„Çè„Åã„Çä„Åæ„Åô„Åã?\n",
            "[00:10.000 --> 00:15.000] „ÅÑ„ÅÑ„Åà„ÄÅ„Çè„Åã„Çä„Åæ„Åõ„Çì„ÄÇÊó•Êú¨Ë™û„Åå„Çè„Åã„Çä„Åæ„Åô„Åã?\n",
            "[00:15.000 --> 00:17.000] „ÅØ„ÅÑ„ÄÅÂ∞ë„Åó„Çè„Åã„Çä„Åæ„Åô„ÄÇ\n",
            "[00:17.000 --> 00:19.000] „ÅÇ„Å™„Åü„ÅØ„Ç¢„É°„É™„Ç´‰∫∫„Åß„Åô„Åã?\n",
            "[00:19.000 --> 00:34.000] „ÅØ„ÅÑ„ÄÅÁßÅ„ÅØ„Ç¢„É°„É™„Ç´‰∫∫„Åß„Åô„ÄÇ\n",
            "\n",
            "Processing: french.wav\n",
            "\n",
            "Detected language: French\n",
            "\n",
            "[00:00.000 --> 00:06.640]  Whisper est un syst√®me de reconnaissance automatique de la parole entra√Æn√© sur 680.000\n",
            "[00:06.640 --> 00:10.720]  heures de donn√©es multilingues et multit√¢ches r√©colt√©es sur Internet.\n",
            "[00:10.720 --> 00:16.000]  Nous √©tablissons que l'utilisation de donn√©es d'un tel nombre et d'une telle diversit√©\n",
            "[00:16.000 --> 00:20.500]  est la raison pour laquelle The System est √† m√™me de comprendre de nombreux accents\n",
            "[00:20.500 --> 00:24.880]  en d√©pit de bruit de fond, de comprendre un vocabulaire technique et de r√©ussir la\n",
            "[00:24.880 --> 00:27.640]  traduction depuis diverses langues en anglais.\n",
            "[00:27.640 --> 00:33.360]  Nous distribuons en tant que logiciel libre le code source pour nos mod√®les et pour l'inf√©rence\n",
            "[00:33.360 --> 00:37.040]  afin que ceux-ci puissent servir comme un point de d√©part pour construire des applications\n",
            "[00:37.040 --> 00:54.560]  utiles et pour aider √† faire progresser la recherche en traitement de la parole.\n"
          ]
        }
      ],
      "source": [
        "# import modules\n",
        "\n",
        "import os\n",
        "\n",
        "import whisper\n",
        "from whisper.utils import format_timestamp, get_writer\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "try:\n",
        "  import tensorflow  # required to avoid protobuf compatibility issues\n",
        "except ImportError:\n",
        "  pass\n",
        "\n",
        "import torch\n",
        "\n",
        "# detect device\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "print(f\"Using {'GPU' if DEVICE == 'cuda' else 'CPU ‚ö†Ô∏è'}\")\n",
        "\n",
        "# https://medium.com/analytics-vidhya/the-google-colab-system-specification-check-69d159597417\n",
        "if DEVICE == \"cuda\":\n",
        "  !nvidia-smi -L\n",
        "else:\n",
        "  !lscpu | grep \"Model name\"\n",
        "  print(\"Not using GPU can result in a very slow execution\")\n",
        "  print(\"Ensure Hardware accelerator by GPU is enabled in Google Colab: Runtime > Change runtime type\")\n",
        "\n",
        "# select task\n",
        "\n",
        "task = \"Transcribe\" #@param [\"Transcribe\", \"Translate to English\"]\n",
        "\n",
        "task = \"transcribe\" if task == \"Transcribe\" else \"translate\"\n",
        "\n",
        "# select audio file\n",
        "\n",
        "audio_file = \"chinese.wav, bruce.mp3, english_japanese.mp3, french.wav\" #@param {type:\"string\"}\n",
        "\n",
        "audio_files = audio_file.split(',')\n",
        "\n",
        "# set model\n",
        "\n",
        "use_model = \"large-v2\" #@param [\"tiny\", \"base\", \"small\", \"medium\", \"large-v1\", \"large-v2\"]\n",
        "\n",
        "# select language\n",
        "\n",
        "WHISPER_LANGUAGES = [k.title() for k in whisper.tokenizer.TO_LANGUAGE_CODE.keys()]\n",
        "\n",
        "language = \"Auto-Detect\" #@param [\"Auto-Detect\", \"Afrikaans\", \"Albanian\", \"Amharic\", \"Arabic\", \"Armenian\", \"Assamese\", \"Azerbaijani\", \"Bashkir\", \"Basque\", \"Belarusian\", \"Bengali\", \"Bosnian\", \"Breton\", \"Bulgarian\", \"Burmese\", \"Castilian\", \"Catalan\", \"Chinese\", \"Croatian\", \"Czech\", \"Danish\", \"Dutch\", \"English\", \"Estonian\", \"Faroese\", \"Finnish\", \"Flemish\", \"French\", \"Galician\", \"Georgian\", \"German\", \"Greek\", \"Gujarati\", \"Haitian\", \"Haitian Creole\", \"Hausa\", \"Hawaiian\", \"Hebrew\", \"Hindi\", \"Hungarian\", \"Icelandic\", \"Indonesian\", \"Italian\", \"Japanese\", \"Javanese\", \"Kannada\", \"Kazakh\", \"Khmer\", \"Korean\", \"Lao\", \"Latin\", \"Latvian\", \"Letzeburgesch\", \"Lingala\", \"Lithuanian\", \"Luxembourgish\", \"Macedonian\", \"Malagasy\", \"Malay\", \"Malayalam\", \"Maltese\", \"Maori\", \"Marathi\", \"Moldavian\", \"Moldovan\", \"Mongolian\", \"Myanmar\", \"Nepali\", \"Norwegian\", \"Nynorsk\", \"Occitan\", \"Panjabi\", \"Pashto\", \"Persian\", \"Polish\", \"Portuguese\", \"Punjabi\", \"Pushto\", \"Romanian\", \"Russian\", \"Sanskrit\", \"Serbian\", \"Shona\", \"Sindhi\", \"Sinhala\", \"Sinhalese\", \"Slovak\", \"Slovenian\", \"Somali\", \"Spanish\", \"Sundanese\", \"Swahili\", \"Swedish\", \"Tagalog\", \"Tajik\", \"Tamil\", \"Tatar\", \"Telugu\", \"Thai\", \"Tibetan\", \"Turkish\", \"Turkmen\", \"Ukrainian\", \"Urdu\", \"Uzbek\", \"Valencian\", \"Vietnamese\", \"Welsh\", \"Yiddish\", \"Yoruba\"]\n",
        "\n",
        "if language == \"Auto-Detect\":\n",
        "  language = \"detect\"\n",
        "\n",
        "if language and language != \"detect\" and language not in WHISPER_LANGUAGES:\n",
        "  print(f\"\\nLanguage '{language}' is invalid\")\n",
        "  language = \"detect\"\n",
        "\n",
        "if language and language != \"detect\":\n",
        "  print(f\"\\nLanguage: {language}\")\n",
        "\n",
        "# load model\n",
        "\n",
        "MODELS_WITH_ENGLISH_VERSION = [\"tiny\", \"base\", \"small\", \"medium\"]\n",
        "\n",
        "if language == \"English\" and use_model in MODELS_WITH_ENGLISH_VERSION:\n",
        "  use_model += \".en\"\n",
        "\n",
        "print(f\"\\nLoading {use_model} model...\")\n",
        "\n",
        "model = whisper.load_model(use_model, device=DEVICE)\n",
        "\n",
        "print(\n",
        "    f\"Model {use_model} is {'multilingual' if model.is_multilingual else 'English-only'} \"\n",
        "    f\"and has {sum(np.prod(p.shape) for p in model.parameters()):,d} parameters.\\n\"\n",
        ")\n",
        "\n",
        "# set options\n",
        "\n",
        "coherence_preference = \"More coherence, but may repeat text\" #@param [\"More coherence, but may repeat text\", \"Less repetitions, but may have less coherence\"]\n",
        "\n",
        "## Info: https://github.com/openai/whisper/blob/main/whisper/transcribe.py#L19\n",
        "options = {\n",
        "    'task': task,\n",
        "    'verbose': True,\n",
        "    'fp16': DEVICE == 'cuda',\n",
        "    'best_of': 5,\n",
        "    'beam_size': 5,\n",
        "    'patience': None,\n",
        "    'length_penalty': None,\n",
        "    'suppress_tokens': '-1',\n",
        "    'temperature': (0.0, 0.2, 0.4, 0.6, 0.8, 1.0),\n",
        "    'condition_on_previous_text': coherence_preference == \"More coherence, but may repeat text\",\n",
        "}\n",
        "\n",
        "if DEVICE == 'cpu':\n",
        "  torch.set_num_threads(os.cpu_count())\n",
        "\n",
        "# execute task\n",
        "# !whisper \"{audio_file}\" --task {task} --model {use_model} --output_dir {output_dir} --device {DEVICE} --verbose {options['verbose']}\n",
        "\n",
        "if task == \"translate\":\n",
        "  print(\"-- TRANSLATE TO ENGLISH --\")\n",
        "else:\n",
        "  print(\"-- TRANSCRIPTION --\")\n",
        "\n",
        "results = {} # audio_path to result\n",
        "\n",
        "for audio_path in audio_files:\n",
        "  audio_path = audio_path.strip()\n",
        "  \n",
        "  print(f\"\\nProcessing: {audio_path}\\n\")\n",
        "\n",
        "  # detect language\n",
        "  detect_language = not language or language == \"detect\"\n",
        "  if detect_language:\n",
        "    # load audio and pad/trim it to fit 30 seconds\n",
        "    audio = whisper.load_audio(audio_path)\n",
        "    audio = whisper.pad_or_trim(audio)\n",
        "\n",
        "    # make log-Mel spectrogram and move to the same device as the model\n",
        "    mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
        "\n",
        "    # detect the spoken language\n",
        "    _, probs = model.detect_language(mel)\n",
        "\n",
        "    language_code = max(probs, key=probs.get)\n",
        "    options['language'] = whisper.tokenizer.LANGUAGES[language_code].title()\n",
        "    \n",
        "    print(f\"Detected language: {options['language']}\\n\")\n",
        "  else:\n",
        "    options['language'] = language\n",
        "\n",
        "  # transcribe\n",
        "  results[audio_path] = whisper.transcribe(model, audio_path, **options)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTrxbUivk_h3"
      },
      "source": [
        "## [Step 4] üíæ **Save results**\n",
        "\n",
        "Run this cell to write the transcription as a file output.\n",
        "\n",
        "Results will be available in the **audio_transcription** folder in the formats selected in `output_formats`.\n",
        "\n",
        "If you don't see that folder, you may need to refresh üîÑ the Files folder.\n",
        "\n",
        "Available formats: `txt,vtt,srt,tsv,json`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "id": "wNsrB45_lCIl",
        "outputId": "ac9e1229-09f7-4ef7-d383-6ab943b37b22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing results...\n",
            "\n",
            "audio_transcription/chinese.txt\n",
            "audio_transcription/chinese.vtt\n",
            "audio_transcription/chinese.srt\n",
            "\n",
            "audio_transcription/bruce.txt\n",
            "audio_transcription/bruce.vtt\n",
            "audio_transcription/bruce.srt\n",
            "\n",
            "audio_transcription/english_japanese.txt\n",
            "audio_transcription/english_japanese.vtt\n",
            "audio_transcription/english_japanese.srt\n",
            "\n",
            "audio_transcription/french.txt\n",
            "audio_transcription/french.vtt\n",
            "audio_transcription/french.srt\n"
          ]
        }
      ],
      "source": [
        "# set output folder\n",
        "output_dir = \"audio_transcription\"\n",
        "\n",
        "# set output formats: https://github.com/openai/whisper/blob/7858aa9c08d98f75575035ecd6481f462d66ca27/whisper/utils.py#L145\n",
        "output_formats = \"txt,vtt,srt\" #@param [\"txt,vtt,srt,tsv,json\", \"txt,vtt,srt\", \"txt,vtt\", \"txt,srt\", \"txt\", \"vtt\", \"srt\", \"tsv\", \"json\"] {allow-input: true}\n",
        "output_formats = output_formats.split(',')\n",
        "\n",
        "def write_result(result, output_format, output_file_name):\n",
        "  output_format = output_format.strip()\n",
        "\n",
        "  # start captions in non-zero timestamp (some media players does not detect the first caption)\n",
        "  fix_vtt = output_format == 'vtt' and result[\"segments\"] and result[\"segments\"][0].get('start') == 0\n",
        "  \n",
        "  if fix_vtt:\n",
        "    result[\"segments\"][0]['start'] += 1/1000 # +1ms\n",
        "\n",
        "  # write result in the desired format\n",
        "  writer = get_writer(output_format, output_dir)\n",
        "  writer(result, output_file_name)\n",
        "\n",
        "  if fix_vtt:\n",
        "    result[\"segments\"][0]['start'] = 0 # reset change\n",
        "\n",
        "  output_file_path = os.path.join(output_dir, f\"{output_file_name}.{output_format}\")\n",
        "  print(output_file_path)\n",
        "\n",
        "# save results\n",
        "\n",
        "print(\"Writing results...\")\n",
        "\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "for audio_path, result in results.items():\n",
        "  print(end='\\n')\n",
        "  \n",
        "  output_file_name = os.path.splitext(os.path.basename(audio_path))[0]\n",
        "\n",
        "  for output_format in output_formats:\n",
        "    write_result(result, output_format, output_file_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfkDhNMMvY8s"
      },
      "source": [
        "## [Step 5] üí¨ Translate results with DeepL (API key needed)\n",
        "\n",
        "This is an **optional** step to translate the transcription to another language using the **DeepL** API.\n",
        "\n",
        "[Get a DeepL Developer Account API Key](https://www.deepl.com/pro-api?cta=header-pro-api)\n",
        "\n",
        "Set the `deepl_api_key` to translate the transcription to a supported language in `deepl_target_language`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "form",
        "id": "28f7EIP-rez0",
        "outputId": "0248833e-2e6b-4db5-f753-c2eba368ffd7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chinese.wav\n",
            "\n",
            "DeepL: Translate results from Chinese [ZH] to Spanish [ES]\n",
            "\n",
            "[00:00.600 --> 00:04.120] Hay una estaci√≥n de metro no muy lejos de la entrada al patio\n",
            "[00:04.140 --> 00:06.720] Es un espect√°culo hermoso y m√°gico\n",
            "[00:07.760 --> 00:10.520] Los √°rboles est√°n llenos de melocotones grandes y dulces\n",
            "[00:11.520 --> 00:14.600] El espect√°culo de delfines y ballenas es un gran espect√°culo para ver\n",
            "[00:14.600 --> 00:19.400] Hay un buz√≥n azul en el camino prism√°tico frente a la oficina de correos\n",
            "\n",
            "DeepL: Character usage: 33634 / 500000 (6.7%)\n",
            "\n",
            "bruce.mp3\n",
            "\n",
            "DeepL: Translate results from English [EN] to Spanish [ES]\n",
            "\n",
            "[00:00.000 --> 00:03.000] Me levanto por la tarde\n",
            "[00:04.000 --> 00:07.000] Y no tengo nada que decir\n",
            "[00:07.000 --> 00:09.000] Vuelvo a casa por la ma√±ana\n",
            "[00:10.000 --> 00:13.000] Me voy a la cama sintiendo lo mismo\n",
            "[00:13.000 --> 00:16.000] No estoy m√°s que cansado\n",
            "[00:17.000 --> 00:20.000] T√≠o, estoy cansado y aburrido de m√≠ mismo\n",
            "[00:20.000 --> 00:22.000] Hola, cari√±o.\n",
            "[00:24.000 --> 00:26.000] Me vendr√≠a bien un poco de ayuda\n",
            "[00:26.000 --> 00:29.000] No puedes encender un fuego\n",
            "[00:29.000 --> 00:33.000] No se puede encender un fuego sin una chispa\n",
            "[00:33.000 --> 00:35.000] Hay armas de alquiler\n",
            "[00:35.000 --> 00:39.000] Aunque s√≥lo estemos bailando en la oscuridad\n",
            "\n",
            "DeepL: Character usage: 33987 / 500000 (6.8%)\n",
            "\n",
            "english_japanese.mp3\n",
            "\n",
            "DeepL: Translate results from Japanese [JA] to Spanish [ES]\n",
            "\n",
            "[00:00.000 --> 00:07.000] Esta es la Unidad 1 de Japon√©s 1 de Pimsleur. Escucha esta conversaci√≥n en japon√©s.\n",
            "[00:07.000 --> 00:10.000] Disculpe, ¬øentiende ingl√©s?\n",
            "[00:10.000 --> 00:15.000] No, no entiendo, ¬øentiendes japon√©s?\n",
            "[00:15.000 --> 00:17.000] S√≠, lo entiendo un poco.\n",
            "[00:17.000 --> 00:19.000] ¬øEs usted estadounidense?\n",
            "[00:19.000 --> 00:34.000] S√≠, soy estadounidense.\n",
            "\n",
            "DeepL: Character usage: 34140 / 500000 (6.8%)\n",
            "\n",
            "french.wav\n",
            "\n",
            "DeepL: Translate results from French [FR] to Spanish [ES]\n",
            "\n",
            "[00:00.000 --> 00:06.640] Whisper es un sistema de reconocimiento autom√°tico del habla entrenado en 680.000\n",
            "[00:06.640 --> 00:10.720] horas de datos multiling√ºes y multitarea recogidos en Internet.\n",
            "[00:10.720 --> 00:16.000] Establecemos que el uso de un n√∫mero tan grande y diversidad de datos\n",
            "[00:16.000 --> 00:20.500] es la raz√≥n por la que El Sistema es capaz de entender muchos acentos\n",
            "[00:20.500 --> 00:24.880] ruido, comprender el vocabulario t√©cnico y completar con √©xito el\n",
            "[00:24.880 --> 00:27.640] traducci√≥n de varios idiomas al ingl√©s.\n",
            "[00:27.640 --> 00:33.360] Distribuimos como c√≥digo abierto el c√≥digo fuente de nuestros modelos y de la inferencia\n",
            "[00:33.360 --> 00:37.040] de modo que puedan utilizarse como punto de partida para crear aplicaciones\n",
            "[00:37.040 --> 00:54.560] y contribuir al avance de la investigaci√≥n en el procesamiento del habla.\n",
            "\n",
            "DeepL: Character usage: 34866 / 500000 (7.0%)\n",
            "\n",
            "Writing translated results...\n",
            "\n",
            "audio_transcription/chinese_Spanish.txt\n",
            "audio_transcription/chinese_Spanish.vtt\n",
            "audio_transcription/chinese_Spanish.srt\n",
            "\n",
            "audio_transcription/bruce_Spanish.txt\n",
            "audio_transcription/bruce_Spanish.vtt\n",
            "audio_transcription/bruce_Spanish.srt\n",
            "\n",
            "audio_transcription/english_japanese_Spanish.txt\n",
            "audio_transcription/english_japanese_Spanish.vtt\n",
            "audio_transcription/english_japanese_Spanish.srt\n",
            "\n",
            "audio_transcription/french_Spanish.txt\n",
            "audio_transcription/french_Spanish.vtt\n",
            "audio_transcription/french_Spanish.srt\n"
          ]
        }
      ],
      "source": [
        "import deepl\n",
        "\n",
        "# translation service options (DeepL Developer Account)\n",
        "\n",
        "deepl_api_key = \"\" #@param {type:\"string\"}\n",
        "deepl_target_language = \"Spanish\" #@param [\"\", \"Bulgarian\", \"Chinese (simplified)\", \"Czech\", \"Danish\", \"Dutch\", \"English (American)\", \"English (British)\", \"Estonian\", \"Finnish\", \"French\", \"German\", \"Greek\", \"Hungarian\", \"Indonesian\", \"Italian\", \"Japanese\", \"Latvian\", \"Lithuanian\", \"Polish\", \"Portuguese (Brazilian)\", \"Portuguese (European)\", \"Romanian\", \"Russian\", \"Slovak\", \"Slovenian\", \"Spanish\", \"Swedish\", \"Turkish\", \"Ukrainian\"]\n",
        "\n",
        "use_deepl_translation = deepl_api_key and deepl_target_language\n",
        "\n",
        "if not use_deepl_translation:\n",
        "  if not deepl_api_key:\n",
        "    print(\"Required: deepl_api_key\")\n",
        "    print(\"Get a DeepL Developer Account API Key: https://www.deepl.com/pro-api?cta=header-pro-api\")\n",
        "  if not deepl_target_language:\n",
        "    print(\"Required: deepl_target_language\")\n",
        "else:\n",
        "  translated_results = {} # audio_path to translated segments results\n",
        "\n",
        "  try:\n",
        "    deepl_translator = deepl.Translator(deepl_api_key)\n",
        "\n",
        "    deepl_source_languages = [lang.code.upper() for lang in deepl_translator.get_source_languages()]\n",
        "    \n",
        "    deepl_target_languages_dict = deepl_translator.get_target_languages()\n",
        "    deepl_target_languages = [lang.name for lang in deepl_target_languages_dict]\n",
        "\n",
        "    deepl_target_language_code = next(lang.code for lang in deepl_target_languages_dict if lang.name == deepl_target_language).upper()\n",
        "    target_language_code = deepl_target_language_code.split('-')[0]\n",
        "    \n",
        "    for audio_path, result in results.items():\n",
        "      deepl_usage = deepl_translator.get_usage()\n",
        "      \n",
        "      if deepl_usage.any_limit_reached:\n",
        "        print(audio_path)\n",
        "        print(\"DeepL: Translation limit reached.\\n\")\n",
        "        use_deepl_translation = False\n",
        "      else:\n",
        "        print(audio_path + '\\n')\n",
        "      \n",
        "      # translate results (DeepL)\n",
        "      if use_deepl_translation:\n",
        "        source_language_code = whisper.tokenizer.TO_LANGUAGE_CODE.get(result['language'].lower()).upper()\n",
        "\n",
        "        if (task == 'translate' and target_language_code != 'EN') or (task == 'transcribe' and source_language_code in deepl_source_languages and source_language_code != target_language_code):\n",
        "          source_lang = source_language_code if task == 'transcribe' else None\n",
        "          translate_from = f\"from {result['language']} [{source_language_code}] \" if source_lang else ''\n",
        "          print(f\"DeepL: Translate results {translate_from}to {deepl_target_language} [{deepl_target_language_code}]\\n\")\n",
        "\n",
        "          translated_results[audio_path] = { \"segments\": [] }\n",
        "\n",
        "          segments = result[\"segments\"]\n",
        "          deepl_batch_requests_size = 10\n",
        "          \n",
        "          for batch_segments in [segments[i:i + deepl_batch_requests_size] for i in range(0, len(segments), deepl_batch_requests_size)]:\n",
        "            deepl_results = deepl_translator.translate_text([segment['text'] for segment in batch_segments], source_lang=source_lang, target_lang=deepl_target_language_code, split_sentences='off')\n",
        "            \n",
        "            for j, deepl_result in enumerate(deepl_results):\n",
        "              segment = batch_segments[j]\n",
        "              translated_text = deepl_result.text\n",
        "              translated_results[audio_path][\"segments\"].append(dict(id=segment['id'], start=segment['start'], end=segment['end'], text=translated_text))\n",
        "\n",
        "              if options['verbose']:\n",
        "                print(f\"[{format_timestamp(segment['start'])} --> {format_timestamp(segment['end'])}] {translated_text}\")\n",
        "\n",
        "          deepl_usage = deepl_translator.get_usage()\n",
        "          \n",
        "          if deepl_usage.character.valid:\n",
        "            print(f\"\\nDeepL: Character usage: {deepl_usage.character.count} / {deepl_usage.character.limit} ({100*(deepl_usage.character.count/deepl_usage.character.limit):.1f}%)\\n\")\n",
        "        elif task == 'transcribe' and source_language_code not in deepl_source_languages:\n",
        "          print(f\"DeepL: {result['language']} is not yet supported\")\n",
        "  except deepl.DeepLException as e:\n",
        "    if isinstance(e, deepl.AuthorizationException) and str(e) == \"Authorization failure, check auth_key\":\n",
        "      e = \"Authorization failure, check deepl_api_key\"\n",
        "    print(f\"DeepL: [Error] {e}\")\n",
        "  \n",
        "  # save translated results (if any)\n",
        "\n",
        "  if translated_results:\n",
        "    print(\"Writing translated results...\")\n",
        "\n",
        "    for audio_path, translated_result in translated_results.items():\n",
        "      print(end='\\n')\n",
        "      \n",
        "      output_file_name = os.path.splitext(os.path.basename(audio_path))[0]\n",
        "      translated_output_file_name = f\"{output_file_name}_{deepl_target_language}\"\n",
        "\n",
        "      for output_format in output_formats:\n",
        "        write_result(translated_result, output_format, translated_output_file_name)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}